# -*- coding: utf-8 -*-
"""plots-comparision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SlQtCEe13LKcwbOWpBG2fLhgszaCMld7
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Define the list of metrics to be scaled by 100
METRICS_TO_SCALE = ['METEOR', 'F1', 'Precision', 'Recall']


def load_results(filename):
    """
    Load results from a given filename and structure them into a nested dictionary.
    Excludes specified metrics entirely and scales certain metrics by 100.
    """
    results = {}
    current_section = None
    with open(filename, 'r') as f:
        for line in f:
            if line.startswith("Summary Statistics for"):
                # Extract section name (e.g., "Abstract")
                current_section = line.split("for")[1].split("Model:")[0].strip().lower()
                results[current_section] = {}
            elif ":" in line and current_section:
                # Extract metric name and value
                parts = line.strip().split(': ', 1)
                if len(parts) != 2:
                    continue  # Skip malformed lines
                key, value = parts
                try:
                    # Extract the actual metric name by removing the type prefix (e.g., 'Average ')
                    metric_name = key.split(' ', 1)[-1] if ' ' in key else key
                    # Check if the metric needs to be excluded (case-insensitive, substring-based)
                    # Check if the metric needs to be scaled
                    if any(scale_metric.lower() == metric_name.lower() for scale_metric in METRICS_TO_SCALE):
                        scaled_value = float(value) * 100  # Scale by 100
                        results[current_section][key] = scaled_value
                    else:
                        # Include other metrics without scaling
                        results[current_section][key] = float(value)
                except ValueError:
                    # Handle non-float values gracefully
                    results[current_section][key] = np.nan
    return results

def plot_heatmap(metrics, baseline_values, peft_values, standard_values, section, metric_type):
    """
    Plot a heatmap comparing Baseline, PEFT T5, and Standard T5 models for given metrics.
    """
    print(f"Debug: Plotting heatmap for {section} - {metric_type}")
    print(f"Debug: Metrics: {metrics}")
    print(f"Debug: Baseline values: {baseline_values}")
    print(f"Debug: PEFT values: {peft_values}")
    print(f"Debug: Standard values: {standard_values}")

    # Create a DataFrame for the heatmap
    data = np.array([baseline_values, peft_values, standard_values])
    df = pd.DataFrame(data, columns=metrics, index=['Baseline', 'PEFT T5', 'Standard T5'])

    print(f"Debug: DataFrame shape: {df.shape}")
    print(f"Debug: DataFrame columns: {df.columns}")

    # Plot the heatmap
    plt.figure(figsize=(12, max(6, len(df.columns)*0.5)))  # Adjust height based on number of metrics
    sns.heatmap(df, annot=True, cmap='YlOrRd', fmt='.2f', linewidths=.5, linecolor='gray')
    plt.title(f'Heatmap of {metric_type} Metrics - {section.capitalize()}', size=16)
    plt.xlabel('Models')
    plt.ylabel('Section_Metric')
    plt.tight_layout()
    plt.savefig(f'heatmap_comparison_{section}_{metric_type.lower()}.png', dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Heatmap for {section} - {metric_type} saved successfully as 'heatmap_comparison_{section}_{metric_type.lower()}.png'.")

def plot_grouped_bar_charts(baseline_results, peft_results, standard_results):
    """
    Create grouped bar charts for Average metrics, faceted by section.
    Each group represents a metric, and each bar within the group represents a model.
    """
    # Prepare data in long-form DataFrame
    data = []
    models = ['Baseline', 'PEFT T5', 'Standard T5']
    model_results = [baseline_results, peft_results, standard_results]

    for model, results in zip(models, model_results):
        for section, metrics in results.items():
            for metric, value in metrics.items():
                # Process only 'Average' metrics
                if metric.startswith('Average'):
                    metric_type = 'Average'
                    metric_name = metric.replace('Average ', '')
                else:
                    continue  # Skip non-Average metrics

                # Append to data list
                data.append({
                    'Section': section.capitalize(),
                    'Metric_Type': metric_type,
                    'Metric_Name': metric_name,
                    'Model': model,
                    'Value': value
                })

    df = pd.DataFrame(data)

    # Debug: Check the structure of the DataFrame
    print("Grouped Bar Charts DataFrame Head:")
    print(df.head())

    # Set the aesthetic style of the plots
    sns.set(style="whitegrid")

    # Create a FacetGrid for Metric_Type and Section
    g = sns.catplot(
        data=df,
        kind="bar",
        x="Metric_Name",
        y="Value",
        hue="Model",
        col="Section",
        row="Metric_Type",
        palette="muted",
        height=4,
        aspect=1.2,
        sharey=False
    )

    # Adjust the titles and labels
    g.set_axis_labels("Metric", "Metric Value (%)")
    g.set_titles("{row_name} - {col_name}")

    # Rotate x-axis labels for better readability
    for ax in g.axes.flatten():
        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')

    plt.tight_layout()
    plt.savefig('grouped_bar_charts_average.png', dpi=300, bbox_inches='tight')
    plt.close()

    print("Grouped bar charts for Average metrics have been generated and saved as 'grouped_bar_charts_average.png'.")

def plot_rough_score(baseline_results, peft_results, standard_results):
    """
    Calculate and plot Rough Scores for each model across all sections.
    Rough Score is defined as the mean of all Average metrics within a section for each model.
    """
    # Define models
    models = ['Baseline', 'PEFT T5', 'Standard T5']
    model_results = [baseline_results, peft_results, standard_results]

    # Prepare data for Rough Score
    rough_score_data = []
    for model, results in zip(models, model_results):
        for section, metrics in results.items():
            # Extract all 'Average' metrics
            average_metrics = [value for key, value in metrics.items() if key.startswith('Average')]
            # Compute Rough Score as the mean of 'Average' metrics
            if average_metrics:
                rough_score = np.nanmean(average_metrics)
            else:
                rough_score = np.nan
            rough_score_data.append({
                'Section': section.capitalize(),
                'Model': model,
                'Rough_Score': rough_score
            })

    df_rough = pd.DataFrame(rough_score_data)

    # Debug: Check the structure of the Rough Score DataFrame
    print("Rough Score DataFrame Head:")
    print(df_rough.head())

    # Set the aesthetic style of the plots
    sns.set(style="whitegrid")

    # Create a grouped bar chart
    plt.figure(figsize=(12, 8))
    sns.barplot(
        data=df_rough,
        x='Section',
        y='Rough_Score',
        hue='Model',
        palette='muted'
    )

    plt.title('Rough Score Comparison Across Models and Sections', size=16)
    plt.xlabel('Section')
    plt.ylabel('Rough Score (Average of Metrics) (%)')
    plt.xticks(rotation=45, ha='right')
    plt.legend(title='Model')
    plt.tight_layout()
    plt.savefig('rough_score_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

    print("Rough score comparison chart has been generated and saved as 'rough_score_comparison.png'.")

def main():
    """
    Main function to load results, process data, and generate visualizations.
    """
    base_path = '/content/drive/MyDrive/Dissertation_review/model_training/'
    baseline_results = load_results(f'{base_path}baseline all results only.txt')
    peft_results = load_results(f'{base_path}peft t5 all results only.txt')
    standard_results = load_results(f'{base_path}standard t5 all results only.txt')

    print("Available sections in baseline results:", list(baseline_results.keys()))
    print("Available sections in PEFT results:", list(peft_results.keys()))
    print("Available sections in standard results:", list(standard_results.keys()))

    # Find common sections across all results
    sections = list(set(baseline_results.keys()) & set(peft_results.keys()) & set(standard_results.keys()))
    print("Common sections across all results:", sections)

    for section in sections:
        print(f"\nProcessing section: {section}")

        # Process only 'Average' metrics
        metric_type = 'Average'
        baseline_metrics = [key for key in baseline_results[section].keys() if key.startswith(metric_type)]
        peft_metrics = [key for key in peft_results[section].keys() if key.startswith(metric_type)]
        standard_metrics = [key for key in standard_results[section].keys() if key.startswith(metric_type)]

        print(f"Debug: Baseline {metric_type} metrics: {baseline_metrics}")
        print(f"Debug: PEFT {metric_type} metrics: {peft_metrics}")
        print(f"Debug: Standard {metric_type} metrics: {standard_metrics}")

        # Find common metrics across all models for the current section
        common_metrics = list(set(baseline_metrics) & set(peft_metrics) & set(standard_metrics))
        common_metrics.sort()  # Ensure consistent ordering

        print(f"Common {metric_type} metrics: {common_metrics}")
        print(f"Number of common {metric_type} metrics: {len(common_metrics)}")

        if not common_metrics:
            print(f"No common {metric_type} metrics found for section {section}. Skipping.")
            continue

        # Extract metric values for each model
        baseline_values = [baseline_results[section][m] for m in common_metrics]
        peft_values = [peft_results[section][m] for m in common_metrics]
        standard_values = [standard_results[section][m] for m in common_metrics]

        print(f"Length of {metric_type} baseline values: {len(baseline_values)}")
        print(f"Length of {metric_type} peft values: {len(peft_values)}")
        print(f"Length of {metric_type} standard values: {len(standard_values)}")

        # Validate that all models have the same number of metrics
        if len(common_metrics) != len(baseline_values) or len(common_metrics) != len(peft_values) or len(common_metrics) != len(standard_values):
            print(f"Error: Mismatch in number of {metric_type} metrics and values for section {section}")
            continue

        # Remove the "Average" prefix from metric names for cleaner labels
        plot_metrics = [m.split(f'{metric_type} ')[-1] for m in common_metrics]

        try:
            # Plot individual heatmaps for each section and metric type
            plot_heatmap(plot_metrics, baseline_values, peft_values, standard_values, section, metric_type)
        except ValueError as e:
            print(f"Error in plot_heatmap: {str(e)}")
            print(f"Plot metrics: {plot_metrics}")
            print(f"Baseline values: {baseline_values}")
            print(f"PEFT values: {peft_values}")
            print(f"Standard values: {standard_values}")

        # Print statistical comparison
        print(f"\n{metric_type} Statistical Comparison for {section.capitalize()} Section:")
        for metric, baseline, peft, standard in zip(common_metrics, baseline_values, peft_values, standard_values):
            metric_name = metric.split(f'{metric_type} ')[-1]
            print(f"{metric_name}:")
            print(f"  Baseline: {baseline:.2f}%")
            print(f"  PEFT T5: {peft:.2f}%")
            print(f"  Standard T5: {standard:.2f}%")

    # Generate Grouped Bar Charts for Average Metrics
    print("\nGenerating Grouped Bar Charts for Average Metrics...")
    plot_grouped_bar_charts(baseline_results, peft_results, standard_results)


    # Generate Rough Score Comparison Chart
    print("\nGenerating Rough Score Comparison Chart...")
    plot_rough_score(baseline_results, peft_results, standard_results)

    print("\nAll plots have been generated and saved as PNG files.")

if __name__ == "__main__":
    main()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

def load_results(filename):
    results = {}
    current_section = None
    with open(filename, 'r') as f:
        for line in f:
            if line.startswith("Summary Statistics for"):
                current_section = line.split("for")[1].split("Model:")[0].strip().lower()
                results[current_section] = {}
            elif ":" in line and current_section:
                parts = line.strip().split(': ', 1)
                if len(parts) == 2:
                    key, value = parts
                    try:
                        results[current_section][key] = float(value)
                    except ValueError:
                        results[current_section][key] = np.nan
    return results

def calculate_section_impact(results):
    section_impact = {}
    for section, metrics in results.items():
        avg_metrics = [value for key, value in metrics.items() if key.startswith('Average')]
        if avg_metrics:
            section_impact[section] = np.mean(avg_metrics)
    return section_impact

def plot_section_impact(baseline_impact, peft_impact, standard_impact):
    sections = list(set(baseline_impact.keys()) | set(peft_impact.keys()) | set(standard_impact.keys()))

    data = []
    for section in sections:
        data.append({
            'Section': section.capitalize(),
            'Baseline': baseline_impact.get(section, 0),
            'PEFT T5': peft_impact.get(section, 0),
            'Standard T5': standard_impact.get(section, 0)
        })

    df = pd.DataFrame(data)
    df_melted = df.melt(id_vars=['Section'], var_name='Model', value_name='Impact Score')

    plt.figure(figsize=(12, 6))
    sns.barplot(x='Section', y='Impact Score', hue='Model', data=df_melted)
    plt.title('Impact of Each Section on Review Generation', fontsize=16)
    plt.xlabel('Section', fontsize=12)
    plt.ylabel('Impact Score (Average of Metrics)', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.legend(title='Model', title_fontsize='12', fontsize='10')
    plt.tight_layout()
    plt.savefig('section_impact_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

    print("Section impact comparison chart has been generated and saved as 'section_impact_comparison.png'.")

    # Find the section with the highest impact for each model
    for model in ['Baseline', 'PEFT T5', 'Standard T5']:
        max_impact_section = df[['Section', model]].sort_values(by=model, ascending=False).iloc[0]
        print(f"The section with the highest impact for {model} is '{max_impact_section['Section']}' with an impact score of {max_impact_section[model]:.2f}")

def main():
    base_path = '/content/drive/MyDrive/Dissertation_review/model_training/'
    baseline_results = load_results(f'{base_path}baseline all results only.txt')
    peft_results = load_results(f'{base_path}peft t5 all results only.txt')
    standard_results = load_results(f'{base_path}standard t5 all results only.txt')

    baseline_impact = calculate_section_impact(baseline_results)
    peft_impact = calculate_section_impact(peft_results)
    standard_impact = calculate_section_impact(standard_results)

    plot_section_impact(baseline_impact, peft_impact, standard_impact)

if __name__ == "__main__":
    main()

import re
import matplotlib.pyplot as plt

def extract_data(file_path):
    with open(file_path, 'r') as file:
        content = file.read()

    sections = ['abstract', 'introduction', 'methods', 'related work', 'results', 'conclusion']
    data = {section: {'epoch': [], 'loss': [], 'grad_norm': []} for section in sections}

    for section in sections:
        pattern = rf"Processing {section} section(.*?)(?:Processing|$)"
        section_content = re.search(pattern, content, re.DOTALL)

        if section_content:
            lines = section_content.group(1).strip().split('\n')
            for line in lines:
                if line.startswith('{') and line.endswith('}'):
                    try:
                        values = eval(line)
                        if 'epoch' in values and 'loss' in values and 'grad_norm' in values:
                            data[section]['epoch'].append(values['epoch'])
                            data[section]['loss'].append(values['loss'])
                            data[section]['grad_norm'].append(values['grad_norm'])
                    except:
                        pass

    return data

def plot_graphs(data):
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 16))

    for section, values in data.items():
        ax1.plot(values['epoch'], values['loss'], label=f'{section} Loss')
        ax2.plot(values['epoch'], values['grad_norm'], label=f'{section} Grad Norm')

    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training Loss')
    ax1.legend()
    ax1.grid(True)

    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Gradient Norm')
    ax2.set_title('Gradient Norm')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.show()

# Usage
file_path = '/content/drive/MyDrive/Dissertation_review/model_training/t5.output'  # Replace with the path to your t5.output file
data = extract_data(file_path)
plot_graphs(data)

import re
import matplotlib.pyplot as plt

def extract_data(file_path):
    with open(file_path, 'r') as file:
        content = file.read()

    sections = ['abstract', 'introduction', 'methods', 'related work', 'results', 'conclusion']
    data = {section: {'epoch': [], 'loss': [], 'grad_norm': []} for section in sections}

    for section in sections:
        pattern = rf"Processing {section} section(.*?)(?:Processing|$)"
        section_content = re.search(pattern, content, re.DOTALL)

        if section_content:
            lines = section_content.group(1).strip().split('\n')
            for line in lines:
                if line.startswith('{') and line.endswith('}'):
                    try:
                        values = eval(line)
                        if 'epoch' in values and 'loss' in values and 'grad_norm' in values:
                            data[section]['epoch'].append(values['epoch'])
                            data[section]['loss'].append(values['loss'])
                            data[section]['grad_norm'].append(values['grad_norm'])
                    except:
                        pass

    return data

def plot_graphs(data):
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 16))

    for section, values in data.items():
        ax1.plot(values['epoch'], values['loss'], label=f'{section} Loss')
        ax2.plot(values['epoch'], values['grad_norm'], label=f'{section} Grad Norm')

    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training Loss')
    ax1.legend()
    ax1.grid(True)

    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Gradient Norm')
    ax2.set_title('Gradient Norm')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.show()

# Usage
file_path = '/content/drive/MyDrive/Dissertation_review/model_training/peft t5.txt'  # Replace with the path to your t5.output file
data = extract_data(file_path)
plot_graphs(data)