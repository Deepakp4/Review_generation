# -*- coding: utf-8 -*-
"""2model-rev-genAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lKn27MJfrW7_pDPkq47drcxgSinL6Z9n
"""

# Importing required libraries
import torch
import os


'''from google.colab import drive
drive.mount('/content/drive')'''

#!pip install jsonlines nltk scikit-learn rouge-score bert_score datasets

import torch
import json
import jsonlines
import pandas as pd
import nltk
import re
from transformers import EarlyStoppingCallback
import numpy as np
from nltk.tokenize import sent_tokenize, word_tokenize
from rouge_score import rouge_scorer
from transformers import T5ForConditionalGeneration, T5Tokenizer, TrainingArguments, Trainer
from peft import PromptTuningConfig, TaskType, get_peft_model
from tqdm import tqdm
import warnings
import logging
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from math import pi

# Suppressing warnings and logging
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)

# # Checking for GPU if not CPU 
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

# Downloading NLTK data
nltk.download('punkt', quiet=True)

# Loading train data
train_data = []
with jsonlines.open('train.jsonl', mode='r') as reader:
    for obj in reader:
        train_data.append(obj)
train = pd.DataFrame(train_data)

# Loading validation data
validation_data = []
with jsonlines.open('validation.jsonl', mode='r') as reader:
    for obj in reader:
        validation_data.append(obj)
val = pd.DataFrame(validation_data)

# Loading test data
test_data = []
with jsonlines.open('test.jsonl', mode='r') as reader:
    for obj in reader:
        test_data.append(obj)
test = pd.DataFrame(test_data)

# Same functions from baseline model to clean and extract text
def clean_text(text):
    if not isinstance(text, str):
        text = str(text)
    text = re.sub(r'[\uD800-\uDFFF]', '', text)
    text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')
    text = re.sub(r'\s+', ' ', text)
    text = text.strip().lower()
    words = text.split()
    return ' '.join(words)

def extract_section(content, section_name):
    try:
        section_text = ''
        section_keywords = {
            'abstract': ['abstract'],
            'introduction': ['introduction', '1 introduction'],
            'methods': ['method', 'methodology', 'experimental', 'experiments'],
            'related work': ['related work', 'background', 'literature review'],
            'results': ['result', 'finding', 'results'],
            'conclusion': ['conclusion', 'discussion']
        }

        for section in content:
            heading = section.get('heading', '')
            if heading is None:
                heading = ''

            normalized_heading = heading.lower().strip()

            if any(keyword in normalized_heading for keyword in section_keywords[section_name]):
                text = section.get('text', '')
                if not isinstance(text, str):
                    text = str(text) if text is not None else ''
                section_text = clean_text(text)
                break

        return section_text if section_text else f'No {section_name.capitalize()} Found'
    except Exception as e:
        return f'Error extracting {section_name}: {str(e)}'

def select_random_sentences(document, num_sentences=20):
    sentences = sent_tokenize(document)
    if len(sentences) <= num_sentences:
        return document
    else:
        return ' '.join(np.random.choice(sentences, num_sentences, replace=False))

def evaluate_rouge(generated_review, reference_review):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference_review, generated_review)
    return {
        'ROUGE-1': scores['rouge1'].fmeasure * 100,
        'ROUGE-2': scores['rouge2'].fmeasure * 100,
        'ROUGE-L': scores['rougeL'].fmeasure * 100
    }

def evaluate_additional_metrics(predictions, references):
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    from nltk.translate.meteor_score import meteor_score

    bleu_scores = []
    meteor_scores = []
    f1_scores = []
    precision_scores = []
    recall_scores = []

    smoothie = SmoothingFunction().method4

    for pred, ref in zip(predictions, references):
        pred_tokens = word_tokenize(pred)
        ref_tokens = [word_tokenize(ref)]

        # BLEU score
        bleu = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothie)
        bleu_scores.append(bleu)

        # METEOR score
        meteor = meteor_score(ref_tokens, pred_tokens)
        meteor_scores.append(meteor)

        # Precision, Recall, F1 (token-level)
        pred_set = set(pred_tokens)
        ref_set = set(word_tokenize(ref))
        common_tokens = pred_set.intersection(ref_set)
        if len(pred_set) > 0:
            precision = len(common_tokens) / len(pred_set)
        else:
            precision = 0
        recall = len(common_tokens) / len(ref_set) if len(ref_set) > 0 else 0
        if precision + recall > 0:
            f1 = 2 * (precision * recall) / (precision + recall)
        else:
            f1 = 0

        precision_scores.append(precision)
        recall_scores.append(recall)
        f1_scores.append(f1)

    metrics = {
        'BLEU': np.mean(bleu_scores),
        'METEOR': np.mean(meteor_scores),
        'Precision': np.mean(precision_scores),
        'Recall': np.mean(recall_scores),
        'F1': np.mean(f1_scores)
    }

    return metrics

# Preparing the dataset
def prepare_dataset(df, sections, num_sentences=20):
    inputs = []
    targets = []
    sections_list = []

    for _, row in df.iterrows():
        combined_text = []
        for section in sections:
            if section == 'abstract':
                section_text = row['abstract']
                if section_text:
                    combined_text.append(section_text)
            else:
                section_text = extract_section(row['content'], section)
                if section_text and not section_text.startswith('No') and not section_text.startswith('Error'):
                    selected_sentences = select_random_sentences(section_text, num_sentences=num_sentences)
                    combined_text.append(selected_sentences)
        if combined_text:
            input_text = ' '.join(combined_text)
            inputs.append(f"summarize: {input_text}")
            targets.append(row['review'])
            sections_list.append(sections)  # Assuming all sections are used
    return inputs, targets, sections_list

def preprocess_function(examples, tokenizer):
    # Clean input texts
    examples["input_text"] = [clean_text(text) for text in examples["input_text"]]
    examples["review"] = [clean_text(text) for text in examples["review"]]
    # Tokenize the inputs
    model_inputs = tokenizer(
        examples["input_text"],
        max_length=512,
        truncation=True,
        padding="max_length"
    )
    # Tokenize the reviews
    labels = tokenizer(
        examples["review"],
        max_length=400,
        truncation=True,
        padding="max_length"
    )
    # Replacing all pad token ids in labels by -100 to ignore padding in loss
    labels["input_ids"] = [
        [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]
        for labels_seq in labels["input_ids"]
    ]
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

def evaluate_model(model, tokenizer, test_df, sections, output_file):
    rouge_scores = {'ROUGE-1': [], 'ROUGE-2': [], 'ROUGE-L': []}
    additional_metrics_list = []
    generated_reviews = []
    reference_reviews = []

    model.to(device)
    model.eval()

    with open(output_file, 'a') as f:
        for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):
            combined_text = []
            for section in sections:
                if section == 'abstract':
                    section_text = row['abstract']
                    if section_text:
                        combined_text.append(section_text)
                else:
                    section_text = extract_section(row['content'], section)
                    if section_text and not section_text.startswith('No') and not section_text.startswith('Error'):
                        selected_sentences = select_random_sentences(section_text, num_sentences=20)
                        combined_text.append(selected_sentences)
            if not combined_text:
                f.write(f"Warning: No valid sections found for document {idx + 1}\n")
                continue
            input_text = ' '.join(combined_text)
            input_text = f"summarize: {input_text}"

            # Generate review
            input_ids = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True).input_ids.to(device)
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=input_ids,
                    max_length=300, 
                    num_beams=4,
                    early_stopping=True,
                    no_repeat_ngram_size=3,
                    do_sample=False,  # Disabled sampling for deterministic outputs
                    num_return_sequences=1
                )
            generated_review = tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_reviews.append(generated_review)
            reference_review = row['review']
            reference_reviews.append(reference_review)

            # Evaluate ROUGE
            rouge_score = evaluate_rouge(generated_review, reference_review)
            for key in rouge_scores:
                rouge_scores[key].append(rouge_score[key])

            # Evaluate additional metrics
            additional_metrics = evaluate_additional_metrics([generated_review], [reference_review])
            additional_metrics_list.append(additional_metrics)

            # Write to file
            f.write(f"\nDocument {idx + 1}:\n")
            f.write(f"Generated Review: {generated_review[:200]}...\n")
            f.write(f"Reference Review: {reference_review[:200]}...\n")
            f.write(f"ROUGE Scores: {rouge_score}\n")
            f.write(f"Additional Metrics: {additional_metrics}\n")

    # Compute average and max scores
    avg_rouge = {key: np.mean(scores) for key, scores in rouge_scores.items()}
    max_rouge = {key: np.max(scores) for key, scores in rouge_scores.items()}

    # Calculate average and max additional metrics
    avg_additional_metrics = {}
    max_additional_metrics = {}
    if additional_metrics_list:
        for key in additional_metrics_list[0].keys():
            values = [metrics[key] for metrics in additional_metrics_list]
            avg_additional_metrics[key] = np.mean(values)
            max_additional_metrics[key] = np.max(values)
    else:
        for key in ['BLEU', 'METEOR', 'Precision', 'Recall', 'F1']:
            avg_additional_metrics[key] = 0
            max_additional_metrics[key] = 0

    with open(output_file, 'a') as f:
        f.write(f"\nSummary Statistics for Evaluated Model:\n")
        f.write(f"Number of documents processed: {len(generated_reviews)}\n")
        for key in rouge_scores.keys():
            f.write(f"Average {key}: {avg_rouge[key]}\n")
            f.write(f"Maximum {key}: {max_rouge[key]}\n")
        for key in avg_additional_metrics.keys():
            f.write(f"Average {key}: {avg_additional_metrics[key]}\n")
            f.write(f"Maximum {key}: {max_additional_metrics[key]}\n")

    # Prepare results dictionary
    results = {
        'Number of documents processed': len(generated_reviews),
        'Mean ROUGE-1': avg_rouge['ROUGE-1'],
        'Max ROUGE-1': max_rouge['ROUGE-1'],
        'Mean ROUGE-2': avg_rouge['ROUGE-2'],
        'Max ROUGE-2': max_rouge['ROUGE-2'],
        'Mean ROUGE-L': avg_rouge['ROUGE-L'],
        'Max ROUGE-L': max_rouge['ROUGE-L'],
        'Mean BLEU': avg_additional_metrics['BLEU'],
        'Max BLEU': max_additional_metrics['BLEU'],
        'Mean METEOR': avg_additional_metrics['METEOR'],
        'Max METEOR': max_additional_metrics['METEOR'],
        'Mean Precision': avg_additional_metrics['Precision'],
        'Max Precision': max_additional_metrics['Precision'],
        'Mean Recall': avg_additional_metrics['Recall'],
        'Max Recall': max_additional_metrics['Recall'],
        'Mean F1': avg_additional_metrics['F1'],
        'Max F1': max_additional_metrics['F1'],
        'ROUGE-1 Scores': rouge_scores['ROUGE-1'],
        'ROUGE-2 Scores': rouge_scores['ROUGE-2'],
        'ROUGE-L Scores': rouge_scores['ROUGE-L']
    }

    return results

# Fine tuning function
def fine_tune_and_evaluate_section(train_df, val_df, test_df, section, output_dir):
    train_sampled = train_df.sample(n=10000, random_state=42)
    train_inputs, train_targets, _ = prepare_dataset(train_sampled, [section], num_sentences=20)
    val_inputs, val_targets, _ = prepare_dataset(val_df, [section], num_sentences=20)

    train_dataset = Dataset.from_dict({"input_text": train_inputs, "review": train_targets})
    val_dataset = Dataset.from_dict({"input_text": val_inputs, "review": val_targets})

    tokenizer = T5Tokenizer.from_pretrained("t5-base")
    model = T5ForConditionalGeneration.from_pretrained("t5-base")

    tokenized_train = train_dataset.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=train_dataset.column_names)
    tokenized_val = val_dataset.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=val_dataset.column_names)

    # PEFT configuration
    peft_config = LoraConfig(
        task_type=TaskType.SEQ_2_SEQ_LM,
        r=8,
        lora_alpha=32,
        lora_dropout=0.1,
    )

    # Apply PEFT to the model
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    training_args = TrainingArguments(
        output_dir=f"{output_dir}/{section}",
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        gradient_accumulation_steps=1,
        learning_rate=3e-5,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir=f"{output_dir}/{section}/logs",
        logging_steps=100,
        evaluation_strategy="steps",
        eval_steps=500,
        save_steps=500,
        load_best_model_at_end=False,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=torch.cuda.is_available(),
        max_grad_norm=1.0,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
    )

    trainer.train()

    model_save_path = f"{output_dir}/{section}/fine_tuned_model"
    model.save_pretrained(model_save_path)
    tokenizer.save_pretrained(model_save_path)
    
    print(f"Fine-tuned model for {section} saved to {model_save_path}")

    # Evaluate the model
    model.eval()
    results = evaluate_model(model, tokenizer, test_df, [section], f"{output_dir}/{section}_results.txt")
    
    return results

def main():
    # Define sections to process
    sections = ['abstract', 'introduction', 'methods', 'related work', 'results', 'conclusion']
    output_dir = "standard_t5_results"  # Changed directory name to reflect standard fine-tuning
    os.makedirs(output_dir, exist_ok=True)

    all_results = {}

    for section in sections:
        print(f"\nProcessing {section} section")
        results = fine_tune_and_evaluate_section(train, val, test, section, output_dir)
        all_results[section] = results

    # Save all results
    with open(f"{output_dir}/all_sections_results.json", 'w') as f:
        json.dump(all_results, f, indent=2)

    print(f"\nAll section results saved to {output_dir}/all_sections_results.json")

if __name__ == "__main__":
    main()
