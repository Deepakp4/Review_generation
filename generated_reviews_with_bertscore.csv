paper_id,title,metadata,introduction,generated_review,bert_precision,bert_recall,bert_f1
ICLR_2019_47,"Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search","{'source': 'CRF', 'title': None, 'authors': ['Lars Buesing', 'Théophane Weber', 'Yori Zwols', 'Sébastien Racanière', 'Arthur Guez', 'Jean-Baptiste Lespiau', 'Nicolas Heess'], 'emails': ['lbuesing@google.com'], 'sections': [{'heading': '1 INTRODUCTION', 'text': 'Imagine that a month ago Alice had two job offers from companies a1 and a2. She decided to join a1 because of the larger salary, in spite of an awkward feeling during the job interview. Since then she learned a lot about a1 and recently received information about a2 from a friend, prodding her now to imagine what would have happened had she joined a2. Re-evaluating her decision in hindsight in this way, she concludes that she made a regrettable decision. She could and should have known that a2 was a better choice, had she only interpreted the cues during the interview correctly... This example tries to illustrate the everyday human capacity to reason about alternate, counterfactual outcomes of past experience with the goal of “mining worlds that could have been” (Pearl & Mackenzie, 2018). Social psychologists theorize that such cognitive processes are beneficial for improving future decision making (Roese, 1997). In this paper we aim to leverage possible advantages of counterfactual reasoning for learning decision making in the reinforcement learning (RL) framework.\nIn spite of recent success, learning policies with standard, model-free RL algorithms can be notoriously data inefficient. This issue can in principle be addressed by learning policies on data synthesized from a model. However, a mismatch between the model and the true environment, often unavoidable in practice, can cause this approach to fail (Talvitie, 2014), resulting in policies that do not generalize to the real environment (Jiang et al., 2015). Motivated by the introductory example, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm: Instead of relying on data synthesized from scratch by a model, policies are trained on model predictions of alternate outcomes of past experience from the true environment under counterfactual actions, i.e. actions that had not actually been taken, while everything else remaining the same (Pearl, 2009). At the heart\nof CF-GPS are structural causal models (SCMs) which model the environment with two ingredients (Wright, 1920): 1) Independent random variables, called scenarios here, summarize all aspects of the environment that cannot be influenced by the agent, e.g. the properties of the companies in Alice’s job search example. 2) Deterministic transition functions (also called causal mechanisms) take these scenarios, together with the agent’s actions, as input and produce the predicted outcome. The central idea of CF-GPS is that, instead of running an agent on scenarios sampled de novo from a model, scenarios are inferred in hindsight from given off-policy data, and then evaluate and improve the agent on these specific scenarios using given or learned causal mechanisms (Balke & Pearl, 1994). CF-GPS can be regarded as a meta-algorithm that extends a given model-base RL algorithm by “grounding” or “anchoring” model-based predictions in inferred scenarios. As a result, this approach explicitly allows to trade-off historical data for model bias. We show empirically in a conceptually simple setting, where unknown initial states are inferred in hindsight and re-used to evalute to counterfactual actions, that this can mitigate model mismatch. CF-GPS differs substantially from standard off-policy RL algorithms based on Importance Sampling (IS), where historical data is re-weighted with respect to the importance weights to evaluate or learn new policies (Precup, 2000). In contrast, CF-GPS explicitly reasons counterfactually about given off-policy data. Our main contributions are:\n1. We formulate model-based RL in POMDPs in terms of structural causal models, thereby connecting concepts from reinforcement learning and causal inference.\n2. We provide the first results, to the best of our knowledge, showing that counterfactual reasoning in structural causal models on off-policy data can facilitate solving non-trivial RL tasks.\n3. We show that two previously proposed classes of RL algorithms, namely Guided Policy Search (Levine & Koltun, 2013) and Stochastic Value Gradient methods (Heess et al., 2015), can be interpreted as counterfactual methods, opening up possible generalizations.\nThe paper is structured as follows. We first give a self-contained, high-level recapitulation of structural causal models and counterfactual inference, as these are less widely known in the RL and generative model communities. In particular we show how to model POMDPs with SCMs. Based on this exposition, we first consider the task of policy evaluation and discuss how we can leverage counterfactual inference in SCMs to improve over standard model-based methods. We then generalize this approach to the policy search setting resulting in the CF-GPS algorithm. We close by highlighting connections to previously proposed algorithms and by discussing assumptions and limitations of the proposed method.'}, {'heading': '2 PRELIMINARIES', 'text': 'We denote random variables (RVs) with capital letters, e.g. X , and particular values with lower caps, e.g. x. For a distribution P over a vector-valued random variable X , we denote the marginal over Y ⊂ X by PY (and density pY ); however we often omit the subscript if it is clear from the context, e.g. as in Y ∼ P . We assume the episodic, Partially Observable Markov Decision Process (POMDP) setting with states St, actions At and observations Ot, for t = 1, . . . , T . For ease of notation, we assume that Ot includes the reward Rt. The undiscounted return is denoted by G = ∑T t=1Rt. We consider stochastic policies π(at|ht) over actions conditioned on observation histories Ht = (O1, A1, . . . , At−1, Ot). We denote the resulting distribution over trajectories T = (S1, O1, A1, . . . , AT−1, ST , OT ) induced by running π in the environment with T ∼ Pπ and the corresponding density by pπ(τ).'}, {'heading': '2.1 STRUCTURAL CAUSAL MODELS', 'text': 'Definition 1 (Structural causal model). A structural causal model (SCM) M over X = (X1, . . . , XN ) is given by a DAG G over nodes X , independent noise RVs U = (U1, . . . , UN ) with distributions PUi and functions f1, . . . , fN such that Xi = fi(pai, Ui), where pai ⊂ X are the parents of Xi in G. An SCM entails a distribution P with density p over (X,U).\nWe also refer to U as scenarios and to fi as causal mechanisms. We give a (broad) definition of an intervention in an SCM. This also includes what is known as stochastic interventions or mechanism changes (Korb et al., 2004) which generalize atomic interventions (Pearl, 2009).\nDefinition 2 (Intervention in SCM). An intervention I in an SCMM consists of replacing some of the original fi(pai, Ui) with other functions f I i (pa I i , Ui) where pa I i are the parents in a new DAG GI . We denote the resulting SCM withMdo(I) with distribution P do(I) and density pdo(I).\nSCM representation of POMDPs We can represent any given POMDP (under a policy π) by an SCMM over trajectories T in the following way. We express all conditional distributions, e.g. the transition kernel PSt+1|St,At , as deterministic functions with independent noise variables U , such as St+1 = fst(St, At, Ust). This is always possible using auto-regressive uniformization, see Lemma 2 in the appendix. The DAG G of the resulting SCM is shown in fig. 1. This procedure is closely related to the ‘reparameterization trick’ for models with location-scale distributions (Kingma & Welling, 2013; Rezende et al., 2014). We denote the distribution over T entailed by the SCM with Pπ and its density by pπ to highlight the role of π; note the difference to the true environment distribution Pπ with density pπ . Running a different policy µ instead of π in the environment can be expressed as an intervention I(π → µ) consisting of replacing At = fπ(Ht, Uat) by At = fµ(Ht, Uat). We denote the resulting model distribution over trajectories with P do(I(π→µ)) = Pµ (analogously pµ).\nIntuition Here, we illustrate the main advantage of SCMs using the example of Alice’s job choice from the introduction. We model it as contextual bandit with feedback shown in fig. 1. Alice has some initial knowledge given by the context Uc that is available to her before taking action A of joining company A = a1 or A = a2. We model Alice’s decision as A = fπ(Uc, Ua), where Ua captures potential indeterminacy in Alice’s decision making. The outcome O = fo(A,Uc, Uo) also depends on the scenario Uo, capturing all relevant, unobserved and highly complex properties of the two companies such as working conditions etc. Given this model, we can reason about alternate outcomes fo(a1, uc, uo) and fo(a2, uc, uo) for same the scenario uo. This is not possible if we only model the outcome on the level of the conditional distribution PO|A,Uc .'}, {'heading': '2.2 COUNTERFACTUAL INFERENCE IN SCMS', 'text': 'For an SCM over X , we define a counterfactual query as a triple (x̂o, I,Xq) of observations x̂o of some variablesXo ⊂ X , an intervention I and query variablesXq ⊂ X . The semantics of the query are that, having observed x̂o, we want to infer what Xq would have been had we done intervention I , while ‘keeping everything else the same’. Counterfactual inference (CFI) in SCMs answers the query in the following way (Balke & Pearl, 1994):\n1. Infer the unobserved noise source U conditioned on the observations x̂o, i.e. compute p(U |x̂o) and replace the prior p(U) with p(U |x̂o). Denote the resulting SCM byMx̂o .\n2. Perform intervention I on Mx̂o . This yields M do(I) x̂o\n, which entails the counterfactual distribution pdo(I)|x̂o(x). Return the marginal pdo(I)|x̂o(xq).\nNote that our definition explicitly allows for partial observations Xo ⊂ X in accordance with Pearl (2009). A sampled-based version, denoted as CFI, is presented in Algorithm 1. An interesting property of the counterfactual distribution pdo(I)|x̂o is that marginalizing it over observations x̂o yields an unbiased estimator of the density of Xq under intervention I . Lemma 1 (CFI for simulation). Let observations x̂o ∼ p come from a SCMM with density p. Then the counterfactual density pdo(I)|x̂o is an unbiased estimator of pdo(I), i.e.\nEx̂o∼p[pdo(I)|x̂o(x)] = pdo(I)(x)\nThe proof is straightforward and outlined in the Appendix A. This lemma and the marginal independence of the Ui leads to the following corollary; the proof is given in the appendix. Corollary 1 (Mixed counterfactual and prior simulation from an SCM). Assume we have observations x̂o ∼ p. We can simulate from M, under any intervention I , i.e. obtain unbiased samples from Mdo(I), by first sampling values uCF for an arbitrary subset UCF ⊂ U from the posterior p(uCF|x̂o) and the remaining UPrior := U\\UCF from the prior p(uPrior), and then computing X with noise u = uCF ∪ uPrior.\nThe corollary essentially states that we can sample from the modelMI , by sampling some of the Ui from the prior, and inferring the rest from data x̂o (as long as the latter was also sampled fromM). We will make use of this later for running a POMDP model on scenariosUst inferred from data while randomizing the action noise Uat. We note that the noise variables UCF from the posterior PUCF|x̂o are not independent anymore. Nevertheless, SCMs with non-independent noise distributions arising from counterfactual inference, denoted here by Mx̂o , are commonly considered in the literature (Peters et al., 2017).\nIntuition Returning to Alice’s job example from the introduction, we give some intuition for counterfactual inference in SCMs. Given the concrete outcome ô, under observed context ûc and having joined company â = a1, Alice can try to infer the underlying scenario uo ∼ p(uo|a1, ûc, ô) that she experiences; this includes factors such as work conditions etc. She can then reason counterfactually about the outcome had she joined the other company, which is given by fo(a2, ûc, uo). This can in principle enable her to make better decisions in the future in similar scenarios by changing her policy fπ(A,Uc, Ua) such that the action with the preferred outcome becomes more likely under ûc, uo. In particular she can do so without having to use her (likely imperfect) prior model over possible companies p(Uo). She can use the counterfactual predictions discussed above instead to learn from her experience. We use this insight for counterfactual policy evaluation and search below.'}, {'heading': '3 OFF-POLICY EVALUATION: MODEL-FREE, MODEL-BASED AND COUNTERFACTUAL', 'text': 'To explain how counterfactual reasoning in SCMs can be used for policy search, we first consider the simpler problem of policy evaluation (PE) on off-policy data. The goal of off-policy PE is to determine the value of a policy π, i.e. its expected return Epπ [G], without running the policy itself. We assume that we have data D = {ĥiT }i=1,...,N consisting of logged episodes ĥiT = (ô i 1, â i 1, . . . â i T−1, ô i T ) from running a behavior policy µ. A standard, model-free approach\nto PE is to use Importance sampling (IS): We can estimate the policy’s value as ∑ i w iĜi, where Ĝi is the empirical return of ĥiT and w i ∝ p π(ĥiT )\npµ(ĥiT ) are importance weights. However, if the trajectory\ndensities pπ and pµ are very different, then this estimator has large variance. In the extreme case, IS can be useless if the support of pµ does not contain that of pπ , irrespective of how much data from pµ is available.\nIf we have access to a model M, then we can evaluate the policy on synthetic data, i.e. we can estimate Epπ [G]. This is called model-based policy evaluation (MB-PE). However, any bias inM propagates from pπ to the estimate Epπ [G]. In the following, we assume thatM is a SCM and we\nAlgorithm 1 Counterfactual policy evaluation and search // Counterfactual inference (CFI)\n1: procedure CFI(data x̂o, SCMM, intervention I , query Xq) 2: û ∼ p(u|x̂o) . Sample noise variables from posterior 3: p(u)← δ(u− û) . Replace noise distribution in p with û 4: fi ← fIi . Perform intervention I 5: return xq ∼ pdo(I)(xq|û) . Simulate from the resulting modelMIx̂o 6: end procedure\n// Counterfactual Policy Evaluation (CF-PE) 7: procedure CF-PE(SCMM, policy π, replay buffer D, number of samples N ) 8: for i ∈ {1, . . . N} do 9: ĥiT ∼ D . Sample from the replay buffer 10: gi = CFI(ĥiT ,M, I(µ→ π), G) . Counterfactual evaluation of return 11: end for 12: return 1\nN ∑N i=1 gi\n13: end procedure\n// Counterfactually-Guided Policy Search (CF-GPS) 14: procedure CF-GPS(SCMM, initial policy π0, number of trajectory samples N ) 15: for k = 1, . . . do 16: if sometimes then 17: µ← πk . Update behavior policy 18: end if 19: for i = 1, . . . , N do 20: ĥiT ∼ pµ . Get off-policy data from the true environment 21: τ i = CFI(ĥiT ,M, I(µ→ πλ), T ) . Counterfactual rollouts under planner 22: end for 23: πk ← policy improvement on trajectories τ i=1,...,N using eqn. 1 24: end for 25: end procedure\nshow that we can use counterfactual reasoning for off-policy evaluation (CF-PE). As the main result for this section, we argue that we expect CF-PE to be less biased than MB-PE, and we illustrate this point with experiments.'}, {'heading': '3.1 COUNTERFACTUAL OFF-POLICY EVALUATION', 'text': 'Naive MB-PE with a SCMM simply consist of sampling the scenarios U ∼ PU from the prior, and then simulating a trajectory τ from the functions fi and computing its return. However, given data D from pµ, our discussion of counterfactual inference in SCMs suggests the following alternative strategy: Assuming no model mismatch, i.e. pµ = pµ, we can regard the task of off-policy evaluation of π as a counterfactual query with data ĥiT , intervention I(µ → π) and query variable G. In other words, instead of sampling from the prior as in MB-PE, we are free to the scenarios from the posterior ui ∼ pµ(·|ĥiT ). The algorithm is given in Algorithm 1. Lemma 1 guarantees that this results in an unbiased estimate: Corollary 2 (CF-PE is unbiased). Assuming no model mismatch, CF-PE is unbiased.\nFurthermore, Corollary 1 allows us to also sample some of the noise variables from the prior instead of the posterior, we can e.g. randomize the counterfactual actions by re-sampling the action noise Ua.\nMotivation When should one prefer CF-PE over the more straightforward MB-PE? Assuming a perfect model, Corollary 2 states that both yield the same answer in expectation for perfect models. For imperfect models however, these algorithms can differ substantially. MB-PE relies on purely synthetic data, sampled from the noise distribution p(U). In practice, this is usually approximated by a parametric density model, which can lead to under-fitting in case of complex distributions. This is a well-known effect in generative models with latent variables: In spite of recent research progress, e.g. models of natural images are still unable to accurately model the variability of the true data\n(Gregor et al., 2016). In contrast, CF-PE samples from the posterior N−1 ∑N i=1 p\nµ(U |ĥiT ), which has access to strictly more information than the prior p(U) by taking into account additional data ĥiT . This semi-nonparametric distribution can help to de-bias the model by effectively winnowing out parts of the domain of U which do not correspond to any real data. We substantiate this intuition with experiments below; a concrete illustration for the difference between the prior and posterior / counterfactual distribution is given in fig. 4 in the appendix and discussed in appendix D. Therefore, we conclude that we expect CF-PE to outperform MB-PE, if the transition and reward kernels fst are accurate models of the environment dynamics, but if the marginal distribution over the noise sources PU is difficult to model.'}, {'heading': '3.2 EXPERIMENTS', 'text': 'Environment As an example, we use a partially-observed variant of the SOKOBAN environment, which we call PO-SOKOBAN. The original SOKOBAN puzzle environment was described in detail by Racanière et al. (2017); we give a brief summary here. The agent is situated in a 10 × 10 grid world and its five actions are to move to one of four adjacent tiles and a NOOP. In our variant, the goal is to push all three boxes onto the three targets. As boxes cannot be pulled, many actions result irreversibly in unsolvable states. Episodes are of length T = 50, and pushing a box onto a target yields a reward of 1, removing a box from a target yields −1, and solving a level results in an additional reward of 10. The state of the environment consists in a 10× 10 matrix of categorical variables taking values in {0, . . . , 6} indicating if the corresponding tile is empty, a wall, box, target, agent, or a valid combinations thereof (box+target and agent+target). In order to introduce partial observability, we define the observations as the state corrupted by i.i.d. (for each tile and time step) flipping each categorical variable to the “empty” state with probability 0.9. Therefore, the state of the game is largely unobserved at any given time, and a successful agent has to integrate observations over tens of time steps. Initial states Us1, also called levels, which are the scenarios in this environment, are generated randomly by a generator algorithm which guarantees solvability (i.e. all boxes can be pushed onto targets). The environment is visualized in fig. 3 in the appendix.\nGiven the full state of PO-SOKOBAN, the transition kernel is deterministic and quite simple as only the agent and potentially an adjacent box moves. Inferring the belief state, i.e. the distribution over states given the history of observations and actions, can however range from trivial to very challenging, depending on the amount of available history. In the limit of a long observed history, every tile is eventually observed and the belief state concentrates on a single state (the true state) that can be easily inferred. With limited observed history however, inferring the posterior distribution over states (belief state) is very complex. Consider e.g. the situation in the beginning of an episode (before pushing the first box). Only the first observation is available, however we know that all POSOKOBAN levels are initially guaranteed to be solvable and therefore satisfy many combinatorial constraints reflecting that the agent is still able to push all boxes onto targets. Learning a compact parametric model of the initial state distribution from empirical data is therefore difficult and likely results in large mismatch between the learned model and the true environment.\nResults To illustrate the potential advantages of CF-PE over MB-PE we perform policy evaluation in the PO-SOKOBAN environment. We first generate a policy π that we wish to evaluate, by training it using a previously-proposed distributed RL algorithm (Espeholt et al., 2018). The policy is parameterized as a deep, recurrent neural network consisting of a 3-layer deep convolutional LSTM (Xingjian et al., 2015) with 32 channels per layer and kernel size of 3. To further increase computational power, the LSTM ticks twice for each environment step. The output of the agent is a value function and a softmax yielding the probabilities of taking the 5 actions. In order to obtain an SCM of the environment, for the sake of simplicity, we assume that the ground-truth transition, observation and reward kernels are given. Therefore the only part of the model that we need to learn is the distribution p(Us1) of initial states S1 = Us1 (for regular MB-PE), and the density p(Us1|ĥit) for inferring levels in hindsight for CF-PE. We vary the amount of true data t that we condition this inference on, ranging from t = 0 (no real data, equivalent to MB-PE) to t = T = 50 (a full episode of real data is used to infer the initial state Us1). We train a separate model for each t ∈ {0, 5, 10, 20, 30, 40, 50}. To simplify model learning, both models were given access to the unobserved state during training, but not at test time. The models are chosen to be powerful, multilayer, generative DRAW models (Gregor et al., 2015) trained by approximate maximum likelihood learning (Kingma & Welling, 2013; Rezende et al., 2014). The models take as input the (potentially\nempty) data ĥit summarized by a backward RNN (a standard convolutional LSTM model with 32 units). The model is shown in fig. 3 in the appendix and additional details are given in appendix C. The data ĥiT was collected under a uniform random policy µ. For all policy evaluations, we use ≈> 105 levels ui from the inferred model. In order to evaluate policies of different proficiency, we derive from the original (trained) π three policies π0, π1, π2 ranging from almost perfect to almost random performance by introducing additional stochasticity during action selection.\nThe policy evaluation results are shown in fig. 2. We found that for t = 0, in spite of extensive hyper-parameter search, the model p(Us1) was unable to accurately capture the marginal distribution of initial levels in PO-SOKOBAN. As argued above, a solvable level satisfies a large number of complex constraints that span the entire grid world, which are hard for a parametric model to capture. Empirically, we found that the model mismatch manifested itself in samples from p(Us1) not being well-formed, e.g. not solvable, and hence the performance of the policies πi are very different on these synthetic levels compared to levels sampled form p. However, inferring levels from full observed episodes i.e. p(Us1|ĥi50) was reliable, and running π on these resulted in accurate policy evaluation. The figure also shows the trade-off between policy evaluation accuracy and the amount of off-policy data for intermediate amounts of the data ĥit. We also want to emphasize that in this setting, model-free policy evaluation by IS fails. The uniform behavior policy µ was too different from πi, resulting in a relative error > 0.8 for all i = 1, 2, 3.'}, {'heading': '4 OFF-POLICY IMPROVEMENT: COUNTERFACTUALLY-GUIDED POLICY SEARCH', 'text': 'In the following we show how we can leverage the insights from counterfactual policy evaluation for policy search. We commence by considering a model-based RL algorithm and discuss how we can generalize it into a counterfactual algorithm to increase its robustness to model mismatch. We chose a particular algorithm to start from to make a connection to the previously proposed Guided Policy Search algorithm (Levine & Koltun, 2013; Levine & Abbeel, 2014), but we think a larger class of MBRL algorithms can be generalized in an analogous manner.'}, {'heading': '4.1 STARTING POINT: VANILLA MODEL-BASED RL WITH RETURN WEIGHTED REGRESSION', 'text': 'We start from the following algorithm. We assume we have a model M of the environment with trajectory distribution pπ . Our current policy estimate πk is improved at iteration k using returnweighted regression:\nπk+1 = arg max π\n∫ exp(G(τ))pπ k (τ) log pπ(τ) dτ,\nwhere G(τ) is the return of trajectory τ . This policy improvement step can be motivated by the framework of RL as variational inference (Toussaint, 2009) and is equivalent to minimizing the KL divergence to a trajectory distribution ∝ exp(G)pπk which puts additional mass on high-return trajectories. Although not strictly necessary for our exposition, we also allow for a dedicated proposal distribution over trajectories pλ(τ), under a policy λ. We refer to λ as a planner to highlight that it could consist of a procedure that solves episodes starting from arbitrary, full states s1 sampled form the model, by repeatedly calling the model transition kernel, e.g. a search procedure such as MCTS (Browne et al., 2012) or an expert policy. Concretely, we optimize the following finite sample objective:\nπk+1 = arg max π N∑ i=1 exp(Gi(τ i)) pπ k (τ i) pλ(τ i) log pπ(τ i), τ i ∼ pλ. (1)\nWe refer to this algorithm as model-based policy search (MB-PS). It is based on model rollouts τ i spanning entire episodes. An alternative would be to consider model rollouts starting from states visited in the real environment (if available). Both versions can be augmented by counterfactual methods, but for the sake of simplicity we focus on the simpler MB-PS version detailed above (also we did not find significant performance differences experimentally between both versions).'}, {'heading': '4.2 INCORPORATING OFF-POLICY DATA: COUNTERFACTUALLY-GUIDED POLICY SEARCH', 'text': 'Now, we assume that the model M is an SCM. Based on our discussion of counterfactual policy evaluation, it is straightforward to generalize the MB-PS described above by anchoring the rollouts τ i under the model pλ in off-policy data D: Instead of sampling τ i directly from the prior pλ, we draw them from counterfactual distribution pλ|ĥ\ni T with data ĥiT ∼ D from the replay buffer,\ni.e. instead of sampling the scenarios U from the prior we infer them from the given data. Again invoking Lemma 1, this procedure is unbiased under no model mismatch. We term the resulting algorithm Counterfactually-Guided Policy Search (CF-GPS), and it is summarized in Algorithm 1. The motivation for using CF-GPS over MB-PS is analogous to the advantage of CF-PE over MB-PE discussed in sec. 3.1. The policy π in CF-GPS is optimized on rollouts τ i that are grounded in data ĥiT by sampling them from the counterfactual distribution p\nλ|ĥiT instead of the prior pλ. If this prior is difficult to model, we expect the counterfactual distribution to be more concentrated in regions where there is actual mass under the true environment pλ.'}, {'heading': '4.3 EXPERIMENTS', 'text': 'We evaluate CF-GPS on the PO-SOKOBAN environment, using a modified distributed actor-learner architecture based on Espeholt et al. (2018): Multiple actors (here 64) collect real data ĥT by running the behavior policy µ in the true environment p. As in many distributed RL settings, µ is chosen to be a copy of the policy π, often slightly outdated, so the data must be considered to be offpolicy. The distribution p(Us1|ĥT ) over levels Us1 is inferred from the data ĥT using from the model M. We sample a scenario Us1 for each logged episode, and simulate 10 counterfactual trajectories τ1,...,10 under the planner λ for each such scenario. Here, for the sake of simplicity, instead of using search, the planner was assumed to be a mixture between π and a pre-trained expert policy λe, i.e. λ = βλe + (1− β)π. The schedule β was set to an exponentially decaying parameter with time constant 105 episodes. The learner performs policy improvement on π using τ1,...,10 according to eqn. 1. M was trained online, in the same way as in sec. 3.2. λ and π were parameterized by deep, recurrent neural networks with the same architecture described in sec. 3.2.\nWe compare CF-GPS with the vanilla MB-PS baseline described in sec. 4.1 (based on the same number of policy updates). MB-PS differs from CF-GPS by just having access to an unconditional model p(Us1|∅) over initial states. We also consider a method which conditions the scenario model p(Us1|o1) on the very first observation o1, which is available when taking the first action and therefore does not involve hindsight reasoning. This is more informed compared to MB-PS; however due to the noise on the observations, the state is still mostly unobserved rendering it very challenging to learn a good parametric model of the belief state p(Us1|o1). We refer to this algorithm as Guided Policy Search-like (GPS-like), as it roughly corresponds to the algorithm presented by Levine & Abbeel (2014), as discussed in greater detail in sec. 5. Fig. 2 shows that CF-GPS outperforms these\ntwo baselines. As expected from the policy evaluation experiments, initial states sampled from the models for GPS and MB-PS are often not solvable, yielding inferior training data for the policy π. In CF-GPS, the levels are inferred from hindsight inference p(U1|ĥT ), yielding high quality training data. For reference, we also show a policy trained by the model-free method of Espeholt et al. (2018) using the same amount of environment data. Not surprisingly, CF-GPS is able to make better use of the data compared to the model-free baseline as it has access to the true transition and reward kernels (which were not given to the model-free method).'}, {'heading': '5 RELATED WORK', 'text': 'Bottou et al. (2013) provide an in-depth discussion of applying models to off-policy evaluation. However, their and related approaches (Li et al., 2015; Jiang & Li, 2015; Swaminathan & Joachims, 2015; Nedelec et al., 2017; Atan et al., 2016; Thomas & Brunskill, 2016) such as doubly-robust estimators, rely on Importance Sampling (IS), also called Propensity Score method. Although some of these algorithms are also termed counterfactual policy evaluation, they are not counterfactual in the sense used in this paper, where noise variables are inferred from logged data and reused to evaluate counterfactual actions. Hence, they are dogged by high variance in the estimators common to IS, although recent work aims to address this (Munos et al., 2016; Guo et al., 2017). Model-based methods for off-policy evaluation have recently been improved to account for the distribution shift between the data-collecting policy and the policy to be evaluated (Johansson et al., 2016; Liu et al., 2018). Recently (Andrychowicz et al., 2017) proposed the Hindsight Experience Replay (HER) algorithm for learning a family of goal directed policies. In HER one observes an outcome in the true environment, which is kept fixed, and searches for the goal-directed policy that should have achieved this goal in order to positively reinforce it. Therefore, this algorithm is complementary to CF-GPS where we search over alternative outcomes for a given policy. Our CF-GPS algorithm is inspired by and extends work presented by Abbeel et al. (2006) on a method for de-biasing weak models by estimating additive terms in the transition kernel to better match individual, real trajectories. The resulting model, which is a counterfactual distribution in the terminology used in our paper, is then used for model-based policy improvement. Our work generalizes this approach and highlights conceptual connections to causal reasoning. Furthermore, we discuss the connection of CF-GPS to two classes of RL algorithms in greater detail below.\nGuided Policy Search (GPS) CF-GPS is closely related to GPS, in particular we focus on GPS as presented by Levine & Abbeel (2014). Consider CF-GPS in the fully-observed MDP setting where Ot = St. Furthermore, assume that the SCM M is structured as follows: Let St+1 = fs(St, At, Ust) be a linear function in (St, At) with coefficients given by Ust. Further, assume an i.i.d. Gaussian mixture model on Ust for all t. As the states are fully observed, the inference step in the CFI procedure simplifies: we can infer the noise sources ûst (samples or MAP estimates), i.e. the unknown linear dynamics, from pairs of observed, true states ŝt, ŝt+1. Furthermore assume that the reward is a quadratic function of the state. Then, the counterfactual distribution pλ(τ |û) is a linear quadratic regulator (LQR) with time-varying coefficients û. An appropriate choice for the planner λ is the optimal linear feedback policy for the given LQR, which can be computed exactly by dynamic programming.\nObservation 1. In the MDP setting, CF-GPS with a linear SCM and a dynamic programming planner for LQRs λ is equivalent to GPS.\nAnother perspective is that GPS is the counterfactual version of the MB-PS procedure from sec. 4.1:\nObservation 2. In the MDP setting with a linear SCM and a dynamic programming planner for LQRs λ, GPS is the counterfactual variant of the MB-PS procedure outlined above.\nThe fact that GPS is a successful algorithm in practice shows that the ‘grounding’ of model-based search / rollouts in real, off-policy data afforded by counterfactual reasoning massively improves the naive, ‘prior sample’-based MB-PS algorithm. These considerations also suggest when we expect CF-GPS to be superior compared to regular GPS: If the uncertainty in the environment transition Ust cannot be reliably identified from subsequent pairs of observations ôt, ôt+1 alone, we expect benefits of inferring Ust from a larger context of observations, in the extreme case from the entire history ĥT as described above.\nStochastic Value Gradient methods There are multiple interesting connections of CF-GPS to Stochastic Value Gradient (SVG) methods (Heess et al., 2015). In SVG, a policy π for a MDP is learned by gradient ascent on the expected return under a model p. Instead of using the scorefunction estimator, SVG relies on a reparameterization of the stochastic model and policy (Kingma & Welling, 2013; Rezende et al., 2014). We note that this reparameterization casts p into an SCM. As in GPS, the noise sources Ust are inferred from two subsequent observed states ŝt, ŝt+1 from the true environment, and the action noise Uat is kept frozen. As pointed out in the GPS discussion, this procedure corresponds to the inference step in a counterfactual query. Given inferred values u for U , gradients ∂θG of the return under the model are taken with respect to the policy parameters θ. We can loosely interpret these gradients as 2 dim(θ) counterfactual policy evaluations of policies π(θ ±∆θi) where a single dimension i of the parameter vector θ is perturbed.'}, {'heading': '6 DISCUSSION', 'text': 'Simulating plausible synthetic experience de novo is a hard problem for many environments, often resulting in biases for model-based RL algorithms. The main takeaway from this work is that we can improve policy learning by evaluating counterfactual actions in concrete, past scenarios. Compared to only considering synthetic scenarios, this procedure mitigates model bias. However, it relies on some crucial assumptions that we want to briefly discuss here. The first assumption is that off-policy experience is available at all. In cases where this is e.g. too costly to acquire, we cannot use any of the proposed methods and have to exclusively rely on the simulator / model. We also assumed that there are no additional hidden confounders in the environment and that the main challenge in modelling the environment is capturing the distribution of the noise sources p(U), whereas we assumed that the transition and reward kernels given the noise is easy to model. This seems a reasonable assumption in some environments, such as the partially observed grid-world considered here, but not all. Probably the most restrictive assumption is that we require the inference over the noise U given data ĥT to be sufficiently accurate. We showed in our example, that we could learn a parametric model of this distribution from privileged information, i.e. from joint samples u, hT from the true environment. However, imperfect inference over the scenario U could result e.g. in wrongly attributing a negative outcome to the agent’s actions, instead environment factors. This could in turn result in too optimistic predictions for counterfactual actions. Future research is needed to investigate if learning a sufficiently strong SCM is possible without privileged information for interesting RL domains. If, however, we can trust the transition and reward kernels of the model, we can substantially improve model-based RL methods by counterfactual reasoning on off-policy data, as demonstrated in our experiments and by the success of Guided Policy Search and Stochastic Value Gradient methods.'}, {'heading': 'A PROOFS', 'text': 'A.1 PROOF OF LEMMA 1\nProof. We start from the fact that the density over noise sources U remains the same for every intervention I as U are root nodes in G:\npdo(I)(u) = p(u).\nThis leads to:\npdo(I)(x) = ∫ pdo(I)(x|u)pdo(I)(u) du\n= ∫ pdo(I)(x|u)p(u) du\n= ∫ pdo(I)(x|u) (∫ p(x̂o, u)dx̂o ) du\n= ∫ ∫ pdo(I)(x|u) p(u|x̂o)p(x̂o) du dx̂o\n= Ex̂o∼p[ ∫ pdo(I)(x|u)p(u|x̂o) du]\n= Ex̂o∼p[pdo(I)|x̂0(x)].\nA.2 PROOF OF COROLLARY 1\nProof. Given two sets CF ⊂ {1, . . . , N} and Prior ⊂ {1, . . . , N} with CF ∩ Prior = ∅ and CF∪Prior = {1, . . . , N}, we define uCF = (un)n∈CF and uPrior = (un)n∈Prior. By construction, the scenarios U are independent under the prior, i.e. p(u) = ∏N n=1 p(un). Therefore uCF and uPrior are independent. We can write:\np(u) = ( ∏ n∈CF p(un) )( ∏ n∈Prior p(un) ) = p(uCF)p(uPrior).\nFollowing the arguments from Lemma 1, the averaged inference distribution is equal to the prior p(u) = Ex̂o∼p[p(u|x̂o)]. This also holds for any subset of the variables u, in particular for for p(uCF). Hence:\np(u) = p(uCF)p(uPrior)\n= Ex̂o∼p[p(uCF|x̂o)]p(uPrior).'}, {'heading': 'B DETAILS ON CASTING A POMDP INTO SCM FORM', 'text': 'Lemma 2 (Auto-regressive uniformization aka Reparametrization). Consider random variables X1, . . . , XN with joint distribution P . There exist functions fn for n ∈ {1, . . . , N} such that with independent random variables Un ∼ Uniform[0, 1] the random variables X ′ equal X in distribution, i.e. X ′ d= X , where X ′ = (X ′1, . . . , X ′ n) are defined as:\nX ′n := fn(Un, X ′ <n).\nProof. We construct the functions fn by induction on n. Consider the conditional distribution PXn|X<n . For fixed X<n, denote its CDF with Fn,X<n . We construct a random variable X ′ := F−1n,X<n(Un) with Un ∼ Uniform[0, 1] independent from X<n and U<n. By virtue of the inverse-CDF method, we have X ′|X<n d = X|X<n. Therefore, fn(Un, X<n) := F−1n,X<n(Un) satisfies the above lemma.'}, {'heading': 'C MODEL ARCHITECTURE', 'text': 'We assume that we are given the true transition and reward kernels. As the transitions are deterministic in PO-SOKOBAN, the only part of the model that remains to be identified is the initial state distribution p(Us1). We learned this model from data using a the DRAW model (Gregor et al., 2015), which is a parametric, multi-layer, latent variable, neural network model for distributions. For our purposes we chose the convolutional DRAW architecture proposed by (Gregor et al., 2016). First, the observation data is summarized by a convolutional LSTM with 32 hidden units and kernel size of 3. The resulting final LSTM state is fed into a conditional Gaussian prior over the latent variables Zk=1,...,8 of the 8-layer conv-DRAW model. Each layer has 32 hidden layers and the canvas had 7 layers, corresponding to the 7 channels of the categorical Us1 ∈ {0, 1}10×10×7 that we wish to model. The model (together with the backward RNN) was trained with the ADAM optimizer (Kingma & Ba, 2014) on the ELBO loss using the reparametrization trick (Kingma & Welling, 2013; Rezende et al., 2014). The mini-batch size was set to 4 and the learning rate to 3e− 4. We want to emphasize that the DRAW latent variables Z are not directly the noise variables U of the SCM, but integrating out these variables yields this distribution p(Us1|ĥT ) = ∫ p(Us1|z, ĥT )p(z|ĥT )dĥT .'}, {'heading': 'D MODEL MISMATCH ANALYSIS', 'text': 'Here we provide some analysis of the DRAW model over the initial state Us1, which is the learned part of the SCMM used for the policy evaluation experiments presented in 3.2. As detailed above, we trained a separate model p(Us1|ĥt) for each t = 1, . . . , 50 parameterizing the cardinality of the data the model is conditioned on. We analyze three particular models for t = 0, 1 and 50 which we term the unconditional / filtering / smoothing model, as they are conditioned on no data / on data that is available at test time / all data that is available in hindsight. Directly visualizing the distributions p(Us1|ĥt) for an analysis is difficult as the domain {0, . . . , 6}10×10 is high-dimensional and discrete. Instead we focus on the latent variables Z which are learned by DRAW to represent this distribution; by construction, these are jointly Normal, facilitating the analysis. In particular, we compare p(Z|ĥt) with the inference distribution q(Z|ûs1) conditioned on the true state Ûs1. We loosely interpret q(Z|ûs1) as the ”true” embedding of the datum ûs1, whereas p(Z|ĥt) is the learned\nembedding. In a perfect model the prior matches the inference distribution on average:\nEĥt∼pµ [p(Z|ĥt)] ! = Eûs1∼pµ [q(Z|ûs1)],\ni.e. every sample from the prior p(Z|ĥt) corresponds to real data and vice versa. We visualize the averaged prior Eĥt∼pµ [p(Z|ĥt)] and the averaged posterior Eûs1∼pµ [q(Z|ûs1)] in fig. 4. We show the two dimensions of Z where these distributions have the largest KL divergence. Also, the plots were whitened w.r.t. the averaged prior, i.e. the latter is a spherical Gaussian in the plots, represented by an iso-probability contour corresponding to one standard deviation. The inference distribution for each datum ûs1 is visualized by its mean (cross) and a level set corresponding the one standard deviation or less. For the unconditional model t = 0, we find that the distributions are not matched well. In particular, there is a lot of prior mass that sits in regions where there is no or little true data. In the RL setting this results in synthetic data from the model that is unrealistic and training a policy on this data leads to reduced test performance. Also, as apparent from the figure, there is structure in the embedding of the true data, that is not captured at all by the prior. This effect is markedly reduced in the filtering posterior, indicating that the conditional distribution p(Z|ĥ1) already captures the data distribution better. The smoothing model is a very good match to the data. With high probability, all tiles of the game are observed in ĥ50, enabling the model to perfectly learn the belief state, which collapses in this setting to a single state.'}], 'references': [{'title': 'Using inaccurate models in reinforcement learning', 'author': ['Pieter Abbeel', 'Morgan Quigley', 'Andrew Y Ng'], 'venue': 'In Proceedings of the 23rd international conference on Machine learning,', 'citeRegEx': 'Abbeel et al\\.,? \\Q2006\\E', 'shortCiteRegEx': 'Abbeel et al\\.', 'year': 2006}, {'title': 'Hindsight experience replay', 'author': ['Marcin Andrychowicz', 'Filip Wolski', 'Alex Ray', 'Jonas Schneider', 'Rachel Fong', 'Peter Welinder', 'Bob McGrew', 'Josh Tobin', 'OpenAI Pieter Abbeel', 'Wojciech Zaremba'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Andrychowicz et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Andrychowicz et al\\.', 'year': 2017}, {'title': 'Constructing Effective Personalized Policies Using Counterfactual Inference from Biased Data Sets with Many Features', 'author': ['Onur Atan', 'William R Zame', 'Qiaojun Feng', 'Mihaela van der Schaar'], 'venue': 'arXiv preprint arXiv:1612.08082,', 'citeRegEx': 'Atan et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Atan et al\\.', 'year': 2016}, {'title': 'Counterfactual probabilities: Computational methods, bounds and applications', 'author': ['Alexander Balke', 'Judea Pearl'], 'venue': 'In Proceedings of the Tenth international conference on Uncertainty in artificial intelligence,', 'citeRegEx': 'Balke and Pearl.,? \\Q1994\\E', 'shortCiteRegEx': 'Balke and Pearl.', 'year': 1994}, {'title': 'Counterfactual reasoning and learning systems: The example of computational advertising', 'author': ['Léon Bottou', 'Jonas Peters', 'Joaquin Quiñonero-Candela', 'Denis X Charles', 'D Max Chickering', 'Elon Portugaly', 'Dipankar Ray', 'Patrice Simard', 'Ed Snelson'], 'venue': 'The Journal of Machine Learning Research,', 'citeRegEx': 'Bottou et al\\.,? \\Q2013\\E', 'shortCiteRegEx': 'Bottou et al\\.', 'year': 2013}, {'title': 'A survey of Monte Carlo tree search methods', 'author': ['Cameron B Browne', 'Edward Powley', 'Daniel Whitehouse', 'Simon M Lucas', 'Peter I Cowling', 'Philipp Rohlfshagen', 'Stephen Tavener', 'Diego Perez', 'Spyridon Samothrakis', 'Simon Colton'], 'venue': 'IEEE Transactions on Computational Intelligence and AI in games,', 'citeRegEx': 'Browne et al\\.,? \\Q2012\\E', 'shortCiteRegEx': 'Browne et al\\.', 'year': 2012}, {'title': 'IMPALA: Scalable distributed Deep-RL with importance weighted actor-learner architectures', 'author': ['Lasse Espeholt', 'Hubert Soyer', 'Remi Munos', 'Karen Simonyan', 'Volodymir Mnih', 'Tom Ward', 'Yotam Doron', 'Vlad Firoiu', 'Tim Harley', 'Iain Dunning'], 'venue': 'arXiv preprint arXiv:1802.01561,', 'citeRegEx': 'Espeholt et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Espeholt et al\\.', 'year': 2018}, {'title': 'Draw: A recurrent neural network for image generation', 'author': ['Karol Gregor', 'Ivo Danihelka', 'Alex Graves', 'Danilo Jimenez Rezende', 'Daan Wierstra'], 'venue': 'arXiv preprint arXiv:1502.04623,', 'citeRegEx': 'Gregor et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Gregor et al\\.', 'year': 2015}, {'title': 'Towards conceptual compression', 'author': ['Karol Gregor', 'Frederic Besse', 'Danilo Jimenez Rezende', 'Ivo Danihelka', 'Daan Wierstra'], 'venue': 'In Advances In Neural Information Processing Systems,', 'citeRegEx': 'Gregor et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Gregor et al\\.', 'year': 2016}, {'title': 'Using options and covariance testing for long horizon off-policy policy evaluation', 'author': ['Zhaohan Guo', 'Philip S Thomas', 'Emma Brunskill'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Guo et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Guo et al\\.', 'year': 2017}, {'title': 'Learning continuous control policies by stochastic value gradients', 'author': ['Nicolas Heess', 'Gregory Wayne', 'David Silver', 'Tim Lillicrap', 'Tom Erez', 'Yuval Tassa'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Heess et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Heess et al\\.', 'year': 2015}, {'title': 'Doubly robust off-policy value evaluation for reinforcement learning', 'author': ['Nan Jiang', 'Lihong Li'], 'venue': 'arXiv preprint arXiv:1511.03722,', 'citeRegEx': 'Jiang and Li.,? \\Q2015\\E', 'shortCiteRegEx': 'Jiang and Li.', 'year': 2015}, {'title': 'The dependence of effective planning horizon on model accuracy', 'author': ['Nan Jiang', 'Alex Kulesza', 'Satinder Singh', 'Richard Lewis'], 'venue': 'In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,', 'citeRegEx': 'Jiang et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Jiang et al\\.', 'year': 2015}, {'title': 'Learning representations for counterfactual inference', 'author': ['Fredrik Johansson', 'Uri Shalit', 'David Sontag'], 'venue': 'In International Conference on Machine Learning,', 'citeRegEx': 'Johansson et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Johansson et al\\.', 'year': 2016}, {'title': 'Adam: A method for stochastic optimization', 'author': ['Diederik P Kingma', 'Jimmy Ba'], 'venue': 'arXiv preprint arXiv:1412.6980,', 'citeRegEx': 'Kingma and Ba.,? \\Q2014\\E', 'shortCiteRegEx': 'Kingma and Ba.', 'year': 2014}, {'title': 'Auto-encoding variational Bayes', 'author': ['Diederik P Kingma', 'Max Welling'], 'venue': 'arXiv preprint arXiv:1312.6114,', 'citeRegEx': 'Kingma and Welling.,? \\Q2013\\E', 'shortCiteRegEx': 'Kingma and Welling.', 'year': 2013}, {'title': 'Varieties of causal intervention', 'author': ['Kevin B Korb', 'Lucas R Hope', 'Ann E Nicholson', 'Karl Axnick'], 'venue': 'In Pacific Rim International Conference on Artificial Intelligence,', 'citeRegEx': 'Korb et al\\.,? \\Q2004\\E', 'shortCiteRegEx': 'Korb et al\\.', 'year': 2004}, {'title': 'Learning neural network policies with guided policy search under unknown dynamics', 'author': ['Sergey Levine', 'Pieter Abbeel'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Levine and Abbeel.,? \\Q2014\\E', 'shortCiteRegEx': 'Levine and Abbeel.', 'year': 2014}, {'title': 'Guided policy search', 'author': ['Sergey Levine', 'Vladlen Koltun'], 'venue': 'In International Conference on Machine Learning, pp', 'citeRegEx': 'Levine and Koltun.,? \\Q2013\\E', 'shortCiteRegEx': 'Levine and Koltun.', 'year': 2013}, {'title': 'Counterfactual estimation and optimization of click metrics in search engines: A case study', 'author': ['Lihong Li', 'Shunbao Chen', 'Jim Kleban', 'Ankur Gupta'], 'venue': 'In Proceedings of the 24th International Conference on World Wide Web,', 'citeRegEx': 'Li et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Li et al\\.', 'year': 2015}, {'title': 'Representation balancing mdps for off-policy policy evaluation', 'author': ['Yao Liu', 'Omer Gottesman', 'Aniruddh Raghu', 'Matthieu Komorowski', 'Aldo Faisal', 'Finale DoshiVelez', 'Emma Brunskill'], 'venue': 'arXiv preprint arXiv:1805.09044,', 'citeRegEx': 'Liu et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Liu et al\\.', 'year': 2018}, {'title': 'Safe and efficient off-policy reinforcement learning', 'author': ['Rémi Munos', 'Tom Stepleton', 'Anna Harutyunyan', 'Marc Bellemare'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Munos et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Munos et al\\.', 'year': 2016}, {'title': 'A comparative study of counterfactual estimators', 'author': ['Thomas Nedelec', 'Nicolas Le Roux', 'Vianney Perchet'], 'venue': 'arXiv preprint arXiv:1704.00773,', 'citeRegEx': 'Nedelec et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Nedelec et al\\.', 'year': 2017}, {'title': 'Causality: Models, Reasoning and Inference', 'author': ['Judea Pearl'], 'venue': 'USA, 2nd edition,', 'citeRegEx': 'Pearl.,? \\Q2009\\E', 'shortCiteRegEx': 'Pearl.', 'year': 2009}, {'title': 'The Book of Why: The New Science of Cause and Effect', 'author': ['Judea Pearl', 'Dana Mackenzie'], 'venue': 'Basic Books,', 'citeRegEx': 'Pearl and Mackenzie.,? \\Q2018\\E', 'shortCiteRegEx': 'Pearl and Mackenzie.', 'year': 2018}, {'title': 'Elements of Causal Inference - Foundations and Learning Algorithms. Adaptive Computation and Machine Learning Series', 'author': ['J. Peters', 'D. Janzing', 'B. Schölkopf'], 'venue': None, 'citeRegEx': 'Peters et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Peters et al\\.', 'year': 2017}, {'title': 'Eligibility traces for off-policy policy evaluation', 'author': ['Doina Precup'], 'venue': 'Computer Science Department Faculty Publication Series, pp', 'citeRegEx': 'Precup.,? \\Q2000\\E', 'shortCiteRegEx': 'Precup.', 'year': 2000}, {'title': 'Imagination-augmented agents for deep reinforcement learning', 'author': ['Sébastien Racanière', 'Théophane Weber', 'David Reichert', 'Lars Buesing', 'Arthur Guez', 'Danilo Jimenez Rezende', 'Adrià Puigdomènech Badia', 'Oriol Vinyals', 'Nicolas Heess', 'Yujia Li'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Racanière et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Racanière et al\\.', 'year': 2017}, {'title': 'Stochastic backpropagation and approximate inference in deep generative models', 'author': ['Danilo Jimenez Rezende', 'Shakir Mohamed', 'Daan Wierstra'], 'venue': 'arXiv preprint arXiv:1401.4082,', 'citeRegEx': 'Rezende et al\\.,? \\Q2014\\E', 'shortCiteRegEx': 'Rezende et al\\.', 'year': 2014}, {'title': 'Counterfactual thinking', 'author': ['Neal J Roese'], 'venue': 'Psychological bulletin,', 'citeRegEx': 'Roese.,? \\Q1997\\E', 'shortCiteRegEx': 'Roese.', 'year': 1997}, {'title': 'Counterfactual risk minimization: Learning from logged bandit feedback', 'author': ['Adith Swaminathan', 'Thorsten Joachims'], 'venue': 'In International Conference on Machine Learning,', 'citeRegEx': 'Swaminathan and Joachims.,? \\Q2015\\E', 'shortCiteRegEx': 'Swaminathan and Joachims.', 'year': 2015}, {'title': 'Model Regularization for Stable Sample Rollouts', 'author': ['Erik Talvitie'], 'venue': 'In UAI, pp', 'citeRegEx': 'Talvitie.,? \\Q2014\\E', 'shortCiteRegEx': 'Talvitie.', 'year': 2014}, {'title': 'Data-efficient off-policy policy evaluation for reinforcement learning', 'author': ['Philip Thomas', 'Emma Brunskill'], 'venue': 'In International Conference on Machine Learning,', 'citeRegEx': 'Thomas and Brunskill.,? \\Q2016\\E', 'shortCiteRegEx': 'Thomas and Brunskill.', 'year': 2016}, {'title': 'Robot trajectory optimization using approximate inference', 'author': ['Marc Toussaint'], 'venue': 'In Proceedings of the 26th annual international conference on machine learning,', 'citeRegEx': 'Toussaint.,? \\Q2009\\E', 'shortCiteRegEx': 'Toussaint.', 'year': 2009}, {'title': 'The relative importance of heredity and environment in determining the piebald pattern of guinea-pigs', 'author': ['Sewall Wright'], 'venue': 'Proceedings of the National Academy of Sciences,', 'citeRegEx': 'Wright.,? \\Q1920\\E', 'shortCiteRegEx': 'Wright.', 'year': 1920}, {'title': 'Convolutional LSTM network: A machine learning approach for precipitation nowcasting', 'author': ['Shi Xingjian', 'Zhourong Chen', 'Hao Wang', 'Dit-Yan Yeung', 'Wai-Kin Wong', 'Wang-chun Woo'], 'venue': 'In Advances in neural information processing systems,', 'citeRegEx': 'Xingjian et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Xingjian et al\\.', 'year': 2015}], 'referenceMentions': [{'referenceID': 29, 'context': 'Social psychologists theorize that such cognitive processes are beneficial for improving future decision making (Roese, 1997).', 'startOffset': 112, 'endOffset': 125}, {'referenceID': 31, 'context': 'However, a mismatch between the model and the true environment, often unavoidable in practice, can cause this approach to fail (Talvitie, 2014), resulting in policies that do not generalize to the real environment (Jiang et al.', 'startOffset': 127, 'endOffset': 143}, {'referenceID': 12, 'context': 'However, a mismatch between the model and the true environment, often unavoidable in practice, can cause this approach to fail (Talvitie, 2014), resulting in policies that do not generalize to the real environment (Jiang et al., 2015).', 'startOffset': 214, 'endOffset': 234}, {'referenceID': 23, 'context': 'actions that had not actually been taken, while everything else remaining the same (Pearl, 2009).', 'startOffset': 83, 'endOffset': 96}, {'referenceID': 34, 'context': 'of CF-GPS are structural causal models (SCMs) which model the environment with two ingredients (Wright, 1920): 1) Independent random variables, called scenarios here, summarize all aspects of the environment that cannot be influenced by the agent, e.', 'startOffset': 95, 'endOffset': 109}, {'referenceID': 26, 'context': 'CF-GPS differs substantially from standard off-policy RL algorithms based on Importance Sampling (IS), where historical data is re-weighted with respect to the importance weights to evaluate or learn new policies (Precup, 2000).', 'startOffset': 213, 'endOffset': 227}, {'referenceID': 10, 'context': 'We show that two previously proposed classes of RL algorithms, namely Guided Policy Search (Levine & Koltun, 2013) and Stochastic Value Gradient methods (Heess et al., 2015), can be interpreted as counterfactual methods, opening up possible generalizations.', 'startOffset': 153, 'endOffset': 173}, {'referenceID': 16, 'context': 'This also includes what is known as stochastic interventions or mechanism changes (Korb et al., 2004) which generalize atomic interventions (Pearl, 2009).', 'startOffset': 82, 'endOffset': 101}, {'referenceID': 23, 'context': ', 2004) which generalize atomic interventions (Pearl, 2009).', 'startOffset': 46, 'endOffset': 59}, {'referenceID': 28, 'context': 'This procedure is closely related to the ‘reparameterization trick’ for models with location-scale distributions (Kingma & Welling, 2013; Rezende et al., 2014).', 'startOffset': 113, 'endOffset': 159}, {'referenceID': 25, 'context': 'Nevertheless, SCMs with non-independent noise distributions arising from counterfactual inference, denoted here by Mx̂o , are commonly considered in the literature (Peters et al., 2017).', 'startOffset': 164, 'endOffset': 185}, {'referenceID': 6, 'context': 'We first generate a policy π that we wish to evaluate, by training it using a previously-proposed distributed RL algorithm (Espeholt et al., 2018).', 'startOffset': 123, 'endOffset': 146}, {'referenceID': 35, 'context': 'The policy is parameterized as a deep, recurrent neural network consisting of a 3-layer deep convolutional LSTM (Xingjian et al., 2015) with 32 channels per layer and kernel size of 3.', 'startOffset': 112, 'endOffset': 135}, {'referenceID': 7, 'context': 'The models are chosen to be powerful, multilayer, generative DRAW models (Gregor et al., 2015) trained by approximate maximum likelihood learning (Kingma & Welling, 2013; Rezende et al.', 'startOffset': 73, 'endOffset': 94}, {'referenceID': 28, 'context': ', 2015) trained by approximate maximum likelihood learning (Kingma & Welling, 2013; Rezende et al., 2014).', 'startOffset': 59, 'endOffset': 105}, {'referenceID': 33, 'context': 'This policy improvement step can be motivated by the framework of RL as variational inference (Toussaint, 2009) and is equivalent to minimizing the KL divergence to a trajectory distribution ∝ exp(G)pπk which puts additional mass on high-return trajectories.', 'startOffset': 94, 'endOffset': 111}, {'referenceID': 5, 'context': 'a search procedure such as MCTS (Browne et al., 2012) or an expert policy.', 'startOffset': 32, 'endOffset': 53}, {'referenceID': 19, 'context': 'However, their and related approaches (Li et al., 2015; Jiang & Li, 2015; Swaminathan & Joachims, 2015; Nedelec et al., 2017; Atan et al., 2016; Thomas & Brunskill, 2016) such as doubly-robust estimators, rely on Importance Sampling (IS), also called Propensity Score method.', 'startOffset': 38, 'endOffset': 170}, {'referenceID': 22, 'context': 'However, their and related approaches (Li et al., 2015; Jiang & Li, 2015; Swaminathan & Joachims, 2015; Nedelec et al., 2017; Atan et al., 2016; Thomas & Brunskill, 2016) such as doubly-robust estimators, rely on Importance Sampling (IS), also called Propensity Score method.', 'startOffset': 38, 'endOffset': 170}, {'referenceID': 2, 'context': 'However, their and related approaches (Li et al., 2015; Jiang & Li, 2015; Swaminathan & Joachims, 2015; Nedelec et al., 2017; Atan et al., 2016; Thomas & Brunskill, 2016) such as doubly-robust estimators, rely on Importance Sampling (IS), also called Propensity Score method.', 'startOffset': 38, 'endOffset': 170}, {'referenceID': 21, 'context': 'Hence, they are dogged by high variance in the estimators common to IS, although recent work aims to address this (Munos et al., 2016; Guo et al., 2017).', 'startOffset': 114, 'endOffset': 152}, {'referenceID': 9, 'context': 'Hence, they are dogged by high variance in the estimators common to IS, although recent work aims to address this (Munos et al., 2016; Guo et al., 2017).', 'startOffset': 114, 'endOffset': 152}, {'referenceID': 13, 'context': 'Model-based methods for off-policy evaluation have recently been improved to account for the distribution shift between the data-collecting policy and the policy to be evaluated (Johansson et al., 2016; Liu et al., 2018).', 'startOffset': 178, 'endOffset': 220}, {'referenceID': 20, 'context': 'Model-based methods for off-policy evaluation have recently been improved to account for the distribution shift between the data-collecting policy and the policy to be evaluated (Johansson et al., 2016; Liu et al., 2018).', 'startOffset': 178, 'endOffset': 220}, {'referenceID': 1, 'context': 'Recently (Andrychowicz et al., 2017) proposed the Hindsight Experience Replay (HER) algorithm for learning a family of goal directed policies.', 'startOffset': 9, 'endOffset': 36}, {'referenceID': 10, 'context': 'Stochastic Value Gradient methods There are multiple interesting connections of CF-GPS to Stochastic Value Gradient (SVG) methods (Heess et al., 2015).', 'startOffset': 130, 'endOffset': 150}, {'referenceID': 28, 'context': 'Instead of using the scorefunction estimator, SVG relies on a reparameterization of the stochastic model and policy (Kingma & Welling, 2013; Rezende et al., 2014).', 'startOffset': 116, 'endOffset': 162}], 'year': 2019, 'abstractText': 'Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for modelbased policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.', 'creator': 'LaTeX with hyperref package'}","imagine that a month ago alice had two job offers from companies a1 and a2. she decided to join a1 because of the larger salary, in spite of an awkward feeling during the job interview. since then she learned a lot about a1 and recently received information about a2 from a friend, prodding her now to imagine what would have happened had she joined a2. re-evaluating her decision in hindsight in this way, she concludes that she made a regrettable decision. she could and should have known that a2 was a better choice, had she only interpreted the cues during the interview correctly... this example tries to illustrate the everyday human capacity to reason about alternate, counterfactual outcomes of past experience with the goal of “mining worlds that could have been” (pearl & mackenzie, 2018). social psychologists theorize that such cognitive processes are beneficial for improving future decision making (roese, 1997). in this paper we aim to leverage possible advantages of counterfactual reasoning for learning decision making in the reinforcement learning (rl) framework. in spite of recent success, learning policies with standard, model-free rl algorithms can be notoriously data inefficient. this issue can in principle be addressed by learning policies on data synthesized from a model. however, a mismatch between the model and the true environment, often unavoidable in practice, can cause this approach to fail (talvitie, 2014), resulting in policies that do not generalize to the real environment (jiang et al., 2015). motivated by the introductory example, we propose the counterfactually-guided policy search (cf-gps) algorithm: instead of relying on data synthesized from scratch by a model, policies are trained on model predictions of alternate outcomes of past experience from the true environment under counterfactual actions, i.e. actions that had not actually been taken, while everything else remaining the same (pearl, 2009). at the heart of cf-gps are structural causal models (scms) which model the environment with two ingredients (wright, 1920): 1) independent random variables, called scenarios here, summarize all aspects of the environment that cannot be influenced by the agent, e.g. the properties of the companies in alice’s job search example. 2) deterministic transition functions (also called causal mechanisms) take these scenarios, together with the agent’s actions, as input and produce the predicted outcome. the central idea of cf-gps is that, instead of running an agent on scenarios sampled de novo from a model, scenarios are inferred in hindsight from given off-policy data, and then evaluate and improve the agent on these specific scenarios using given or learned causal mechanisms (balke & pearl, 1994). cf-gps can be regarded as a meta-algorithm that extends a given model-base rl algorithm by “grounding” or “anchoring” model-based predictions in inferred scenarios. as a result, this approach explicitly allows to trade-off historical data for model bias. we show empirically in a conceptually simple setting, where unknown initial states are inferred in hindsight and re-used to evalute to counterfactual actions, that this can mitigate model mismatch. cf-gps differs substantially from standard off-policy rl algorithms based on importance sampling (is), where historical data is re-weighted with respect to the importance weights to evaluate or learn new policies (precup, 2000). in contrast, cf-gps explicitly reasons counterfactually about given off-policy data. our main contributions are: 1. we formulate model-based rl in pomdps in terms of structural causal models, thereby connecting concepts from reinforcement learning and causal inference. 2. we provide the first results, to the best of our knowledge, showing that counterfactual reasoning in structural causal models on off-policy data can facilitate solving non-trivial rl tasks. 3. we show that two previously proposed classes of rl algorithms, namely guided policy search (levine & koltun, 2013) and stochastic value gradient methods (heess et al., 2015), can be interpreted as counterfactual methods, opening up possible generalizations. the paper is structured as follows. we first give a self-contained, high-level recapitulation of structural causal models and counterfactual inference, as these are less widely known in the rl and generative model communities. in particular we show how to model pomdps with scms. based on this exposition, we first consider the task of policy evaluation and discuss how we can leverage counterfactual inference in scms to improve over standard model-based methods. we then generalize this approach to the policy search setting resulting in the cf-gps algorithm. we close by highlighting connections to previously proposed algorithms and by discussing assumptions and limitations of the proposed method.","this example tries to illustrate the everyday human capacity to reason about alternate, counterfactual outcomes of past experience with the goal of “mining worlds that could have been” (pearl & mackenzie, 2018) however, a mismatch between the model and the true environment, often unavoidable in practice, can cause this approach to fail (talvitie, 2014), resulting in policies that do not generalize to the real environment (jiang et al motivated by the introductory example, we propose the counterfactually-guided policy search (cf-gps) algorithm: instead of relying on data synthesized from scratch by a model, policies are trained on model predictions of alternate outcomes of past experience from the true environment under counterfactual actions, i at the heart of cf-gps are structural causal models (scms) which model the environment with two ingredients (wright, 1920): 1) independent random variables, called scenarios here, summarize all aspects of the environment that cannot be influenced by the agent, e the central idea of cf-gps is that, instead of running an agent on scenarios sampled de novo from a model, scenarios are inferred in hindsight from given off-policy data, and then evaluate and improve the agent on these specific scenarios using given or learned causal mechanisms (balke & pearl, 1994) cf-gps differs substantially from standard off-policy rl algorithms based on importance sampling (is), where historical data is re-weighted with respect to the importance weights to evaluate or learn new policies (precup, 2000) we provide the first results, to the best of our knowledge, showing that counterfactual reasoning in structural causal models on off-policy data can facilitate solving non-trivial rl tasks we show that two previously proposed classes of rl algorithms, namely guided policy search (levine & koltun, 2013) and stochastic value gradient methods (heess et al",0.8923393487930298,0.8698195219039917,0.880935549736023
NIPS_2017_300,Shallow Updates for Deep Reinforcement Learning,"{'source': 'META', 'title': 'Shallow Updates for Deep Reinforcement Learning', 'authors': ['Nir Levine', 'Tom Zahavy'], 'emails': ['levin.nir1@gmail.com', 'tomzahavy@campus.technion.ac.il', 'danielm@tx.technion.ac.il', 'avivt@berkeley.edu', 'shie@ee.technion.ac.il'], 'sections': [{'heading': '1 Introduction', 'text': 'Reinforcement learning (RL) is a field of research that uses dynamic programing (DP; Bertsekas 2008), among other approaches, to solve sequential decision making problems. The main challenge in applying DP to real world problems is an exponential growth of computational requirements as the problem size increases, known as the curse of dimensionality (Bertsekas, 2008).\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nRL tackles the curse of dimensionality by approximating terms in the DP calculation such as the value function or policy. Popular function approximators for this task include deep neural networks, henceforth termed deep RL (DRL), and linear architectures, henceforth termed shallow RL (SRL).\nSRL methods have enjoyed wide popularity over the years (see, e.g., Tsitsiklis et al. 1997; Bertsekas 2008 for extensive reviews). In particular, batch algorithms based on a least squares (LS) approach, such as Least Squares Temporal Difference (LSTD, Lagoudakis & Parr 2003) and Fitted-Q Iteration (FQI, Ernst et al. 2005) are known to be stable and data efficient. However, the success of these algorithms crucially depends on the quality of the feature representation. Ideally, the representation encodes rich, expressive features that can accurately represent the value function. However, in practice, finding such good features is difficult and often hampers the usage of linear function approximation methods.\nIn DRL, on the other hand, the features are learned together with the value function in a deep architecture. Recent advancements in DRL using convolutional neural networks demonstrated learning of expressive features (Zahavy et al., 2016; Wang et al., 2016) and state-of-the-art performance in challenging tasks such as video games (Mnih et al. 2015; Tessler et al. 2017; Mnih et al. 2016), and Go (Silver et al., 2016). To date, the most impressive DRL results (E.g., the works of Mnih et al. 2015, Mnih et al. 2016) were obtained using online RL algorithms, based on a stochastic gradient descent (SGD) procedure.\nOn the one hand, SRL is stable and data efficient. On the other hand, DRL learns powerful representations. This motivates us to ask: can we combine DRL with SRL to leverage the benefits of both?\nIn this work, we develop a hybrid approach that combines batch SRL algorithms with online DRL. Our main insight is that the last layer in a deep architecture can be seen as a linear representation, with the preceding layers encoding features. Therefore, the last layer can be learned using standard SRL algorithms. Following this insight, we propose a method that repeatedly re-trains the last hidden layer of a DRL network with a batch SRL algorithm, using data collected throughout the DRL run.\nWe focus on value-based DRL algorithms (e.g., the popular DQN of Mnih et al. 2015) and on SRL based on LS methods1, and propose the Least Squares DQN algorithm (LS-DQN). Key to our approach is a novel regularization term for the least squares method that uses the DRL solution as a prior in a Bayesian least squares formulation. Our experiments demonstrate that this hybrid approach significantly improves performance on the Atari benchmark for several combinations of DRL and SRL methods.\nTo support our results, we performed an in-depth analysis to tease out the factors that make our hybrid approach outperform DRL. Interestingly, we found that the improved performance is mainly due to the large batch size of SRL methods compared to the small batch size that is typical for DRL.'}, {'heading': '2 Background', 'text': 'In this section we describe our RL framework and several shallow and deep RL algorithms that will be used throughout the paper.\nRL Framework: We consider a standard RL formulation (Sutton & Barto, 1998) based on a Markov Decision Process (MDP). An MDP is a tuple 〈S,A,R, P, γ〉, where S is a finite set of states, A is a finite set of actions, and γ ∈ [0, 1] is the discount factor. A transition probability function P : S × A → ∆S maps states and actions to a probability distribution over next states. Finally, R : S×A→ [Rmin, Rmax] denotes the reward. The goal in RL is to learn a policy π : S → ∆A that solves the MDP by maximizing the expected discounted return E [ ∑∞ t=0 γ\ntrt|π]. Value based RL methods make use of the action value function Qπ(s, a) = E[ ∑∞ t=0 γ\ntrt|st = s, at = a, π], which represents the expected discounted return of executing action a ∈ A from state s ∈ S and following the policy π thereafter. The optimal action value function Q∗(s, a) obeys a fundamental recursion known as the Bellman equation Q∗(s, a) = E [rt + γmaxa′ Q∗(st+1, a′)| st = s, at = a].\n1Our approach can be generalized to other DRL/SRL variants.'}, {'heading': '2.1 SRL algorithms', 'text': 'Least Squares Temporal Difference Q-Learning (LSTD-Q): LSTD (Barto & Crites, 1996) and LSTD-Q (Lagoudakis & Parr, 2003) are batch SRL algorithms. LSTD-Q learns a control policy π from a batch of samples by estimating a linear approximation Q̂π = Φwπ of the action value function Qπ ∈ R|S||A|, where wπ ∈ Rk are a set of weights and Φ ∈ R|S||A|×k is a feature matrix. Each row of Φ represents a feature vector for a state-action pair 〈s, a〉. The weights wπ are learned by enforcing Q̂π to satisfy a fixed point equation w.r.t. the projected Bellman operator, resulting in a system of linear equations Awπ = b, where A = ΦT (Φ− γPΠπΦ) and b = ΦTR. Here, R ∈ R|S||A| is the reward vector, P ∈ R|S||A|×|S| is the transition matrix and Ππ ∈ R|S|×|S||A| is a matrix describing the policy. Given a set of NSRL samples D = {si, ai, ri, si+1}NSRLi=1 , we can approximate A and b with the following empirical averages:\nÃ = 1\nNSRL NSRL∑ i=1 [ φ(si, ai) T ( φ(si, ai)−γφ(si+1, π(si+1)) )] , b̃ = 1 NSRL NSRL∑ i=1 [ φ(si, ai) T ri ] .\n(1) The weights wπ can be calculated using a least squares minimization: w̃π = arg minw ‖Ãw − b̃‖22 or by calculating the pseudo-inverse: w̃π = Ã†b̃. LSTD-Q is an off-policy algorithm: the same set of samples D can be used to train any policy π so long as π(si+1) is defined for every si+1 in the set.\nFitted Q Iteration (FQI): The FQI algorithm (Ernst et al., 2005) is a batch SRL algorithm that computes iterative approximations of the Q-function using regression. At iteration N of the algorithm, the set D defined above and the approximation from the previous iteration QN−1 are used to generate supervised learning targets: yi = ri + γmaxa′ Q N−1(si+1, a ′ ), ,∀i ∈ NSRL. These targets are then used by a supervised learning (regression) method to compute the next function in the sequence QN , by minimizing the MSE loss QN = argminQ ∑NSRL i=1 (Q(si, ai)− (ri + γmaxa′ QN−1(si+1, a′)))2. For a linear function approximation Qn(a, s) = φT (s, a)wn, LS can be used to give the FQI solution wn = arg minw ‖Ãw − b̃‖22, where Ã, b̃ are given by:\nÃ = 1\nNSRL NSRL∑ i=1 [ φ(si, ai) Tφ(si, ai) ] , b̃ = 1 NSRL NSRL∑ i=1 [ φ(si, ai) T yi ] . (2)\nThe FQI algorithm can also be used with non-linear function approximations such as trees (Ernst et al., 2005) and neural networks (Riedmiller, 2005). The DQN algorithm (Mnih et al., 2015) can be viewed as online form of FQI.'}, {'heading': '2.2 DRL algorithms', 'text': 'Deep Q-Network (DQN): The DQN algorithm (Mnih et al., 2015) learns the Q function by minimizing the mean squared error of the Bellman equation, defined as Est,at,rt,st+1‖Qθ(st, at)− yt‖22, where yt = rt + γmaxa′ Qθtarget(st+1, a ′ ). The DQN maintains two separate networks, namely the current network with weights θ and the target network with weights θtarget. Fixing the target network makes the DQN algorithm equivalent to FQI (see the FQI MSE loss defined above), where the regression algorithm is chosen to be SGD (RMSPROP, Hinton et al. 2012). The DQN is an off-policy learning algorithm. Therefore, the tuples 〈st, at, rt, st+1〉 that are used to optimize the network weights are first collected from the agent’s experience and are stored in an Experience Replay (ER) buffer (Lin, 1993) providing improved stability and performance.\nDouble DQN (DDQN): DDQN (Van Hasselt et al., 2016) is a modification of the DQN algorithm that addresses overly optimistic estimates of the value function. This is achieved by performing action selection with the current network θ and evaluating the action with the target network, θtarget, yielding the DDQN target update yt = rt if st+1 is terminal, otherwise yt = rt + γQθtarget(st+1,maxaQθ(st+1, a)).'}, {'heading': '3 The LS-DQN Algorithm', 'text': 'We now present a hybrid approach for DRL with SRL updates2. Our algorithm, the LS-DQN Algorithm, periodically switches between training a DRL network and re-training its last hidden layer using an SRL method. 3\nWe assume that the DRL algorithm uses a deep network for representing the Q function4, where the last layer is linear and fully connected. Such networks have been used extensively in deep RL recently (e.g., Mnih et al. 2015; Van Hasselt et al. 2016; Mnih et al. 2016). In such a representation, the last layer, which approximates the Q function, can be seen as a linear combination of features (the output of the penultimate layer), and we propose to learn more accurate weights for it using SRL.\nExplicitly, the LS-DQN algorithm begins by training the weights of a DRL network, wk, using a value-based DRL algorithm for NDRL steps (Line 2). LS-DQN then updates the last hidden layer weights, wlastk , by executing LS-UPDATE: retraining the weights using a SRL algorithm with NSRL samples (Line 3).\nThe LS-UPDATE consists of the following steps. First, data trajectories D for the batch update are gathered using the current network weights, wk (Line 7). In practice, the current experience replay can be used and no additional samples need to be collected. The algorithm next generates new features Φ (s, a) from the data trajectories using the current DRL network with weights wk. This step guarantees that we do not use samples with inconsistent features, as the ER contains features from ’old’ networks weights. Computationally, this step requires running a forward pass of the deep network for every sample in D, and can be performed quickly using parallelization.\nOnce the new features are generated, LS-DQN uses an SRL algorithm to re-calculate the weights of the last hidden layer wlastk (Line 9). While the LS-DQN algorithm is conceptually straightforward, we found that naively running it with off-the-shelf SRL algorithms such as FQI or LSTD resulted in instability and a degradation of the DRL performance. The reason is that the ‘slow’ SGD computation in DRL essentially retains information from older training epochs, while the batch SRL method ‘forgets’ all data but the most recent batch. In the following, we propose a novel regularization method for addressing this issue.\nAlgorithm 1 LS-DQN Algorithm Require: w0\n1: for k = 1 · · ·SRLiters do 2: wk ← trainDRLNetwork(wk−1) . Train the DRL network for NDRL steps 3: wlastk ← LS-UPDATE(wk) . Update the last layer weights with the SRL solution 4: end for 5: 6: function LS-UPDATE(w) 7: D ← gatherData(w) 8: Φ(s, a)← generateFeatures(D,w) 9: wlast ← SRL-Algorithm(D,Φ(s, a))\n10: return wlast 11: end function\nRegularization\nOur goal is to improve the performance of a value-based DRL agent using a batch SRL algorithm. Batch SRL algorithms, however, do not leverage the knowledge that the agent has gained before the most recent batch5. We observed that this issue prevents the use of off-the-shelf implementations of SRL methods in our hybrid LS-DQN algorithm.\n2Code is available online at https://github.com/Shallow-Updates-for-Deep-RL 3We refer the reader to Appendix B for a diagram of the algorithm. 4The features in the last DQN layer are not action dependent. We generate action-dependent features Φ (s, a) by zero-padding to a one-hot state-action feature vector. See Appendix E for more details. 5While conceptually, the data batch can include all the data seen so far, due to computational limitations, this is not a practical solution in the domains we consider.\nTo enjoy the benefits of both worlds, that is, a batch algorithm that can use the accumulated knowledge gained by the DRL network, we introduce a novel Bayesian regularization method for LSTD-Q and FQI that uses the last hidden layer weights of the DRL network wlastk as a Bayesian prior for the SRL algorithm 6.\nSRL Bayesian Prior Formulation: We are interested in learning the weights of the last hidden layer (wlast), using a least squares SRL algorithm. We pursue a Bayesian approach, where the prior weights distribution at iteration k of LS-DQN is given by wprior ∼ N(wlastk , λ−2), and we recall that wlastk are the last hidden layer weights of the DRL network at iteration SRLiter = k. The Bayesian solution for the regression problem in the FQI algorithm is given by (Box & Tiao, 2011)\nwlast = (Ã+ λI)−1(b̃+ λwlastk ) ,\nwhere Ã and b̃ are given in Equation 2. A similar regularization can be added to LSTD-Q based on a regularized fixed point equation (Kolter & Ng, 2009). Full details are in Appendix A.'}, {'heading': '4 Experiments', 'text': 'In this section, we present experiments showcasing the improved performance attained by our LSDQN algorithm compared to state-of-the-art DRL methods. Our experiments are divided into three sections. In Section 4.1, we start by investigating the behavior of SRL algorithms in high dimensional environments. We then show results for the LS-DQN on five Atari domains, in Section 4.2, and compare the resulting performance to regular DQN and DDQN agents. Finally, in Section 4.3, we present an ablative analysis of the LS-DQN algorithm, which clarifies the reasons behind our algorithm’s success.'}, {'heading': '4.1 SRL Algorithms with High Dimensional Observations', 'text': 'In the first set of experiments, we explore how least squares SRL algorithms perform in domains with high dimensional observations. This is an important step before applying a SRL method within the LS-DQN algorithm. In particular, we focused on answering the following questions: (1) What regularization method to use? (2) How to generate data for the LS algorithm? (3) How many policy improvement iterations to perform?\nTo answer these questions, we performed the following procedure: We trained DQN agents on two games from the Arcade Learning Environment (ALE, Bellemare et al.); namely, Breakout and Qbert, using the vanilla DQN implementation (Mnih et al., 2015). For each DQN run, we (1) periodically 7 save the current DQN network weights and ER; (2) Use an SRL algorithm (LSTD-Q or FQI) to re-learn the weights of the last layer, and (3) evaluate the resulting DQN network by temporarily replacing the DQN weights with the SRL solution weights. After the evaluation, we replace back the original DQN weights and continue training.\nEach evaluation entails 20 roll-outs 8 with an -greedy policy (similar to Mnih et al., = 0.05). This periodic evaluation setup allowed us to effectively experiment with the SRL algorithms and obtain clear comparisons with DQN, without waiting for full DQN runs to complete.\n(1) Regularization: Experiments with standard SRL methods without any regularization yielded poor results. We found the main reason to be that the matrices used in the SRL solutions (Equations 1 and 2) are ill-conditioned, resulting in instability. One possible explanation stems from the sparseness of the features. The DQN uses ReLU activations (Jarrett et al., 2009), which causes the network to learn sparse feature representations. For example, once the DQN completed training on Breakout, 96% of features were zero.\nOnce we added a regularization term, we found that the performance of the SRL algorithms improved. We experimented with the `2 and Bayesian Prior (BP) regularizers (λ ∈ [ 0, 102 ] ). While the `2 regularizer showed competitive performance in Breakout, we found that the BP performed better across domains (Figure 1, best regularizers chosen, shows the average score of each configuration following the explained evaluation procedure, for the different epochs). Moreover, the BP regularizer\n6The reader is referred to Ghavamzadeh et al. (2015) for an overview on using Bayesian methods in RL. 7Every three million DQN steps, referred to as one epoch (out of a total of 50 million steps). 8Each roll-out starts from a new (random) game and follows a policy until the agent loses all of its lives.\nwas not sensitive to the scale of the regularization coefficient. Regularizers in the range (10−1, 101) performed well across all domains. A table of average scores for different coefficients can be found in Appendix C.1. Note that we do not expect for much improvement as we replace back the original DQN weights after evaluation.\n(2) Data Gathering: We experimented with two mechanisms for generating data: (1) generating new data from the current policy, and (2) using the ER. We found that the data generation mechanism had a significant impact on the performance of the algorithms. When the data is generated only from the current DQN policy (without ER) the SRL solution resulted in poor performance compared to a solution using the ER (as was observed by Mnih et al. 2015). We believe that the main reason the ER works well is that the ER contains data sampled from multiple (past) policies, and therefore exhibits more exploration of the state space.\n(3) Policy Improvement: LSTD-Q and FQI are off-policy algorithms and can be applied iteratively on the same dataset (e.g. LSPI, Lagoudakis & Parr 2003). However, in practice, we found that performing multiple iterations did not improve the results. A possible explanation is that by improving the policy, the policy reaches new areas in the state space that are not represented well in the current ER, and therefore are not approximated well by the SRL solution and the current DRL network.'}, {'heading': '4.2 Atari Experiments', 'text': 'We next ran the full LS-DQN algorithm (Alg. 1) on five Atari domains: Asterix, Space Invaders, Breakout, Q-Bert and Bowling. We ran the LS-DQN using both DQN and DDQN as the DRL algorithm, and using both LSTD-Q and FQI as the SRL algorithms. We chose to run a LS-update every NDRL = 500k steps, for a total of 50M steps (SRLiters = 100). We used the current ER buffer as the ‘generated’ data in the LS-UPDATE function (line 7 in Alg. 1, NSRL = 1M ), and a regularization coefficient λ = 1 for the Bayesian prior solution (both for FQI and LSTQ-Q). We emphasize the we did not use any additional samples beyond the samples already obtained by the DRL algorithm.\nFigure 2 presents the learning curves of the DQN network, LS-DQN with LSTD-Q, and LS-DQN with FQI (referred to as DQN, LS-DQNLSTD-Q, and LS-DQNFQI, respectively) on three domains: Asterix, Space Invaders and Breakout. Note that we use the same evaluation process as described in Mnih et al. (2015). We were also interested in a test to measure differences between learning curves, and not only their maximal score. Hence we chose to perform Wilcoxon signed-rank test on the average scores between the three DQN variants. This non-parametric statistical test measures whether related samples differ in their means (Wilcoxon, 1945). We found that the learning curves for both LS-DQNLSTD-Q and LS-DQNFQI were statistically significantly better than those of DQN, with p-values smaller than 1e-15 for all three domains.\nTable 1 presents the maximum average scores along the learning curves of the five domains, when the SRL algorithms were incorporated into both DQN agents and DDQN agents (the notation is similar, i.e., LS-DDQNFQI)9. Our algorithm, LS-DQN, attained better performance compared to the\n9 Scores for DQN and DDQN were taken from Van Hasselt et al. (2016).\nvanilla DQN agents, as seen by the higher scores in Table 1 and Figure 2. We observe an interesting phenomenon for the game Asterix: In Figure 2, the DQN’s score “crashes” to zero (as was observed by Van Hasselt et al. 2016). LS-DQNLSTD-Q did not manage to resolve this issue, even though it achieved a significantly higher score that that of the DQN. LS-DQNFQI, however, maintained steady performance and did not “crash” to zero. We found that, in general, incorporating FQI as an SRL algorithm into the DRL agents resulted in improved performance.'}, {'heading': '4.3 Ablative Analysis', 'text': 'In the previous section, we saw that the LS-DQN algorithm has improved performance, compared to the DQN agents, across a number of domains. The goal of this section is to understand the reasons behind the LS-DQN’s improved performance by conducting an ablative analysis of our algorithm. For this analysis, we used a DQN agent that was trained on the game of Breakout, in the same manner as described in Section 4.1. We focus on analyzing the LS-DQNFQI algorithm, that has the same optimization objective as DQN (cf. Section 2), and postulate the following conjectures for its improved performance:\n(i) The SRL algorithms use a Bayesian regularization term, which is not included in the DQN objective.\n(ii) The SRL algorithms have less hyperparameters to tune and generate an explicit solution compared to SGD-based DRL solutions.\n(iii) Large-batch methods perform better than small-batch methods when combining DRL with SRL.\n(iv) SRL algorithms focus on training the last layer and are easier to optimize.\nThe Experiments: We started by analyzing the learning method of the last layer (i.e., the ‘shallow’ part of the learning process). We did this by optimizing the last layer, at each LS-UPDATE epoch, using (1) FQI with a Bayesian prior and a LS solution, and (2) an ADAM (Kingma & Ba, 2014) optimizer with and without an additional Bayesian prior regularization term in the loss function. We compared these approaches for different mini-batch sizes of 32, 512, and 4096 data points, and used λ = 1 for all experiments.\nRelating to conjecture (ii), note that the FQI algorithm has only one hyper-parameter to tune and produces an explicit solution using the whole dataset simultaneously. ADAM, on the other hand, has more hyper-parameters to tune and works on different mini-batch sizes.\nThe Experimental Setup: The experiments were done in a periodic fashion similar to Section 4.1, i.e., testing behavior in different epochs over a vanilla DQN run. For both ADAM and FQI, we first collected 80k data samples from the ER at each epoch. For ADAM, we performed 20 iterations over the data, where each iteration consisted of randomly permuting the data, dividing it into mini-batches and optimizing using ADAM over the mini-batches10. We then simulate the agent and report average scores across 20 trajectories.\nThe Results: Figure 3 depicts the difference between the average scores of (1) and (2) to that of the DQN baseline scores. We see that larger mini-batches result in improved performance. Moreover, the LS solution (FQI) outperforms the ADAM solutions for mini-batch sizes of 32 and 512 on most epochs, and even slightly outperforms the best of them (mini-batch size of 4096 and a Bayesian prior). In addition, a solution with a prior performs better than a solution without a prior.\nSummary: Our ablative analysis experiments strongly support conjectures (iii) and (iv) from above, for explaining LS-DQN’s improved performance. That is, large-batch methods perform better than small-batch methods when combining DRL with SRL as explained above; and SRL algorithms that focus on training only the last layer are easier to optimize, as we see that optimizing the last layer improved the score across epochs.\nWe finish this Section with an interesting observation. While the LS solution improves the performance of the DRL agents, we found that the LS solution weights are very close to the baseline DQN solution. See Appendix D, for the full results. Moreover, the distance was inversely proportional to the performance of the solution. That is, the FQI solution that performed the best, was the closest (in `2 norm) to the DQN solution, and vice versa. There were orders of magnitude differences between the norms of solutions that performed well and those that did not. Similar results, i.e., that large-batch solutions find solutions that are close to the baseline, have been reported in (Keskar et al., 2016). We further compare our results with the findings of Keskar et al. in the section to follow.'}, {'heading': '5 Related work', 'text': 'We now review recent works that are related to this paper.\nRegularization: The general idea of applying regularization for feature selection, and to avoid overfitting is a common theme in machine learning. However, applying it to RL algorithms is challenging due to the fact that these algorithms are based on finding a fixed-point rather than optimizing a loss function (Kolter & Ng, 2009).Value-based DRL approaches do not use regularization layers (e.g. pooling, dropout and batch normalization), which are popular in other deep learning methods. The DQN, for example, has a relatively shallow architecture (three convolutional layers, followed by two fully connected layers) without any regularization layers. Recently, regularization was introduced\n10 The selected hyper-parameters used for these experiments can be found in Appendix D, along with results for one iteration of ADAM.\nin problems that combine value-based RL with other learning objectives. For example, Hester et al. (2017) combine RL with supervised learning from expert demonstration, and introduce regularization to avoid over-fitting the expert data; and Kirkpatrick et al. (2017) introduces regularization to avoid catastrophic forgetting in transfer learning. SRL methods, on the other hand, perform well with regularization (Kolter & Ng, 2009) and have been shown to converge Farahmand et al. (2009).\nBatch size: Our results suggest that a large batch LS solution for the last layer of a value-based DRL network can significantly improve it’s performance. This result is somewhat surprising, as it has been observed by practitioners that using larger batches in deep learning degrades the quality of the model, as measured by its ability to generalize (Keskar et al., 2016).\nHowever, our method differs from the experiments performed by Keskar et al. 2016 and therefore does not contradict them, for the following reasons: (1) The LS-DQN Algorithm uses the large batch solution only for the last layer. The lower layers of the network are not affected by the large batch solution and therefore do not converge to a sharp minimum. (2) The experiments of (Keskar et al., 2016) were performed for classification tasks, whereas our algorithm is minimizing an MSE loss. (3) Keskar et al. showed that large-batch solutions work well when piggy-backing (warm-started) on a small-batch solution. Similarly, our algorithm mixes small and large batch solutions as it switches between them periodically.\nMoreover, it was recently observed that flat minima in practical deep learning model classes can be turned into sharp minima via re-parameterization without changing the generalization gap, and hence it requires further investigation Dinh et al. (2017). In addition, Hoffer et al. showed that large-batch training can generalize as well as small-batch training by adapting the number of iterations Hoffer et al. (2017). Thus, we strongly believe that our findings on combining large and small batches in DRL are in agreement with recent results of other deep learning research groups.\nDeep and Shallow RL: Using the last-hidden layer of a DNN as a feature extractor and learning the last layer with a different algorithm has been addressed before in the literature, e.g., in the context of transfer learning (Donahue et al., 2013). In RL, there have been competitive attempts to use SRL with unsupervised features to play Atari (Liang et al., 2016; Blundell et al., 2016), and to learn features automatically followed by a linear control rule (Song et al., 2016), but to the best of our knowledge, this is the first attempt that successfully combines DRL with SRL algorithms.'}, {'heading': '6 Conclusion', 'text': 'In this work we presented LS-DQN, a hybrid approach that combines least-squares RL updates within online deep RL. LS-DQN obtains the best of both worlds: rich representations from deep RL networks as well as stability and data efficiency of least squares methods. Experiments with two deep RL methods and two least squares methods revealed that a hybrid approach consistently improves over vanilla deep RL in the Atari domain. Our ablative analysis indicates that the success of the LS-DQN algorithm is due to the large batch updates made possible by using least squares.\nThis work focused on value-based RL. However, our hybrid linear/deep approach can be extended to other RL methods, such as actor critic (Mnih et al., 2016). More broadly, decades of research on linear RL methods have provided methods with strong guarantees, such as approximate linear programming (Desai et al., 2012) and modified policy iteration (Scherrer et al., 2015). Our approach shows that with the correct modifications, such as our Bayesian regularization term, linear methods can be combined with deep RL. This opens the door to future combinations of well-understood linear RL with deep representation learning.\nAcknowledgement This research was supported by the European Community’s Seventh Framework Program (FP7/2007-2013) under grant agreement 306638 (SUPREL). A. Tamar is supported in part by Siemens and the Viterbi Scholarship, Technion.'}], 'references': [{'title': 'Improving elevator performance using reinforcement learning', 'author': ['AG Barto', 'Crites', 'RH'], 'venue': 'Advances in neural information processing systems,', 'citeRegEx': 'Barto et al\\.,? \\Q1996\\E', 'shortCiteRegEx': 'Barto et al\\.', 'year': 1996}, {'title': 'The arcade learning environment: An evaluation platform for general agents', 'author': ['Bellemare', 'Marc G', 'Naddaf', 'Yavar', 'Veness', 'Joel', 'Bowling', 'Michael'], 'venue': 'Journal of Artificial Intelligence Research,', 'citeRegEx': 'Bellemare et al\\.,? \\Q2013\\E', 'shortCiteRegEx': 'Bellemare et al\\.', 'year': 2013}, {'title': 'Model-free episodic control', 'author': ['Blundell', 'Charles', 'Uria', 'Benigno', 'Pritzel', 'Alexander', 'Li', 'Yazhe', 'Ruderman', 'Avraham', 'Leibo', 'Joel Z', 'Rae', 'Jack', 'Wierstra', 'Daan', 'Hassabis', 'Demis'], 'venue': None, 'citeRegEx': 'Blundell et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Blundell et al\\.', 'year': 2016}, {'title': 'Bayesian inference in statistical analysis', 'author': ['Box', 'George EP', 'Tiao', 'George C'], 'venue': None, 'citeRegEx': 'Box et al\\.,? \\Q2011\\E', 'shortCiteRegEx': 'Box et al\\.', 'year': 2011}, {'title': 'Approximate dynamic programming via a smoothed linear program', 'author': ['Desai', 'Vijay V', 'Farias', 'Vivek F', 'Moallemi', 'Ciamac C'], 'venue': 'Operations Research,', 'citeRegEx': 'Desai et al\\.,? \\Q2012\\E', 'shortCiteRegEx': 'Desai et al\\.', 'year': 2012}, {'title': 'Sharp minima can generalize for deep nets', 'author': ['Dinh', 'Laurent', 'Pascanu', 'Razvan', 'Bengio', 'Samy', 'Yoshua'], 'venue': 'arXiv preprint arXiv:1703.04933,', 'citeRegEx': 'Dinh et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Dinh et al\\.', 'year': 2017}, {'title': 'Decaf: A deep convolutional activation feature for generic visual recognition', 'author': ['Donahue', 'Jeff', 'Jia', 'Yangqing', 'Vinyals', 'Oriol', 'Hoffman', 'Judy', 'Zhang', 'Ning', 'Tzeng', 'Eric', 'Darrell', 'Trevor'], 'venue': 'In Proceedings of the 30th international conference on machine learning', 'citeRegEx': 'Donahue et al\\.,? \\Q2013\\E', 'shortCiteRegEx': 'Donahue et al\\.', 'year': 2013}, {'title': 'Tree-based batch mode reinforcement learning', 'author': ['Ernst', 'Damien', 'Geurts', 'Pierre', 'Wehenkel', 'Louis'], 'venue': 'Journal of Machine Learning Research,', 'citeRegEx': 'Ernst et al\\.,? \\Q2005\\E', 'shortCiteRegEx': 'Ernst et al\\.', 'year': 2005}, {'title': 'Regularized policy iteration', 'author': ['Farahmand', 'Amir M', 'Ghavamzadeh', 'Mohammad', 'Mannor', 'Shie', 'Szepesvári', 'Csaba'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Farahmand et al\\.,? \\Q2009\\E', 'shortCiteRegEx': 'Farahmand et al\\.', 'year': 2009}, {'title': 'Learning from demonstrations for real world reinforcement learning', 'author': ['Hester', 'Todd', 'Vecerik', 'Matej', 'Pietquin', 'Olivier', 'Lanctot', 'Marc', 'Schaul', 'Tom', 'Piot', 'Bilal', 'Sendonaris', 'Andrew', 'Dulac-Arnold', 'Gabriel', 'Osband', 'Ian', 'Agapiou', 'John'], 'venue': 'arXiv preprint arXiv:1704.03732,', 'citeRegEx': 'Hester et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Hester et al\\.', 'year': 2017}, {'title': 'Neural networks for machine learning lecture 6a overview of mini–batch gradient descent', 'author': ['Hinton', 'Geoffrey', 'Srivastava', 'NiRsh', 'Swersky', 'Kevin'], 'venue': None, 'citeRegEx': 'Hinton et al\\.,? \\Q2012\\E', 'shortCiteRegEx': 'Hinton et al\\.', 'year': 2012}, {'title': 'Train longer, generalize better: closing the generalization gap in large batch training of neural networks', 'author': ['Hoffer', 'Elad', 'Hubara', 'Itay', 'Soudry', 'Daniel'], 'venue': 'arXiv preprint arXiv:1705.08741,', 'citeRegEx': 'Hoffer et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Hoffer et al\\.', 'year': 2017}, {'title': 'What is the best multi-stage architecture for object recognition', 'author': ['Jarrett', 'Kevin', 'Kavukcuoglu', 'Koray', 'LeCun', 'Yann'], 'venue': 'In Computer Vision,', 'citeRegEx': 'Jarrett et al\\.,? \\Q2009\\E', 'shortCiteRegEx': 'Jarrett et al\\.', 'year': 2009}, {'title': 'On large-batch training for deep learning: Generalization gap and sharp minima', 'author': ['Keskar', 'Nitish Shirish', 'Mudigere', 'Dheevatsa', 'Nocedal', 'Jorge', 'Smelyanskiy', 'Mikhail', 'Tang', 'Ping Tak Peter'], 'venue': 'arXiv preprint arXiv:1609.04836,', 'citeRegEx': 'Keskar et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Keskar et al\\.', 'year': 2016}, {'title': 'Adam: A method for stochastic optimization', 'author': ['Kingma', 'Diederik', 'Ba', 'Jimmy'], 'venue': 'arXiv preprint arXiv:1412.6980,', 'citeRegEx': 'Kingma et al\\.,? \\Q2014\\E', 'shortCiteRegEx': 'Kingma et al\\.', 'year': 2014}, {'title': 'Overcoming catastrophic forgetting in neural networks', 'author': ['Kirkpatrick', 'James', 'Pascanu', 'Razvan', 'Rabinowitz', 'Neil', 'Veness', 'Joel', 'Desjardins', 'Guillaume', 'Rusu', 'Andrei A', 'Milan', 'Kieran', 'Quan', 'John', 'Ramalho', 'Tiago', 'Grabska-Barwinska', 'Agnieszka'], 'venue': 'Proceedings of the National Academy of Sciences,', 'citeRegEx': 'Kirkpatrick et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Kirkpatrick et al\\.', 'year': 2017}, {'title': 'Regularization and feature selection in least-squares temporal difference learning', 'author': ['Kolter', 'J Zico', 'Ng', 'Andrew Y'], 'venue': 'In Proceedings of the 26th annual international conference on machine learning', 'citeRegEx': 'Kolter et al\\.,? \\Q2009\\E', 'shortCiteRegEx': 'Kolter et al\\.', 'year': 2009}, {'title': 'Least-squares policy iteration', 'author': ['Lagoudakis', 'Michail G', 'Parr', 'Ronald'], 'venue': 'Journal of machine learning research,', 'citeRegEx': 'Lagoudakis et al\\.,? \\Q2003\\E', 'shortCiteRegEx': 'Lagoudakis et al\\.', 'year': 2003}, {'title': 'State of the art control of atari games using shallow reinforcement learning', 'author': ['Liang', 'Yitao', 'Machado', 'Marlos C', 'Talvitie', 'Erik', 'Bowling', 'Michael'], 'venue': 'In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,', 'citeRegEx': 'Liang et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Liang et al\\.', 'year': 2016}, {'title': 'Reinforcement learning for robots using neural networks', 'author': ['Lin', 'Long-Ji'], 'venue': None, 'citeRegEx': 'Lin and Long.Ji.,? \\Q1993\\E', 'shortCiteRegEx': 'Lin and Long.Ji.', 'year': 1993}, {'title': 'Human-level control through deep reinforcement learning', 'author': ['Mnih', 'Volodymyr', 'Kavukcuoglu', 'Koray', 'Silver', 'David', 'Rusu', 'Andrei A', 'Veness', 'Joel', 'Bellemare', 'Marc G', 'Graves', 'Alex', 'Riedmiller', 'Martin', 'Fidjeland', 'Andreas K', 'Ostrovski', 'Georg'], 'venue': 'Nature, 518(7540):529–533,', 'citeRegEx': 'Mnih et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Mnih et al\\.', 'year': 2015}, {'title': 'Asynchronous methods for deep reinforcement learning', 'author': ['Mnih', 'Volodymyr', 'Badia', 'Adria Puigdomenech', 'Mirza', 'Mehdi', 'Graves', 'Alex', 'Lillicrap', 'Timothy P', 'Harley', 'Tim', 'Silver', 'David', 'Kavukcuoglu', 'Koray'], 'venue': 'In International Conference on Machine Learning,', 'citeRegEx': 'Mnih et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Mnih et al\\.', 'year': 2016}, {'title': 'Neural fitted q iteration–first experiences with a data efficient neural reinforcement learning method', 'author': ['Riedmiller', 'Martin'], 'venue': 'In European Conference on Machine Learning,', 'citeRegEx': 'Riedmiller and Martin.,? \\Q2005\\E', 'shortCiteRegEx': 'Riedmiller and Martin.', 'year': 2005}, {'title': 'Approximate modified policy iteration and its application to the game of tetris', 'author': ['Scherrer', 'Bruno', 'Ghavamzadeh', 'Mohammad', 'Gabillon', 'Victor', 'Lesner', 'Boris', 'Geist', 'Matthieu'], 'venue': 'Journal of Machine Learning Research,', 'citeRegEx': 'Scherrer et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Scherrer et al\\.', 'year': 2015}, {'title': 'Mastering the game of go with deep neural networks and tree', 'author': ['Silver', 'David', 'Huang', 'Aja', 'Maddison', 'Chris J', 'Guez', 'Arthur', 'Sifre', 'Laurent', 'Van Den Driessche', 'George', 'Schrittwieser', 'Julian', 'Antonoglou', 'Ioannis', 'Panneershelvam', 'Veda', 'Lanctot', 'Marc'], 'venue': 'search. Nature,', 'citeRegEx': 'Silver et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Silver et al\\.', 'year': 2016}, {'title': 'Linear feature encoding for reinforcement learning', 'author': ['Song', 'Zhao', 'Parr', 'Ronald E', 'Liao', 'Xuejun', 'Carin', 'Lawrence'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Song et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Song et al\\.', 'year': 2016}, {'title': 'Reinforcement Learning: An Introduction', 'author': ['Sutton', 'Richard', 'Barto', 'Andrew'], 'venue': None, 'citeRegEx': 'Sutton et al\\.,? \\Q1998\\E', 'shortCiteRegEx': 'Sutton et al\\.', 'year': 1998}, {'title': 'A deep hierarchical approach to lifelong learning in minecraft', 'author': ['Tessler', 'Chen', 'Givony', 'Shahar', 'Zahavy', 'Tom', 'Mankowitz', 'Daniel J', 'Mannor', 'Shie'], 'venue': 'Proceedings of the National Conference on Artificial Intelligence (AAAI),', 'citeRegEx': 'Tessler et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Tessler et al\\.', 'year': 2017}, {'title': 'An analysis of temporal-difference learning with function approximation', 'author': ['Tsitsiklis', 'John N', 'Van Roy', 'Benjamin'], 'venue': 'IEEE transactions on automatic control', 'citeRegEx': 'Tsitsiklis et al\\.,? \\Q1997\\E', 'shortCiteRegEx': 'Tsitsiklis et al\\.', 'year': 1997}, {'title': 'Deep reinforcement learning with double q-learning', 'author': ['Van Hasselt', 'Hado', 'Guez', 'Arthur', 'Silver', 'David'], 'venue': 'Proceedings of the National Conference on Artificial Intelligence (AAAI),', 'citeRegEx': 'Hasselt et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Hasselt et al\\.', 'year': 2016}, {'title': 'Dueling network architectures for deep reinforcement learning', 'author': ['Wang', 'Ziyu', 'Schaul', 'Tom', 'Hessel', 'Matteo', 'van Hasselt', 'Hado', 'Lanctot', 'Marc', 'de Freitas', 'Nando'], 'venue': 'In Proceedings of The 33rd International Conference on Machine Learning,', 'citeRegEx': 'Wang et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Wang et al\\.', 'year': 2016}, {'title': 'Individual comparisons by ranking methods', 'author': ['Wilcoxon', 'Frank'], 'venue': 'Biometrics bulletin,', 'citeRegEx': 'Wilcoxon and Frank.,? \\Q1945\\E', 'shortCiteRegEx': 'Wilcoxon and Frank.', 'year': 1945}, {'title': 'Graying the black box: Understanding dqns', 'author': ['Zahavy', 'Tom', 'Ben-Zrihem', 'Nir', 'Mannor', 'Shie'], 'venue': 'In Proceedings of The 33rd International Conference on Machine Learning,', 'citeRegEx': 'Zahavy et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Zahavy et al\\.', 'year': 2016}], 'referenceMentions': [{'referenceID': 32, 'context': 'Recent advancements in DRL using convolutional neural networks demonstrated learning of expressive features (Zahavy et al., 2016; Wang et al., 2016) and state-of-the-art performance in challenging tasks such as video games (Mnih et al.', 'startOffset': 108, 'endOffset': 148}, {'referenceID': 30, 'context': 'Recent advancements in DRL using convolutional neural networks demonstrated learning of expressive features (Zahavy et al., 2016; Wang et al., 2016) and state-of-the-art performance in challenging tasks such as video games (Mnih et al.', 'startOffset': 108, 'endOffset': 148}, {'referenceID': 20, 'context': ', 2016) and state-of-the-art performance in challenging tasks such as video games (Mnih et al. 2015; Tessler et al. 2017; Mnih et al. 2016), and Go (Silver et al.', 'startOffset': 82, 'endOffset': 139}, {'referenceID': 27, 'context': ', 2016) and state-of-the-art performance in challenging tasks such as video games (Mnih et al. 2015; Tessler et al. 2017; Mnih et al. 2016), and Go (Silver et al.', 'startOffset': 82, 'endOffset': 139}, {'referenceID': 21, 'context': ', 2016) and state-of-the-art performance in challenging tasks such as video games (Mnih et al. 2015; Tessler et al. 2017; Mnih et al. 2016), and Go (Silver et al.', 'startOffset': 82, 'endOffset': 139}, {'referenceID': 7, 'context': 'Fitted Q Iteration (FQI): The FQI algorithm (Ernst et al., 2005) is a batch SRL algorithm that computes iterative approximations of the Q-function using regression.', 'startOffset': 44, 'endOffset': 64}, {'referenceID': 7, 'context': 'The FQI algorithm can also be used with non-linear function approximations such as trees (Ernst et al., 2005) and neural networks (Riedmiller, 2005).', 'startOffset': 89, 'endOffset': 109}, {'referenceID': 20, 'context': 'The DQN algorithm (Mnih et al., 2015) can be viewed as online form of FQI.', 'startOffset': 18, 'endOffset': 37}, {'referenceID': 20, 'context': 'Deep Q-Network (DQN): The DQN algorithm (Mnih et al., 2015) learns the Q function by minimizing the mean squared error of the Bellman equation, defined as Est,at,rt,st+1‖Qθ(st, at)− yt‖(2)2, where yt = rt + γmaxa′ Qθtarget(st+1, a ′ ).', 'startOffset': 40, 'endOffset': 59}, {'referenceID': 21, 'context': 'Such networks have been used extensively in deep RL recently (e.g., Mnih et al. 2015; Van Hasselt et al. 2016; Mnih et al. 2016).', 'startOffset': 61, 'endOffset': 128}, {'referenceID': 20, 'context': '); namely, Breakout and Qbert, using the vanilla DQN implementation (Mnih et al., 2015).', 'startOffset': 68, 'endOffset': 87}, {'referenceID': 12, 'context': 'The DQN uses ReLU activations (Jarrett et al., 2009), which causes the network to learn sparse feature representations.', 'startOffset': 30, 'endOffset': 52}, {'referenceID': 13, 'context': ', that large-batch solutions find solutions that are close to the baseline, have been reported in (Keskar et al., 2016).', 'startOffset': 98, 'endOffset': 119}, {'referenceID': 13, 'context': 'This result is somewhat surprising, as it has been observed by practitioners that using larger batches in deep learning degrades the quality of the model, as measured by its ability to generalize (Keskar et al., 2016).', 'startOffset': 196, 'endOffset': 217}, {'referenceID': 13, 'context': '(2) The experiments of (Keskar et al., 2016) were performed for classification tasks, whereas our algorithm is minimizing an MSE loss.', 'startOffset': 23, 'endOffset': 44}, {'referenceID': 6, 'context': ', in the context of transfer learning (Donahue et al., 2013).', 'startOffset': 38, 'endOffset': 60}, {'referenceID': 18, 'context': 'In RL, there have been competitive attempts to use SRL with unsupervised features to play Atari (Liang et al., 2016; Blundell et al., 2016), and to learn features automatically followed by a linear control rule (Song et al.', 'startOffset': 96, 'endOffset': 139}, {'referenceID': 2, 'context': 'In RL, there have been competitive attempts to use SRL with unsupervised features to play Atari (Liang et al., 2016; Blundell et al., 2016), and to learn features automatically followed by a linear control rule (Song et al.', 'startOffset': 96, 'endOffset': 139}, {'referenceID': 25, 'context': ', 2016), and to learn features automatically followed by a linear control rule (Song et al., 2016), but to the best of our knowledge, this is the first attempt that successfully combines DRL with SRL algorithms.', 'startOffset': 79, 'endOffset': 98}, {'referenceID': 21, 'context': 'However, our hybrid linear/deep approach can be extended to other RL methods, such as actor critic (Mnih et al., 2016).', 'startOffset': 99, 'endOffset': 118}, {'referenceID': 4, 'context': 'More broadly, decades of research on linear RL methods have provided methods with strong guarantees, such as approximate linear programming (Desai et al., 2012) and modified policy iteration (Scherrer et al.', 'startOffset': 140, 'endOffset': 160}, {'referenceID': 23, 'context': ', 2012) and modified policy iteration (Scherrer et al., 2015).', 'startOffset': 38, 'endOffset': 61}], 'year': 2017, 'abstractText': 'Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN) have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy. Batch reinforcement learning methods with linear representations, on the other hand, are more stable and require less hyper parameter tuning. Yet, substantial feature engineering is necessary to achieve good results. In this work we propose a hybrid approach – the Least Squares Deep Q-Network (LS-DQN), which combines rich feature representations learned by a DRL algorithm with the stability of a linear least squares method. We do this by periodically re-training the last hidden layer of a DRL network with a batch least squares update. Key to our approach is a Bayesian regularization term for the least squares update, which prevents over-fitting to the more recent data. We tested LS-DQN on five Atari games and demonstrate significant improvement over vanilla DQN and Double-DQN. We also investigated the reasons for the superior performance of our method. Interestingly, we found that the performance improvement can be attributed to the large batch size used by the LS method when optimizing the last layer.', 'creator': None}","reinforcement learning (rl) is a field of research that uses dynamic programing (dp; bertsekas 2008), among other approaches, to solve sequential decision making problems. the main challenge in applying dp to real world problems is an exponential growth of computational requirements as the problem size increases, known as the curse of dimensionality (bertsekas, 2008). 31st conference on neural information processing systems (nips 2017), long beach, ca, usa. rl tackles the curse of dimensionality by approximating terms in the dp calculation such as the value function or policy. popular function approximators for this task include deep neural networks, henceforth termed deep rl (drl), and linear architectures, henceforth termed shallow rl (srl). srl methods have enjoyed wide popularity over the years (see, e.g., tsitsiklis et al. 1997; bertsekas 2008 for extensive reviews). in particular, batch algorithms based on a least squares (ls) approach, such as least squares temporal difference (lstd, lagoudakis & parr 2003) and fitted-q iteration (fqi, ernst et al. 2005) are known to be stable and data efficient. however, the success of these algorithms crucially depends on the quality of the feature representation. ideally, the representation encodes rich, expressive features that can accurately represent the value function. however, in practice, finding such good features is difficult and often hampers the usage of linear function approximation methods. in drl, on the other hand, the features are learned together with the value function in a deep architecture. recent advancements in drl using convolutional neural networks demonstrated learning of expressive features (zahavy et al., 2016; wang et al., 2016) and state-of-the-art performance in challenging tasks such as video games (mnih et al. 2015; tessler et al. 2017; mnih et al. 2016), and go (silver et al., 2016). to date, the most impressive drl results (e.g., the works of mnih et al. 2015, mnih et al. 2016) were obtained using online rl algorithms, based on a stochastic gradient descent (sgd) procedure. on the one hand, srl is stable and data efficient. on the other hand, drl learns powerful representations. this motivates us to ask: can we combine drl with srl to leverage the benefits of both? in this work, we develop a hybrid approach that combines batch srl algorithms with online drl. our main insight is that the last layer in a deep architecture can be seen as a linear representation, with the preceding layers encoding features. therefore, the last layer can be learned using standard srl algorithms. following this insight, we propose a method that repeatedly re-trains the last hidden layer of a drl network with a batch srl algorithm, using data collected throughout the drl run. we focus on value-based drl algorithms (e.g., the popular dqn of mnih et al. 2015) and on srl based on ls methods1, and propose the least squares dqn algorithm (ls-dqn). key to our approach is a novel regularization term for the least squares method that uses the drl solution as a prior in a bayesian least squares formulation. our experiments demonstrate that this hybrid approach significantly improves performance on the atari benchmark for several combinations of drl and srl methods. to support our results, we performed an in-depth analysis to tease out the factors that make our hybrid approach outperform drl. interestingly, we found that the improved performance is mainly due to the large batch size of srl methods compared to the small batch size that is typical for drl.","reinforcement learning (rl) is a field of research that uses dynamic programing (dp; bertsekas 2008), among other approaches, to solve sequential decision making problems the main challenge in applying dp to real world problems is an exponential growth of computational requirements as the problem size increases, known as the curse of dimensionality (bertsekas, 2008) popular function approximators for this task include deep neural networks, henceforth termed deep rl (drl), and linear architectures, henceforth termed shallow rl (srl) in particular, batch algorithms based on a least squares (ls) approach, such as least squares temporal difference (lstd, lagoudakis & parr 2003) and fitted-q iteration (fqi, ernst et al recent advancements in drl using convolutional neural networks demonstrated learning of expressive features (zahavy et al this motivates us to ask: can we combine drl with srl to leverage the benefits of both? in this work, we develop a hybrid approach that combines batch srl algorithms with online drl following this insight, we propose a method that repeatedly re-trains the last hidden layer of a drl network with a batch srl algorithm, using data collected throughout the drl run our experiments demonstrate that this hybrid approach significantly improves performance on the atari benchmark for several combinations of drl and srl methods",0.9165684580802917,0.8714510202407837,0.8934405446052551
ICLR_2020_1491,CURSOR-BASED ADAPTIVE QUANTIZATION FOR DEEP NEURAL NETWORK,"{'source': 'CRF', 'title': None, 'authors': [], 'emails': [], 'sections': [{'heading': '1 INTRODUCTION', 'text': 'Deep learning (DL) has achieved great successes in varied fields such as gaming, natural language processing, speech recognition, computer vision and so on. However, its huge computational burden and large memory consumption still intimidate many potential applications, especially for mobile devices and embedded systems.\nA number of efforts have been devoted to compress the DL model size and accelerate its training and test speed. These efforts can be roughly categorized into four major classes: network pruning (Han et al. (2015); Anwar et al. (2015); Peng et al. (2019); Zhuang et al. (2018)), low rank approximation (Tai et al. (2015); Wang et al. (2018); Hayashi et al. (2019)), knowledge distillation (Hinton et al. (2015); Zagoruyko & Komodakis (2016)), and network quantization (Courbariaux & Bengio (2016); Lin et al. (2015); Wu et al. (2015); Polino et al. (2018); Zhang et al. (2018)). Among them, network quantization methods, jointly optimizing the whole network weights, activations or gradients with low bit (such as 8 bits or even 1 bit), show great potential in compressing model size and accelerating inference time. In addition, quantization based approaches are preferable for mobile devices and embedded systems since these devices are gradually equipped by specifically designed low bit computing hardware.\nAlthough existing quantization based approaches, which mainly use one fixed bit to represent the whole DNN model, yields encouraging compression ratio while keeping the model’s performance, we argue that simply using only a fixed bit for quantization is not the optimal choice for the trade-off between a model size and its performance. For example, to run a model on chips with strict memory limitations, 1 bit or 2 bits’ quantization suffers from severe accuracy loss (Rastegari et al. (2016)) while 16 bits’ or 8 bits’ quantization cannot significantly reduce the model size.\nTo address the above problem, we propose a cursor based adaptive quantization method to derive a different number of bits in different layers for DNN model compression, i.e., we search for the best\nconfiguration of different bit quantization for different layers in a neural network model. Distinctive from most algorithms aforementioned, our approach is motivated by recent neural architecture search (NAS) that aims to find better performance neural architecture with less calculations or less size automatically. The key in our algorithm is using a continuous cursor that represents the bit quantization scheme for each layer. For different layers, many cursors will be adaptively searched during the NAS process. Since the cursor itself is continuous and the whole search procedure can be considered as a differentiable architecture search (DAS) process, which can be effectively solved based on an alternative optimization strategy. A novel cost function that considers the model compression and prediction accuracy is also proposed in the DAS process. In the searching process of the cursors, a quantization process is applied to compress the model size at the same time. To reduce the possible quantization noise and local convergence problem, we make use of the closest two integer bits to the cursor to quantize the weights for each layer in a DNN model. We validate our proposed method with image classification tasks on CIFAR10,CIFAR100 and ImageNet. Comprehensive experiments on some backbone DNN models such as ResNet20, ResNet18, ResNet56 and MobileNetV2 show that the proposed cursor based quantization method achieves remarkably better performance of compression ratio with ignorable accuracy drop or even better accuracy.\nIn summary, the contributions of this work are four-fold:\n• We cast the adaptive quantization of neural network as a problem of neural architecture search. A continuous cursor is proposed to represent the possible quantization bit, leading to a more efficient search space.\n• A novel regularization function is proposed to optimize model compression in the search process. Thus the search for the cursor position and weights can be efficiently solved in an alternative optimization manner.\n• Two nearest neighbor integers to the cursor are adopted with a carefully designed strategy to implement the quantization of the network to reduce the quantization noise and avoid possible local convergence.\n• We comprehensively evaluate the proposed adaptive quantization method on some benchmark datasets and achieve new state-of-the-art performance for different number of bits quantization of neural network.'}, {'heading': '2 RELATED WORK', 'text': 'Recently, a lot of new quantization approaches have been proposed, enabling the quantized compressed model to compete with their full precision counterparts. Clustering method is applied for the weight codebook representation (Han et al. (2015)), and then the network is retrained to get better quantized centroids. In (Zhang et al. (2018)), the authors jointly trained a DNN and its associated quantizes to reduce the noticeable predication accuracy gap between the quantized model and its full precision one. A direct differentiable quantization method was introduced in (Louizos et al. (2018)) with promising test accuracy. A new activation quantization method that takes an activation clipping parameter was proposed in (Choi et al. (2018)) to ensure the suitable quantization scale.\nSome efforts have been taken on quantization of the neural network with a different number of bits used for different layers. In (Lin et al. (2015)), signal-to-quantization-noise ratio (SQNR) is applied on layer weight to evaluate the effects of quantization error. Based on SQNR, different bits were used for quantization of each layer, yielding about 20% model size reduction without accuracy loss in their tests. The authors of (Wang et al. (2018)) proposed an automated mixed precision quantization scheme based on reinforcement learning (RL) to achieve better latency on different hardware platforms such as edge devices and cloud data center. They also claimed that their actorcritic model produced efficient actions that result in better latency and less energy consumption with negligible loss of accuracy.\nIn the past few years, a new trend has been witnessed for network design, i.e., neural architecture search (NAS). RL based approaches are first utilized to generate network with high accuracy (Zoph & Le (2016)), and they also build a strong basis for the following recent works such as (Gao et al. (2019); Guo et al. (2018)). Then, evolution based approach (Liang et al. (2018) is further applied to obtain the possible optimal solution in the large search space. Both of these two category approaches tend to yield large amount of computational burden because NAS is treated as a blackbox\noptimization problem in a discrete domain, yielding a large number of architecture evaluations, and thus run very slow even on the most advanced GPU machine. To alleviate this problem, in 2018, the authors (Liu et al. (2018)) proposed a differentiable approach to accelerate the search of a good neural network by relaxation of the possible operation on the cell level structure. Wu et al. recently proposed a new approach to find the mixed bits for different layers by applying differentiable NAS (DNAS) method based on a model of super net (Wu et al. (2018)), which is a kind of directed acyclic graph. They considered the quantization as a problem of sampling on a stochastic super net, where a Gumbel softmax function is applied to make the sampling process differentiable.\nWe cast the different bit quantization for DNN as a cursor based adaptive architecture search problem, and it is different from the traditional direct quantization works and the learning based mixed bits’ quantization approaches. Moreover, it is also distinctive from DARTs and DNAS in the methodology itself.'}, {'heading': '3 CURSOR-BASED ADAPTIVE QUANTIZATION', 'text': ''}, {'heading': '3.1 NEURAL ARCHITECTURE SEARCH', 'text': 'It is well known that DNN model needs much time to design its structure. Neural architecture search (NAS) recently emerged as a new methodology to overcome this challenge. It designs the optimal architecture of a neural network by considering all possible factors such as number of layers, width of each layer, different operators in each layer and so on. Two key concepts are related to a NAS process, i.e., search space and search strategy. All the possible combinations of the major factors that determine a network structure constitute the search space. Generally speaking, the search space of a DNN is very large, leading to a huge computational task. As such, the previous NAS works first devise normal and reduction cell (Pham et al. (2018)). Such kind of motif is then repeated to build the final neural network. Another definition is about search strategy, that is, how to transverse in such a large search space. With each searched network structure, the performance of it will be evaluated. A typical search method is random search, however, its computational efficiency is not ideal. Therefore, most recent works (Cai et al. (2018); Liu et al. (2018)) have been proposed along this big direction to improve the search efficiency as much as possible.'}, {'heading': '3.2 SEARCH SPACE FOR QUANTIZATION PROBLEM', 'text': 'Quantization has also been a very hot research topic in the past few years. Rounding function, vector quantization or stochastic function are typically applied to implement quantization to compact the model size while maintaining equivalent performance or acceptable loss. Some other approaches also use stochastic or probabilistic methods to quantize the neural network. Most previous methods simply apply one kind of bit quantization to the whole network due to the simplicity of implementation. A few recent works begin to utilize different bit quantization scheme to further improve the compression ratio and prediction accuracy. If we consider quantization choice as a part of the neural architecture, we can estimate its corresponding search space. Let us take Resent20 as an example and if we decide to quantize the neural network with the possible bit width of 1, 2, 4, 8, 16, 32, then all the possible quantization choices for ResNet20 will be 620. In the context of NAS, this is a very large number for the search space. Hence, evaluation of so many designs one by one seems infeasible right now.'}, {'heading': '3.3 DIFFERENTIABLE CURSOR SEARCH FOR ADAPTIVE QUANTIZATION', 'text': 'The discrete search space of the above quantization scheme is so large. If we further consider the possible bit choice for each layer as a virtual continuous cursor in the range of [1, 32], the cursors then become significant parts of the architecture for a neural network model. Here, we define the cursor as a position that is related to the quantization choice for each layer. Its value is a floatingpoint number within [1, 32]. If we assume a DNN has N layers, and each layer has a different cursor, denoted by c1, c2, ..., cN , together with their weights of WC , our goal is to find a good combination of c1, c2, ..., cN to achieve better prediction accuracy and compression ratio. As such, for the whole neural network it can be described as an optimization problem that minimizes the loss on the validation data after training through the minimization of the loss on the training data as follows (Liu et al. (2018)):\nminE(x′, y′)DT (LossT (C,WC)) s.t. WC∗ = argminE(x, y)DV (LossV (C,WC)) (1)\nwhere C represents the cursor vector, WC∗ is the weights corresponding to the optimal C, LossT (C,WC) and LossV (C,WC) is the respective training and validation loss function based on the cursors and weights with condition of C. DT and DV represents the training and validation dataset respectively, (x, y) and (x′, y′) means data from the training and validation dataset. It should be noted that using training and validation data is a tradition to derive the weight parameters and architecture parameters in the field of NAS, which is a little bit different from the other problems in deep learning. For simplicity and efficiency, in this paper, we let LossT (C,WC) equals to LossV (C,WC) and assume they share the same form of Loss(C,WC). To consider both the prediction accuracy and model size, we design the loss function as a combination of cross entropy and model size as follows:\nLoss(C,WC) = CrossEntropy(C,WC) + λ× LossC (2)\nwhere CrossEntropy(C,WC) is the widely used cross entropy function, encoding the prediction accuracy of the model. The reason why we add a regularization item to the loss function is first because it may determines the model’s performance compromise between the accuracy and quantization, which directly determines the model size, and λ is a regularization coefficient that adjusts the trade-off of accuracy and compression. In addition, it may prevent overfitting to some extent. Concerning the loss related to LossC , we focus on the model size change with quantization. So we design it in the form of Eq.(3), which will be introduced in details in the next subsection.\nThe above process is a bi-level optimization problem, which requires to deduce higher order derivatives and is hard to obtain an exact solution. An approximated iterative solution can be applied instead, so we alternatively take the optimization strategy in weight and cursor space to update WC based on the training losses fromDT and renew C based on the validation losses fromDV . By solving this bi-level optimization problem using an alternative approximation approach, the cursors can be efficiently searched by gradient based optimization approach such as Adam. Our later experimental results also show that this alternative optimization method may yield a good solution with high compression ratio and accuracy. Compared to the original discrete search space, this search method is more efficient because the design of continuous cursor and the direct gradient based optimization approach. The whole differentiable cursor search for adaptive quantization based on the alternative optimization of WC and C is described in Algorithm 1. With the training and validation set, initialized cursor value and a pretrained model as inputs, our algorithm first quantizes the weights in each layer of the network using the two most close integers to the cursor and calculates the loss based on the training data for forward process, and then it updates the original 32 bit weight by gradient descent algorithm. For the subsequent validation, it also first quantizes the network with the two most close integers to the cursor, and use them to obtain the validation error, then our algorithm utilizes this error to update the cursors with gradient descend algorithm. It should be emphasized that we utilize the original 32 bit weight in the above training and validation step, and use it for sharing when implementing quantization with the cursor’s two neighbor integers. The key step in the algorithm about quantization with the nearest two integers to the cursor will be elaborated in the subsequent section. The outputs of the whole algorithm are rounded cursor values for each layer and its quantized network model.\nIt should be noted that our proposed cursor based differentiable search is different from DARTs (Liu et al. (2018)) in the following three aspects. First, DARTs method considers the possible operation in each layer as a mixture of primitive operations. Here, we directly make use of cursor to represent the quantization bit for each layer, no similar mixture operation exists in the whole search algorithm. Second, in DARTs, each primitive operation is assigned with a probability through a softmax function. Cursor based search is optimized directly without probability. Third, DARTs approach concentrates on the cell structure, but we apply the DAS directly on the whole network. Compared to DNAS (Wu et al. (2018)), our approach is also distinctive. For DNAS, the authors build a stochastic super net first to describe all the possible quantization choices, then a sampling approach based on Gumbel softmax function that enables the discrete distribution to be continuous and differentiable is applied in each layer of the super net. Our cursor based differentiable search has no super net or sampling process in the pipeline. Hence, the subsequent solutions to the optimization problem is also completely different. In short, the proposed method requires no relaxation anymore as in both DARTs and DNAS approach.\ninput : The training set DT , validation set DV , initialized cursors C, pretrained 32-bit weight W , and the batch size n while not reaching the target epochs or not converge do Sample data from training data DT ; Quantize the network using two integers that are closest to the cursor, calculate the loss L\non training data with Eq.(2); Update the weight W by gradient descent W ′ =W −∇W L(C,W ); Sample data from validation data DV ; Quantize the network using two integers that are closest to the cursor, calculate the loss L\non validation data with Eq. (2); Update the cursor C by gradient descent C ′ = C −∇C L(C,W );\nend output: Rounded cursor values for each layer and quantized network\nAlgorithm 1: Differentiable Cursor Search for Adaptive Quantization'}, {'heading': '3.4 TRAINING FOR NETWORK QUANTIZATION', 'text': 'Aiming for DNN quantization, we attempt to apply the cursor that represents the bit to quantize the weight layers. Unfortunately, the cursor obtained during the search is a fractional number, which cannot be directly used for quantization. One choice is to round the cursor to some integers, but it may cause rather large quantization error if we choose the rather distant bits. On the other hand, if we directly round the cursor to its nearest integer, it may not efficiently represent the variation of cursor. For example, if cursor1 and cursor2 for different epochs in the same layer are 2.6 and 2.8 respectively, they will be rounded to the same integer 3, yielding no change in the model size for this layer when implementing quantization. In addition, in the whole search process, such one integer choice may result in local convergence because the iteration process of one integer quantization may get stuck in a local minimum region for the cursor search. To alleviate the above two problems, we propose instead to make use of the nearest lower and upper integer bound at the same time in the search training process. Compared to directly using the nearest one neighbor to quantize, the lower and upper integer bounds may produce more variations in the loss function that describes the quantization effects, yielding effective gradient changes to update the cursors more efficiently. Subsequent experiments also demonstrate that this design can obtain better quantization performance compared to simply applying rounding function on the searched cursor. As such, the loss function for model size part in Eq.(2) is designed as follows:\nLossC = (Cost(C)) γ (3)\nwhere γ is a postive coefficient, Cost(C) is a quantization related continuous cost function with cursor C as its variable. In this work and for the convenience of implementation, we further design it as:\nLossC = ( ∑N i=1 Si × ci∑N i=1 Si )γ (4)\nwhere Si is defined as the size of the ith layer in bits when each of parameters is represented by 1 bit. In fact, for a trained model, the size of a layer in bits, i.e., Si , is a constant for lay i. Since ci is a continuous cursor, we may consider the above equation differentiable with respect to ci.\nWhen implementing the quantization for each layer, we utilize the DoReFa-Net (Zhou et al. (2016)) quantization:\nwk = 2Qk( tanh(w)\n2max(|tanh(w)|) + 0.5)− 1 (5)\nwhere w represents the full precision weight of a model and Qk(.) is the k-bit quantization function that transforms a continuous value x ∈ [0, 1] to a k-bit output y ∈ [0, 1] as below:\ny = round((2k − 1)× x)\n2k − 1 (6)\nwhere round function is the typical rounding operation used in quantization. In other words, in the process of quantization, after searching the possible quantization bit of ci in each layer, its\ncorresponding two nearest neighbor integers will be applied to Eq.(6) and Eq.(5) to quantize each layer of the network.\nIn the forward process of the proposed quantization scheme, output of the ith layer, assuming Oi, can be described with the following equation:\nOi = M∑ j=1 ReLu(1− |ci − qj |)×Oj s.t. M∑ j=1 ReLu(1− |ci − qj |) = 1 (7)\nwhereReLu(x) = max(0, x) is the Rectified Linear Unit that zeros out the negative values, ci is the cursor in one layer, qj denotes the jth quantization bit width within M total quantization choices, and Oj is the corresponding output of one convolution layer of the model when quantizing with qj . Due to the introduction of ReLu(x) in the forward process, the whole loss function may not be differetiable at the point of zero. In such a corner case, fortunately, the gradient may be still also estimated through the method of straight-through-estimator(STE) (Guo (2018)), whose details are beyond the scope of this paper. In this work, we mainly consider qj ∈ {1, 2, 3, 4, 5, 6, 7, 8}, M = 8 and ci ∈ [1, 8], because it has been observed that with greater than 8 bits, the neural network’s performance almost has no degradation (Elthakeb et al. (2018)). We also concentrate on such a design in all our experiments. In fact, there is also a new trend that investigates the possible quantization with bit that is not power of 2 (Elthakeb et al. (2018); Park et al. (2018b;a); Wang et al. (2018)). In addition, some hardware such as FPGA also gradually supports efficient quantization using such bits (Wang et al. (2018); Wei et al. (2019)).\nBased on Eq.(7), it can be found that there is only two closest integers that take effects in each layer’s output. As such, our proposed search algorithm can much reduce the coupling of different quantization operations, leading to more effective search of a good cursor. Assuming the cursor’s lower and upper bound integer in the ith layer is ai1 and ai2, we can define two coefficients di1 and di2 as below:\ndi1 = 1− |ci − ai1|; di2 = 1− |ai2 − ci| (8)\nwhere ci represents a cursor searched in the ith layer, di1 and di2 represents how close the bounds are to a cursor. The closer it is, the larger the coefficient is. A continuous move of a cursor then adjusts the weights continuously. Then, based on Eq.(7), given the inputX of one convolution layer, output of it (we only consider the quantization of the convolution layers in this work ) in the forward process of quantization can be simplified and described with the following equation:\nOi = di1 × (Conv(X,Wi1) + di2 × Conv(X,Wi2)) (9)\nwhere Wi1 and Wi2 are the temporary weights in one layer after quantization using ai1 and ai2 based on its corresponding 32 bit weight, Conv is the convolution operation. The 32 bit weight will be updated as the whole algorithm iterates, while Wi1 and Wi2 will be recalculated in the forward process based on the new ai1 and ai2 . In other words, we activate the two closest bits to the cursor, and sum the convolution results of these two quantization bit choices based on the coefficients of the L1 distance. As such, the whole process is differentiable. This can be also intuitively explained by that the outcome of the desired quantization scheme for each layer in the forward process might be represented by a weighted sum of the two different quantization schemes using the approximated closest bit choices. Hence, the proposed scheme may find the best quantization scheme for the whole network in the cursor searching process based on the alternative optimization solution.\nAfter the approximate alternative optimization approach converges or reaches the target epoch number, the final quantization bit in each layer can be obtained by applying rounding operation on each cursor for inference. The final quantized model may also need to be finetuned based on the quantization bit.'}, {'heading': '4 EXPERIMENTS', 'text': 'Currently, we only apply quantization on the weights and use full precision activations. In addition, we also follow the traditions in the domain of DNN quantization to avoid the quantization of the first and last layer in a model. In all the experiments, we take ResNet18, ResNet20, ResNet56 (He et al. (2015)) or MobileNetV2 (Sandler et al. (2018)) as the backbone models. Please be noted that these\nmodels should be pretrained to obtain the floating point models first. For the initialization of cursors in each layer, all of them are set with 4 bits for the convenience of iteration. When the cursors are obtained by our method, the model may be further fine tuned to get its final accuracy, which is a practical tradition in the fields of NAS and quantization.\nAs for the parameter λ in Eq.(2) and γ in the loss of model size in Eq.(3), a rather optimal set of them is chosen as (0.25, 0.3) after trials. Based on our experiments, we also generalize a trial rule for λ. We control the balance of the two loss parts in Eq.(2) to ensure the ratio of the first and second part between [0.5, 2.0]. We also study the influence of λ in the experiments to show that in most cases, the cursor based adaptive quantization scheme is not senstive to its change if λ is at a larger interval of λ ≥ 0.1. Concerning the learning rate schedule of weight and cursor, we apply cosine annealing method to adjust them. The minimum learning rate for them is 0.001 and 0.0001 respectively.'}, {'heading': '4.1 COMPARISON OF MODEL SIZE LOSS', 'text': 'To show the validity of quantization approach using two integer bounds nearest to the cursor, we first implement the search process by comparing it to using only one nearest integer of the cursor. We analyze their model size losses, i.e., Eq.(3), to show the great distinction in the training process.\nHere we apply ResNet20 on CIFAR10 dataset to demonstrate the optimization process. For illustrative purpose, we only draw the training and validation loss change of the model size, i.e., the second term of Eq.(2), in the first 20 epochs. As shown in Figure 1, the red curve represents the training and validation loss of model size using one nearest integer to implement quantization, while the blue one denotes the training and validation loss of size obtained by using two neighbor integers nearest to the curser searched by the proposed scheme. The major differences in these two tests lie at the quantization choices. In fact, we also tried some other parameters and random initialization for one integer quantization scheme, and similar curves can be found. Obviously, the blue one looks more smooth and natural for a convergence process. The red loss may lead to a strong possibility that the cursors are stuck in a local minimum region instead. From this Figure, we can clearly notice that the training and validation loss of the model size using only one integer quantization will keep constant immediately in the first epoch, which is NOT desired for cursor search because it will cause no change in the model size anymore. In fact, the cursor values obtained by the one neighbor scheme tend to be 1 bit for all layers. The reason why the one integer quantization scheme fails may be because, in most cases, the weights in one layer span a rather small range, one lower integer quantization may lead to the same quantization results on the weights in the training process. Such same quantization results further yield almost no change in the backward gradient process, which is not beneficial for the optimal cursor search. The designed two integers’ quantization process, on the other hand, can map the cursor to two different integer values, leading to efficient change in the model size even for the weights in rather a small value range. Figure 1 also shows that Eq.(4) is continuous and differntiable.'}, {'heading': '4.2 SEARCH PROCESS ANALYSIS', 'text': 'To get some insights of our adaptive cursor search algorithm, we investigate its iteration process in this subsection. For illustration only, we take MobileNetV2 on CIFAR10 as an example. Its search process is depicted in Figure 2 with the quantization bits ignored due to space limitation. Here the abscissa and vertical coordinate respectively represents the compression ratio and prediction accuracy. It should be noted that here our proposed algorithm runs 10 epochs only to clearly show the variation of performance. In addition, because of the cosine annealing scheduler, such an iteration process may also be representative. From Figure 2, we observe that for the proposed adaptive cursor search scheme, it first begins at the lower left region (lower accuracy and compression) and then gradually assembles to the upper right region (higher accuracy and compression). Meanwhile, there is some small vibrations in the whole process, for example, from epoch 8 to epoch 9, there is some increase in accuracy as well as compression ratio, but from epoch 9 to epoch 10, there is a slight reduction in both measures. It can also be noticed that the search process is rather stable and gathers to the final upper right region with better accuracy and compression ratio. We also observed similar pattern for ResNet20 and ResNet56 on CIFAR10, but we ignored the picture of them because of space limitation. The reason why the search process of our method can reach to a region with high prediction accuracy and compression ratio may be due to the alternative optimization approach to solve this bi-level problem with two goals. In addition, the regularization item may also play a positive role in this process.'}, {'heading': '4.3 IMPACT OF REGULARIZATION COEFFICIENT', 'text': 'The regularization coefficient λ in Eq.(2) determines the balance between the model precision and size. In this part, we carry out some experiments to analyze the influence of it on the whole performance. We choose λ = 0.9, 0.7, 0.5, 0.25, 0.1, 0.05, 0.01, and we test its effects on the quantized model. For the purpose of illustration, we test ResNet20 on CIFAR10. To directly show the effects of our cursor based differentiable search, we do NOT implement finetune step for all these results after finishing the cursor search. The results of the quantized ResNet20 on CIFAR10 is demonstrated in Table 1, and all the results are obtained by implementing the search with 200 epochs.\nFrom Table 1, we can observe that forλ >= 0.1, the whole performance of the proposed quantization method is rather steady, that is, the accuracy and compression ratio of the quantized model maintain at a concentrated region with the accuracy about 90% while the compression ratio about 29.00. When λ < 0.1, the cursor based adaptive quantization approach may still have a good performance of prediction but gradually loses its effects on model compression. This can be explained that when the regularization becomes gradually weak, it does NOT exert its compression effects so well as when the coefficient is large. This further validates the effectiveness of the regularization function proposed in this paper.'}, {'heading': '4.4 CIFAR10 RESULTS', 'text': 'We demonstrate our cursor based adaptive quantization algorithm on CIFAR10 benchmark dataset with ResNet20, ResNet56 an MobileNetV2. For ResNet20,we compare the accuracy and compression ratio of the proposed approach to some related or similar works such as DNAS (Wu et al. (2018)), TTQ (Zhu et al. (2016)), PACT (Choi et al. (2018)) and LQE (Zhang et al. (2018)) with Resnet20 on CIFAR-10, and the details of accuracy and compression ratio are shown in Table 2. It can be noticed that, compared to the other related works, our method achieves much better compression ratio while achieving comparable or better classification accuracy on CIFAR10 dataset. The reason why the proposed approach is better than the quantization methods such as LQE, TTQ and PACT may be due to the adaptive cursor based search mechanism. By considering both the model accuracy and compression ratio, the cursor based approach can effectively search different quantization bit for each layer as a whole, leading to better compression ratio with better accuracy. Compared to DNAS, the reason for our better performance in terms of CR is partially due to that the two closest integers’ quantization scheme produces less quantization error in each layer. In addition, it may be also because of our multiple lower bits’ design in the search process.\nWe also apply the proposed approach to ResNet56 and compare its performance with DNAS (Wu et al. (2018)), and the results are recorded in Table 5. We further test the proposed approach on MobilenetV2, with the results shown in Table 6. Because of space limitation, we put the detailed Tables and descriptions in Appendix.'}, {'heading': '4.5 CIFAR100 RESULTS', 'text': 'To further show the effectiveness of the proposed scheme, we test our method on CIFAR100 dataset using ResNet20, ResNet56 and MobileNetV2. We illustrate compressed ResNet20’s performance compared to the original one on CIFAR100 in Table 3, it should be pointed out that here we also do not fine tune the original model, so its accuracy may not be the best one in the literature. For ResNet20, our approach achieves a good compression ratio of 11.6 while maintaining a comparable accuracy of 68.18%.\nThe performances of the quantized network of ResNet56 and MobileNetV2 on CIFAR100 datasets are presented in Table 7 and in Table 8 respectively, which are also placed in Appendix due to limited space. We notice that both quantized models show a little better accuracy with impressive compression ratios of 17.2 and 12.9 for ResNet56 and MobileNetV2 respectively.'}, {'heading': '4.6 IMAGENET RESULTS', 'text': 'In this subsection, we apply ResNet18 and MobileNetV2 to ImageNet dataset, which is a much larger dataset compared to CIFAR10 and CIFAR100. Here, as in (Han et al. (2015); Wang et al. (2018)), we present two sets of our results, i.e., the most efficient result and most accurate one to compare more conveniently.\nWe record the performance of the proposed method with ResNet18 on ImageNet in Table 4 as well as some comparisons to LQE (Zhang et al. (2018)), TTQ (Zhu et al. (2016)), PACT (Choi et al. (2018)) methods. From Table 4, it can be noticed that, compared to the original 32 bit model, the most accurate result of our algorithm achieves a promising compression rate of 13.9 with a slight accuracy drop of 0.15%, and for the most efficient one, our algorithm achieves an accuracy of 68.80% and an impressive compression ratio of 18.1. The most accurate result of our algorithm shows much better accuracy over LQE, TTQ and PACT methods although the compression ratio is a little bit smaller. As for the most efficient one, both the accuracy and compression ratio are better than those of LQE, TTQ and PACT, validating the effectiveness of the proposed scheme. The results of MobileNetV2 on ImageNet are illustrated in Appendix, please refer to Table 9 for details.'}, {'heading': '5 CONCLUSIONS', 'text': 'In this paper, we have proposed a novel cursor based DAS algorithm for obtaining the mixed precision DNN model. Different from most of the traditional approaches, which choose quantization configuration using heuristics or learning based rules, we adaptively choose the quantization bit for each layer in the DNN model from the perspective of NAS. A cursor based search algorithm with alternative manner is applied for efficient optimization. The nearest two neighbor integers to the cursor are used to implement the quantization in the training process to reduce the quantization noise and avoid local convergence. The result of our algorithm is the adaptive bit width choice for different layers as a whole. Extensive experiments with some typical models demonstrate that the proposed approach provides dramatic compression capability with accuracy on par with or better than the state-of-the-art of methods on benchmark datasets. In the near future, we will apply the proposed scheme to object detection tasks to further show its possible wider application. We may also utilize some hardware platforms to test some performance such as inference time and resource consumption.'}, {'heading': 'A APPENDIX', 'text': 'Due to space limitations, we put a number of experimental results here.\nA.1 RESNET56 ON CIFAR10\nWe apply the proposed approach to ResNet56 and compare its performance with DNAS (Wu et al. (2018)), and the results are recorded in Table 5. In this Table, we notice that ResNet56 compressed using our quantization approach shows a higher compression ratio of 20.6 compared to the most efficient DNAS’s result of 18.9, while its accuracy is almost the same. As for the most accurate one of DNAS, our quantized model is still comparable, while the compression ratio is much higher compared to 14.6. It should also be pointed out that due to different environments and other implementation differences in the whole process, the accuracy of our ResNet56 baseline is not as good as the one in DNAS. It should also be noted that DNAS takes cutout operation in its experiments, we also test such an operation to show the comparison. It may be noticed that our test with cutout achieves a accuracy of 94.48% with an impressive compression ratio of 23.3.\nA.2 MOBILENETV2 ON CIFAR10\nWe further apply the proposed approach to MobilenetV2, which is a typical DL model for mobile devices and embedded systems, with the results shown in Table 6. It can be noticed that our adaptive cursor based quantization shows a better classification accuracy of 93.28% as well as a promising compression ratio of 12.4.\nA.3 RESNET56 ON CIFAR100\nWe also test ResNet56 on CIFAR100 and present its corresponding results in Table 7. We can see that the proposed algorithm yields a little bit better accuracy(1.05%) together with an impressive compression ratio of 17.2.\nA.4 MOBILENETV2 ON CIFAR100\nThe quantized MobilenetV2 on CIFAR100 and its corresponding results are demonstrated in Table 8. As for MobileNetV2, the proposed algorithm yields a little bit better accuracy together with an impressive compression ratio of 12.9.\nA.5 MOBILENETV2 ON IMAGENET\nThe performance of MobileNetV2 on ImageNet is illustrated in Table 9 together with comparisons to some related works such as HAQ (Wang et al. (2018)) and deep compression (Han et al. (2015)). In Table 9, we notice that, for the most accurate result, the quantized MobileNetV2 model using our approach shows a slight accuracy loss (71.65% vs 72.19% of the original 32 bit model) while achieves an encouraging compression ratio of 9.1. It may also be observed that the accuracy of our most accurate one is a little bit higher than the corresponding most accurate results of HAQ and deep compression together with a better compression ratio. While for the most efficient one, our algorithm shows a compression ratio of 14.3, which is better than that of HAQ, but smaller than that of deep compression. However, our approach demonstrates a dramatically better accuracy of 70.59% compared to the corresponding 66.75% of HAQ and 58.07% of deep compression, which again validated the advantage of our algorithm.'}], 'references': [{'title': 'Structured pruning of deep convolutional neural networks', 'author': ['Sajid Anwar', 'Kyuyeon Hwang', 'Wonyong Sung'], 'venue': 'CoRR, abs/1512.08571,', 'citeRegEx': 'Anwar et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Anwar et al\\.', 'year': 2015}, {'title': 'Path-level network transformation for efficient architecture', 'author': ['Han Cai', 'Jiacheng Yang', 'Weinan Zhang', 'Song Han', 'Yong Yu'], 'venue': 'search. CoRR,', 'citeRegEx': 'Cai et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Cai et al\\.', 'year': 2018}, {'title': 'PACT: parameterized clipping activation for quantized neural networks', 'author': ['Jungwook Choi', 'Zhuo Wang', 'Swagath Venkataramani', 'Pierce I-Jen Chuang', 'Vijayalakshmi Srinivasan', 'Kailash Gopalakrishnan'], 'venue': 'CoRR, abs/1805.06085,', 'citeRegEx': 'Choi et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Choi et al\\.', 'year': 2018}, {'title': 'Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1', 'author': ['Matthieu Courbariaux', 'Yoshua Bengio'], 'venue': 'CoRR, abs/1602.02830,', 'citeRegEx': 'Courbariaux and Bengio.,? \\Q2016\\E', 'shortCiteRegEx': 'Courbariaux and Bengio.', 'year': 2016}, {'title': 'Releq: A reinforcement learning approach for deep quantization of neural networks', 'author': ['Ahmed T. Elthakeb', 'Prannoy Pilligundla', 'Amir Yazdanbakhsh', 'Sean Kinzer', 'Hadi Esmaeilzadeh'], 'venue': 'CoRR, abs/1811.01704,', 'citeRegEx': 'Elthakeb et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Elthakeb et al\\.', 'year': 2018}, {'title': 'Graphnas: Graph neural architecture search with reinforcement learning', 'author': ['Yang Gao', 'Hong Yang', 'Peng Zhang', 'Chuan Zhou', 'Yue Hu'], 'venue': 'CoRR, abs/1904.09981,', 'citeRegEx': 'Gao et al\\.,? \\Q2019\\E', 'shortCiteRegEx': 'Gao et al\\.', 'year': 2019}, {'title': 'IRLAS: inverse reinforcement learning for architecture', 'author': ['Minghao Guo', 'Zhao Zhong', 'Wei Wu', 'Dahua Lin', 'Junjie Yan'], 'venue': 'search. CoRR,', 'citeRegEx': 'Guo et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Guo et al\\.', 'year': 2018}, {'title': 'A survey on methods and theories of quantized neural networks', 'author': ['Yunhui Guo'], 'venue': 'CoRR, abs/1808.04752,', 'citeRegEx': 'Guo.,? \\Q2018\\E', 'shortCiteRegEx': 'Guo.', 'year': 2018}, {'title': 'Deep compression: Compressing deep neural network with pruning, trained quantization and huffman', 'author': ['Song Han', 'Huizi Mao', 'William J. Dally'], 'venue': 'coding. CoRR,', 'citeRegEx': 'Han et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Han et al\\.', 'year': 2015}, {'title': 'Einconv: Exploring unexplored tensor decompositions for convolutional neural networks. ArXiv', 'author': ['Kohei Hayashi', 'Taiki Yamaguchi', 'Yohei Sugawara', 'Shin ichi Maeda'], 'venue': None, 'citeRegEx': 'Hayashi et al\\.,? \\Q1908\\E', 'shortCiteRegEx': 'Hayashi et al\\.', 'year': 1908}, {'title': 'Deep residual learning for image recognition', 'author': ['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'], 'venue': 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR),', 'citeRegEx': 'He et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'He et al\\.', 'year': 2016}, {'title': 'Distilling the Knowledge in a Neural Network', 'author': ['Geoffrey Hinton', 'Oriol Vinyals', 'Jeff Dean'], 'venue': 'arXiv e-prints, art', 'citeRegEx': 'Hinton et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Hinton et al\\.', 'year': 2015}, {'title': 'Evolutionary architecture search for deep multitask networks. CoRR, abs/1803.03745, 2018', 'author': ['Jason Zhi Liang', 'Elliot Meyerson', 'Risto Miikkulainen'], 'venue': None, 'citeRegEx': 'Liang et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Liang et al\\.', 'year': 2018}, {'title': 'Fixed point quantization of deep convolutional networks', 'author': ['Darryl Dexu Lin', 'Sachin S. Talathi', 'V. Sreekanth Annapureddy'], 'venue': 'CoRR, abs/1511.06393,', 'citeRegEx': 'Lin et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Lin et al\\.', 'year': 2015}, {'title': 'DARTS: differentiable architecture', 'author': ['Hanxiao Liu', 'Karen Simonyan', 'Yiming Yang'], 'venue': 'search. CoRR,', 'citeRegEx': 'Liu et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Liu et al\\.', 'year': 2018}, {'title': 'Relaxed quantization for discretized neural networks', 'author': ['Christos Louizos', 'Matthias Reisser', 'Tijmen Blankevoort', 'Efstratios Gavves', 'Max Welling'], 'venue': 'CoRR, abs/1810.01875,', 'citeRegEx': 'Louizos et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Louizos et al\\.', 'year': 2018}, {'title': 'Precision highway for ultra lowprecision quantization', 'author': ['Eunhyeok Park', 'Dongyoung Kim', 'Sungjoo Yoo', 'Peter Vajda'], 'venue': 'CoRR, abs/1812.09818,', 'citeRegEx': 'Park et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Park et al\\.', 'year': 2018}, {'title': 'Value-aware quantization for training and inference of neural networks. CoRR, abs/1804.07802, 2018b', 'author': ['Eunhyeok Park', 'Sungjoo Yoo', 'Peter Vajda'], 'venue': 'URL http://arxiv.org/abs/1804', 'citeRegEx': 'Park et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Park et al\\.', 'year': 2018}, {'title': 'Collaborative channel pruning for deep networks', 'author': ['Hanyu Peng', 'Jiaxiang Wu', 'Shifeng Chen', 'Junzhou Huang'], 'venue': 'In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,', 'citeRegEx': 'Peng et al\\.,? \\Q2019\\E', 'shortCiteRegEx': 'Peng et al\\.', 'year': 2019}, {'title': 'Efficient neural architecture search via parameter sharing', 'author': ['Hieu Pham', 'Melody Y. Guan', 'Barret Zoph', 'Quoc V. Le', 'Jeff Dean'], 'venue': 'CoRR, abs/1802.03268,', 'citeRegEx': 'Pham et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Pham et al\\.', 'year': 2018}, {'title': 'Model compression via distillation and quantization', 'author': ['Antonio Polino', 'Razvan Pascanu', 'Dan Alistarh'], 'venue': 'CoRR, abs/1802.05668,', 'citeRegEx': 'Polino et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Polino et al\\.', 'year': 2018}, {'title': 'Xnor-net: Imagenet classification using binary convolutional neural networks', 'author': ['Mohammad Rastegari', 'Vicente Ordonez', 'Joseph Redmon', 'Ali Farhadi'], 'venue': 'CoRR, abs/1603.05279,', 'citeRegEx': 'Rastegari et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Rastegari et al\\.', 'year': 2016}, {'title': 'Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation', 'author': ['Mark Sandler', 'Andrew G. Howard', 'Menglong Zhu', 'Andrey Zhmoginov', 'Liang-Chieh Chen'], 'venue': 'CoRR, abs/1801.04381,', 'citeRegEx': 'Sandler et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Sandler et al\\.', 'year': 2018}, {'title': 'Convolutional neural networks with low-rank regularization', 'author': ['Cheng Tai', 'Tong Xiao', 'Yi Zhang', 'Xiaogang Wang', 'Weinan E'], 'venue': 'arXiv e-prints, art', 'citeRegEx': 'Tai et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Tai et al\\.', 'year': 2015}, {'title': 'Fpga-based hybrid-type implementation of quantized neural networks for remote sensing applications', 'author': ['Xin Wei', 'Wenchao Liu', 'Lei Chen', 'Long Ma', 'He Chen', 'Yin Zhuang'], 'venue': 'doi: 10.3390/s19040924. URL', 'citeRegEx': 'Wei et al\\.,? \\Q2019\\E', 'shortCiteRegEx': 'Wei et al\\.', 'year': 2019}, {'title': 'Mixed precision quantization of convnets via differentiable neural architecture', 'author': ['Bichen Wu', 'Yanghan Wang', 'Peizhao Zhang', 'Yuandong Tian', 'Peter Vajda', 'Kurt Keutzer'], 'venue': 'search. CoRR,', 'citeRegEx': 'Wu et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Wu et al\\.', 'year': 2018}, {'title': 'Quantized convolutional neural networks for mobile', 'author': ['Jiaxiang Wu', 'Cong Leng', 'Yuhang Wang', 'Qinghao Hu', 'Jian Cheng'], 'venue': 'devices. CoRR,', 'citeRegEx': 'Wu et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Wu et al\\.', 'year': 2015}, {'title': 'Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer', 'author': ['Sergey Zagoruyko', 'Nikos Komodakis'], 'venue': 'CoRR, abs/1612.03928,', 'citeRegEx': 'Zagoruyko and Komodakis.,? \\Q2016\\E', 'shortCiteRegEx': 'Zagoruyko and Komodakis.', 'year': 2016}, {'title': 'Lq-nets: Learned quantization for highly accurate and compact deep neural networks', 'author': ['Dongqing Zhang', 'Jiaolong Yang', 'Dongqiangzi Ye', 'Gang Hua'], 'venue': 'CoRR, abs/1807.10029,', 'citeRegEx': 'Zhang et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Zhang et al\\.', 'year': 2018}, {'title': 'Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients', 'author': ['Shuchang Zhou', 'Zekun Ni', 'Xinyu Zhou', 'He Wen', 'Yuxin Wu', 'Yuheng Zou'], 'venue': 'CoRR, abs/1606.06160,', 'citeRegEx': 'Zhou et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Zhou et al\\.', 'year': 2016}, {'title': 'Discrimination-aware channel pruning for deep neural networks', 'author': ['Zhuangwei Zhuang', 'Mingkui Tan', 'Bohan Zhuang', 'Jing Liu', 'Yong Guo', 'Qingyao Wu', 'Junzhou Huang', 'Jin-Hui Zhu'], 'venue': 'CoRR, abs/1810.11809,', 'citeRegEx': 'Zhuang et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Zhuang et al\\.', 'year': 2018}, {'title': 'Neural architecture search with reinforcement learning', 'author': ['Barret Zoph', 'Quoc V. Le'], 'venue': 'CoRR, abs/1611.01578,', 'citeRegEx': 'Zoph and Le.,? \\Q2016\\E', 'shortCiteRegEx': 'Zoph and Le.', 'year': 2016}], 'referenceMentions': [{'referenceID': 7, 'context': 'These efforts can be roughly categorized into four major classes: network pruning (Han et al. (2015); Anwar et al.', 'startOffset': 83, 'endOffset': 101}, {'referenceID': 0, 'context': '(2015); Anwar et al. (2015); Peng et al. (2019); Zhuang et al. (2018)), low rank approximation (Tai et al.', 'startOffset': 8, 'endOffset': 70}, {'referenceID': 0, 'context': '(2015); Anwar et al. (2015); Peng et al. (2019); Zhuang et al. (2018)), low rank approximation (Tai et al. (2015); Wang et al. (2018); Hayashi et al. (2019)), knowledge distillation (Hinton et al.', 'startOffset': 8, 'endOffset': 157}, {'referenceID': 0, 'context': '(2015); Anwar et al. (2015); Peng et al. (2019); Zhuang et al. (2018)), low rank approximation (Tai et al. (2015); Wang et al. (2018); Hayashi et al. (2019)), knowledge distillation (Hinton et al. (2015); Zagoruyko & Komodakis (2016)), and network quantization (Courbariaux & Bengio (2016); Lin et al.', 'startOffset': 8, 'endOffset': 204}, {'referenceID': 0, 'context': '(2015); Anwar et al. (2015); Peng et al. (2019); Zhuang et al. (2018)), low rank approximation (Tai et al. (2015); Wang et al. (2018); Hayashi et al. (2019)), knowledge distillation (Hinton et al. (2015); Zagoruyko & Komodakis (2016)), and network quantization (Courbariaux & Bengio (2016); Lin et al.', 'startOffset': 8, 'endOffset': 234}, {'referenceID': 21, 'context': 'For example, to run a model on chips with strict memory limitations, 1 bit or 2 bits’ quantization suffers from severe accuracy loss (Rastegari et al. (2016)) while 16 bits’ or 8 bits’ quantization cannot significantly reduce the model size.', 'startOffset': 134, 'endOffset': 158}, {'referenceID': 4, 'context': 'Clustering method is applied for the weight codebook representation (Han et al. (2015)), and then the network is retrained to get better quantized centroids.', 'startOffset': 69, 'endOffset': 87}, {'referenceID': 4, 'context': 'Clustering method is applied for the weight codebook representation (Han et al. (2015)), and then the network is retrained to get better quantized centroids. In (Zhang et al. (2018)), the authors jointly trained a DNN and its associated quantizes to reduce the noticeable predication accuracy gap between the quantized model and its full precision one.', 'startOffset': 69, 'endOffset': 182}, {'referenceID': 4, 'context': 'Clustering method is applied for the weight codebook representation (Han et al. (2015)), and then the network is retrained to get better quantized centroids. In (Zhang et al. (2018)), the authors jointly trained a DNN and its associated quantizes to reduce the noticeable predication accuracy gap between the quantized model and its full precision one. A direct differentiable quantization method was introduced in (Louizos et al. (2018)) with promising test accuracy.', 'startOffset': 69, 'endOffset': 438}, {'referenceID': 2, 'context': 'A new activation quantization method that takes an activation clipping parameter was proposed in (Choi et al. (2018)) to ensure the suitable quantization scale.', 'startOffset': 98, 'endOffset': 117}, {'referenceID': 2, 'context': 'A new activation quantization method that takes an activation clipping parameter was proposed in (Choi et al. (2018)) to ensure the suitable quantization scale. Some efforts have been taken on quantization of the neural network with a different number of bits used for different layers. In (Lin et al. (2015)), signal-to-quantization-noise ratio (SQNR) is applied on layer weight to evaluate the effects of quantization error.', 'startOffset': 98, 'endOffset': 309}, {'referenceID': 2, 'context': 'A new activation quantization method that takes an activation clipping parameter was proposed in (Choi et al. (2018)) to ensure the suitable quantization scale. Some efforts have been taken on quantization of the neural network with a different number of bits used for different layers. In (Lin et al. (2015)), signal-to-quantization-noise ratio (SQNR) is applied on layer weight to evaluate the effects of quantization error. Based on SQNR, different bits were used for quantization of each layer, yielding about 20% model size reduction without accuracy loss in their tests. The authors of (Wang et al. (2018)) proposed an automated mixed precision quantization scheme based on reinforcement learning (RL) to achieve better latency on different hardware platforms such as edge devices and cloud data center.', 'startOffset': 98, 'endOffset': 612}, {'referenceID': 2, 'context': 'A new activation quantization method that takes an activation clipping parameter was proposed in (Choi et al. (2018)) to ensure the suitable quantization scale. Some efforts have been taken on quantization of the neural network with a different number of bits used for different layers. In (Lin et al. (2015)), signal-to-quantization-noise ratio (SQNR) is applied on layer weight to evaluate the effects of quantization error. Based on SQNR, different bits were used for quantization of each layer, yielding about 20% model size reduction without accuracy loss in their tests. The authors of (Wang et al. (2018)) proposed an automated mixed precision quantization scheme based on reinforcement learning (RL) to achieve better latency on different hardware platforms such as edge devices and cloud data center. They also claimed that their actorcritic model produced efficient actions that result in better latency and less energy consumption with negligible loss of accuracy. In the past few years, a new trend has been witnessed for network design, i.e., neural architecture search (NAS). RL based approaches are first utilized to generate network with high accuracy (Zoph & Le (2016)), and they also build a strong basis for the following recent works such as (Gao et al.', 'startOffset': 98, 'endOffset': 1186}, {'referenceID': 2, 'context': 'A new activation quantization method that takes an activation clipping parameter was proposed in (Choi et al. (2018)) to ensure the suitable quantization scale. Some efforts have been taken on quantization of the neural network with a different number of bits used for different layers. In (Lin et al. (2015)), signal-to-quantization-noise ratio (SQNR) is applied on layer weight to evaluate the effects of quantization error. Based on SQNR, different bits were used for quantization of each layer, yielding about 20% model size reduction without accuracy loss in their tests. The authors of (Wang et al. (2018)) proposed an automated mixed precision quantization scheme based on reinforcement learning (RL) to achieve better latency on different hardware platforms such as edge devices and cloud data center. They also claimed that their actorcritic model produced efficient actions that result in better latency and less energy consumption with negligible loss of accuracy. In the past few years, a new trend has been witnessed for network design, i.e., neural architecture search (NAS). RL based approaches are first utilized to generate network with high accuracy (Zoph & Le (2016)), and they also build a strong basis for the following recent works such as (Gao et al. (2019); Guo et al.', 'startOffset': 98, 'endOffset': 1281}, {'referenceID': 2, 'context': 'A new activation quantization method that takes an activation clipping parameter was proposed in (Choi et al. (2018)) to ensure the suitable quantization scale. Some efforts have been taken on quantization of the neural network with a different number of bits used for different layers. In (Lin et al. (2015)), signal-to-quantization-noise ratio (SQNR) is applied on layer weight to evaluate the effects of quantization error. Based on SQNR, different bits were used for quantization of each layer, yielding about 20% model size reduction without accuracy loss in their tests. The authors of (Wang et al. (2018)) proposed an automated mixed precision quantization scheme based on reinforcement learning (RL) to achieve better latency on different hardware platforms such as edge devices and cloud data center. They also claimed that their actorcritic model produced efficient actions that result in better latency and less energy consumption with negligible loss of accuracy. In the past few years, a new trend has been witnessed for network design, i.e., neural architecture search (NAS). RL based approaches are first utilized to generate network with high accuracy (Zoph & Le (2016)), and they also build a strong basis for the following recent works such as (Gao et al. (2019); Guo et al. (2018)).', 'startOffset': 98, 'endOffset': 1300}, {'referenceID': 2, 'context': 'A new activation quantization method that takes an activation clipping parameter was proposed in (Choi et al. (2018)) to ensure the suitable quantization scale. Some efforts have been taken on quantization of the neural network with a different number of bits used for different layers. In (Lin et al. (2015)), signal-to-quantization-noise ratio (SQNR) is applied on layer weight to evaluate the effects of quantization error. Based on SQNR, different bits were used for quantization of each layer, yielding about 20% model size reduction without accuracy loss in their tests. The authors of (Wang et al. (2018)) proposed an automated mixed precision quantization scheme based on reinforcement learning (RL) to achieve better latency on different hardware platforms such as edge devices and cloud data center. They also claimed that their actorcritic model produced efficient actions that result in better latency and less energy consumption with negligible loss of accuracy. In the past few years, a new trend has been witnessed for network design, i.e., neural architecture search (NAS). RL based approaches are first utilized to generate network with high accuracy (Zoph & Le (2016)), and they also build a strong basis for the following recent works such as (Gao et al. (2019); Guo et al. (2018)). Then, evolution based approach (Liang et al. (2018) is further applied to obtain the possible optimal solution in the large search space.', 'startOffset': 98, 'endOffset': 1354}, {'referenceID': 14, 'context': 'To alleviate this problem, in 2018, the authors (Liu et al. (2018)) proposed a differentiable approach to accelerate the search of a good neural network by relaxation of the possible operation on the cell level structure.', 'startOffset': 49, 'endOffset': 67}, {'referenceID': 14, 'context': 'To alleviate this problem, in 2018, the authors (Liu et al. (2018)) proposed a differentiable approach to accelerate the search of a good neural network by relaxation of the possible operation on the cell level structure. Wu et al. recently proposed a new approach to find the mixed bits for different layers by applying differentiable NAS (DNAS) method based on a model of super net (Wu et al. (2018)), which is a kind of directed acyclic graph.', 'startOffset': 49, 'endOffset': 402}, {'referenceID': 17, 'context': 'As such, the previous NAS works first devise normal and reduction cell (Pham et al. (2018)).', 'startOffset': 72, 'endOffset': 91}, {'referenceID': 1, 'context': 'Therefore, most recent works (Cai et al. (2018); Liu et al.', 'startOffset': 30, 'endOffset': 48}, {'referenceID': 1, 'context': 'Therefore, most recent works (Cai et al. (2018); Liu et al. (2018)) have been proposed along this big direction to improve the search efficiency as much as possible.', 'startOffset': 30, 'endOffset': 67}, {'referenceID': 14, 'context': 'As such, for the whole neural network it can be described as an optimization problem that minimizes the loss on the validation data after training through the minimization of the loss on the training data as follows (Liu et al. (2018)):', 'startOffset': 217, 'endOffset': 235}, {'referenceID': 14, 'context': 'It should be noted that our proposed cursor based differentiable search is different from DARTs (Liu et al. (2018)) in the following three aspects.', 'startOffset': 97, 'endOffset': 115}, {'referenceID': 14, 'context': 'It should be noted that our proposed cursor based differentiable search is different from DARTs (Liu et al. (2018)) in the following three aspects. First, DARTs method considers the possible operation in each layer as a mixture of primitive operations. Here, we directly make use of cursor to represent the quantization bit for each layer, no similar mixture operation exists in the whole search algorithm. Second, in DARTs, each primitive operation is assigned with a probability through a softmax function. Cursor based search is optimized directly without probability. Third, DARTs approach concentrates on the cell structure, but we apply the DAS directly on the whole network. Compared to DNAS (Wu et al. (2018)), our approach is also distinctive.', 'startOffset': 97, 'endOffset': 717}, {'referenceID': 29, 'context': 'When implementing the quantization for each layer, we utilize the DoReFa-Net (Zhou et al. (2016)) quantization:', 'startOffset': 78, 'endOffset': 97}, {'referenceID': 6, 'context': 'In such a corner case, fortunately, the gradient may be still also estimated through the method of straight-through-estimator(STE) (Guo (2018)), whose details are beyond the scope of this paper.', 'startOffset': 132, 'endOffset': 143}, {'referenceID': 4, 'context': 'In this work, we mainly consider qj ∈ {1, 2, 3, 4, 5, 6, 7, 8}, M = 8 and ci ∈ [1, 8], because it has been observed that with greater than 8 bits, the neural network’s performance almost has no degradation (Elthakeb et al. (2018)).', 'startOffset': 207, 'endOffset': 230}, {'referenceID': 4, 'context': 'In this work, we mainly consider qj ∈ {1, 2, 3, 4, 5, 6, 7, 8}, M = 8 and ci ∈ [1, 8], because it has been observed that with greater than 8 bits, the neural network’s performance almost has no degradation (Elthakeb et al. (2018)). We also concentrate on such a design in all our experiments. In fact, there is also a new trend that investigates the possible quantization with bit that is not power of 2 (Elthakeb et al. (2018); Park et al.', 'startOffset': 207, 'endOffset': 428}, {'referenceID': 4, 'context': 'In this work, we mainly consider qj ∈ {1, 2, 3, 4, 5, 6, 7, 8}, M = 8 and ci ∈ [1, 8], because it has been observed that with greater than 8 bits, the neural network’s performance almost has no degradation (Elthakeb et al. (2018)). We also concentrate on such a design in all our experiments. In fact, there is also a new trend that investigates the possible quantization with bit that is not power of 2 (Elthakeb et al. (2018); Park et al. (2018b;a); Wang et al. (2018)).', 'startOffset': 207, 'endOffset': 471}, {'referenceID': 4, 'context': 'In this work, we mainly consider qj ∈ {1, 2, 3, 4, 5, 6, 7, 8}, M = 8 and ci ∈ [1, 8], because it has been observed that with greater than 8 bits, the neural network’s performance almost has no degradation (Elthakeb et al. (2018)). We also concentrate on such a design in all our experiments. In fact, there is also a new trend that investigates the possible quantization with bit that is not power of 2 (Elthakeb et al. (2018); Park et al. (2018b;a); Wang et al. (2018)). In addition, some hardware such as FPGA also gradually supports efficient quantization using such bits (Wang et al. (2018); Wei et al.', 'startOffset': 207, 'endOffset': 596}, {'referenceID': 4, 'context': 'In this work, we mainly consider qj ∈ {1, 2, 3, 4, 5, 6, 7, 8}, M = 8 and ci ∈ [1, 8], because it has been observed that with greater than 8 bits, the neural network’s performance almost has no degradation (Elthakeb et al. (2018)). We also concentrate on such a design in all our experiments. In fact, there is also a new trend that investigates the possible quantization with bit that is not power of 2 (Elthakeb et al. (2018); Park et al. (2018b;a); Wang et al. (2018)). In addition, some hardware such as FPGA also gradually supports efficient quantization using such bits (Wang et al. (2018); Wei et al. (2019)).', 'startOffset': 207, 'endOffset': 615}, {'referenceID': 10, 'context': 'In all the experiments, we take ResNet18, ResNet20, ResNet56 (He et al. (2015)) or MobileNetV2 (Sandler et al.', 'startOffset': 62, 'endOffset': 79}, {'referenceID': 10, 'context': 'In all the experiments, we take ResNet18, ResNet20, ResNet56 (He et al. (2015)) or MobileNetV2 (Sandler et al. (2018)) as the backbone models.', 'startOffset': 62, 'endOffset': 118}, {'referenceID': 24, 'context': 'For ResNet20,we compare the accuracy and compression ratio of the proposed approach to some related or similar works such as DNAS (Wu et al. (2018)), TTQ (Zhu et al.', 'startOffset': 131, 'endOffset': 148}, {'referenceID': 24, 'context': 'For ResNet20,we compare the accuracy and compression ratio of the proposed approach to some related or similar works such as DNAS (Wu et al. (2018)), TTQ (Zhu et al. (2016)), PACT (Choi et al.', 'startOffset': 131, 'endOffset': 173}, {'referenceID': 2, 'context': '(2016)), PACT (Choi et al. (2018)) and LQE (Zhang et al.', 'startOffset': 15, 'endOffset': 34}, {'referenceID': 2, 'context': '(2016)), PACT (Choi et al. (2018)) and LQE (Zhang et al. (2018)) with Resnet20 on CIFAR-10, and the details of accuracy and compression ratio are shown in Table 2.', 'startOffset': 15, 'endOffset': 64}, {'referenceID': 25, 'context': 'We also apply the proposed approach to ResNet56 and compare its performance with DNAS (Wu et al. (2018)), and the results are recorded in Table 5.', 'startOffset': 87, 'endOffset': 104}, {'referenceID': 7, 'context': 'Here, as in (Han et al. (2015); Wang et al. (2018)), we present two sets of our results, i.', 'startOffset': 13, 'endOffset': 51}], 'year': 2019, 'abstractText': 'Deep neural network (DNN) has rapidly found many applications in different scenarios. However, its large computational cost and memory consumption are barriers to computing restrained applications. DNN model quantization is a widely used method to reduce the DNN storage and computation burden by decreasing the bit width. In this paper, we propose a novel cursor based adaptive quantization method using differentiable architecture search (DAS). The multiple bits’ quantization mechanism is formulated as a DAS process with a continuous cursor that represents the possible quantization bit. The cursor-based DAS adaptively searches for the desired quantization bit for each layer. The DAS process can be solved via an alternative approximate optimization process, which is designed for mixed quantization scheme of a DNN model. We further devise a new loss function in the search process to simultaneously optimize accuracy and parameter size of the model. In the quantization step, based on a new strategy, the closest two integers to the cursor are adopted as the bits to quantize the DNN together to reduce the quantization noise and avoid the local convergence problem. Comprehensive experiments on benchmark datasets show that our cursor based adaptive quantization approach can efficiently obtain lower size model with comparable or even better classification accuracy.', 'creator': 'LaTeX with hyperref'}","deep learning (dl) has achieved great successes in varied fields such as gaming, natural language processing, speech recognition, computer vision and so on. however, its huge computational burden and large memory consumption still intimidate many potential applications, especially for mobile devices and embedded systems. a number of efforts have been devoted to compress the dl model size and accelerate its training and test speed. these efforts can be roughly categorized into four major classes: network pruning (han et al. (2015); anwar et al. (2015); peng et al. (2019); zhuang et al. (2018)), low rank approximation (tai et al. (2015); wang et al. (2018); hayashi et al. (2019)), knowledge distillation (hinton et al. (2015); zagoruyko & komodakis (2016)), and network quantization (courbariaux & bengio (2016); lin et al. (2015); wu et al. (2015); polino et al. (2018); zhang et al. (2018)). among them, network quantization methods, jointly optimizing the whole network weights, activations or gradients with low bit (such as 8 bits or even 1 bit), show great potential in compressing model size and accelerating inference time. in addition, quantization based approaches are preferable for mobile devices and embedded systems since these devices are gradually equipped by specifically designed low bit computing hardware. although existing quantization based approaches, which mainly use one fixed bit to represent the whole dnn model, yields encouraging compression ratio while keeping the model’s performance, we argue that simply using only a fixed bit for quantization is not the optimal choice for the trade-off between a model size and its performance. for example, to run a model on chips with strict memory limitations, 1 bit or 2 bits’ quantization suffers from severe accuracy loss (rastegari et al. (2016)) while 16 bits’ or 8 bits’ quantization cannot significantly reduce the model size. to address the above problem, we propose a cursor based adaptive quantization method to derive a different number of bits in different layers for dnn model compression, i.e., we search for the best configuration of different bit quantization for different layers in a neural network model. distinctive from most algorithms aforementioned, our approach is motivated by recent neural architecture search (nas) that aims to find better performance neural architecture with less calculations or less size automatically. the key in our algorithm is using a continuous cursor that represents the bit quantization scheme for each layer. for different layers, many cursors will be adaptively searched during the nas process. since the cursor itself is continuous and the whole search procedure can be considered as a differentiable architecture search (das) process, which can be effectively solved based on an alternative optimization strategy. a novel cost function that considers the model compression and prediction accuracy is also proposed in the das process. in the searching process of the cursors, a quantization process is applied to compress the model size at the same time. to reduce the possible quantization noise and local convergence problem, we make use of the closest two integer bits to the cursor to quantize the weights for each layer in a dnn model. we validate our proposed method with image classification tasks on cifar10,cifar100 and imagenet. comprehensive experiments on some backbone dnn models such as resnet20, resnet18, resnet56 and mobilenetv2 show that the proposed cursor based quantization method achieves remarkably better performance of compression ratio with ignorable accuracy drop or even better accuracy. in summary, the contributions of this work are four-fold: • we cast the adaptive quantization of neural network as a problem of neural architecture search. a continuous cursor is proposed to represent the possible quantization bit, leading to a more efficient search space. • a novel regularization function is proposed to optimize model compression in the search process. thus the search for the cursor position and weights can be efficiently solved in an alternative optimization manner. • two nearest neighbor integers to the cursor are adopted with a carefully designed strategy to implement the quantization of the network to reduce the quantization noise and avoid possible local convergence. • we comprehensively evaluate the proposed adaptive quantization method on some benchmark datasets and achieve new state-of-the-art performance for different number of bits quantization of neural network.","deep learning (dl) has achieved great successes in varied fields such as gaming, natural language processing, speech recognition, computer vision and so on among them, network quantization methods, jointly optimizing the whole network weights, activations or gradients with low bit (such as 8 bits or even 1 bit), show great potential in compressing model size and accelerating inference time although existing quantization based approaches, which mainly use one fixed bit to represent the whole dnn model, yields encouraging compression ratio while keeping the model’s performance, we argue that simply using only a fixed bit for quantization is not the optimal choice for the trade-off between a model size and its performance for example, to run a model on chips with strict memory limitations, 1 bit or 2 bits’ quantization suffers from severe accuracy loss (rastegari et al to reduce the possible quantization noise and local convergence problem, we make use of the closest two integer bits to the cursor to quantize the weights for each layer in a dnn model comprehensive experiments on some backbone dnn models such as resnet20, resnet18, resnet56 and mobilenetv2 show that the proposed cursor based quantization method achieves remarkably better performance of compression ratio with ignorable accuracy drop or even better accuracy • two nearest neighbor integers to the cursor are adopted with a carefully designed strategy to implement the quantization of the network to reduce the quantization noise and avoid possible local convergence • we comprehensively evaluate the proposed adaptive quantization method on some benchmark datasets and achieve new state-of-the-art performance for different number of bits quantization of neural network",0.8909289240837097,0.8528421521186829,0.8714695572853088
ICLR_2020_1571,The Generalization-Stability Tradeoff in Neural Network Pruning,"{'source': 'CRF', 'title': 'THE GENERALIZATION-STABILITY TRADEOFF IN NEURAL NETWORK PRUNING', 'authors': [], 'emails': [], 'sections': [{'heading': '1 INTRODUCTION', 'text': 'Pruning weights and/or convolutional filters from deep neural networks (DNNs) can substantially shrink parameter counts with minimal loss in accuracy (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015a; Li et al., 2016; Molchanov et al., 2017; Louizos et al., 2017; Liu et al., 2017; Ye et al., 2018), enabling broader application of DNNs via reductions in memory-footprint and inference-FLOPs requirements. Moreover, many pruning methods have been found to actually improve generalization (measured by model accuracy on previously unobserved inputs) (Narang et al., 2017; Frankle & Carbin, 2018; You et al., 2019). Consistent with this, pruning was originally motivated as a means to prevent over-parameterized networks from overfitting to comparatively small datasets (LeCun et al., 1990).\nConcern about over-parameterizing models has weakened, however, as many recent studies have found that adding parameters can actually reduce a DNN’s generalization-gap (the drop in performance when moving from previously seen to previously unseen inputs), even though it has been shown that the same networks have enough parameters to fit large datasets of randomized data (Neyshabur et al., 2014; Zhang et al., 2016). Potential explanations for this unintuitive phenomenon have come via experiments (Keskar et al., 2016; Morcos et al., 2018; Yao et al., 2018; Belkin et al., 2018; Nagarajan & Kolter, 2019), and the derivation of bounds on DNN generalization-gaps that suggest less overfitting might occur as parameter counts increase (Neyshabur et al., 2018). This research has implications for neural network pruning, where a puzzling question has arisen: if larger parameter counts don’t increase overfitting, how does pruning parameters throughout training improve generalization?\nTo address this question we first introduce the notion of pruning instability, which we define to be the size of the drop in network accuracy caused by a pruning iteration (Section 3). We then empirically analyze the instability and generalization associated with various magnitude-pruning (Han et al., 2015b) algorithms in different settings, making the following contributions:\n1. We find a tradeoff between the stability and potential generalization benefits of pruning, and show iterative pruning’s similarity to regularizing with noise—suggesting a mechanism unrelated to parameter counts through which pruning appears to affect generalization.\n2. We characterize the properties of pruning algorithms which lead to instability and correspondingly higher generalization.\n3. We derive a batch-normalized-parameter pruning algorithm to better control pruning stability.'}, {'heading': '2 RELATED WORK', 'text': 'There are various approaches to pruning neural networks. Pruning may be performed post-hoc (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015b; Liu et al., 2017), or iteratively throughout training, such that there are multiple pruning events as the model trains (Hochreiter & Schmidhuber, 1997; Narang et al., 2017; Zhu & Gupta, 2017). Most methods prune parameters that appear unimportant to the function computed by the neural network, though means of identifying importance vary. Magnitude pruning (Han et al., 2015b) uses small-magnitude to indicate unimportance and has been shown to perform competitively with more sophisticated approaches (Gale et al., 2019).\nMany pruning studies have shown that the pruned model has heightened generalization (Narang et al., 2017; Frankle & Carbin, 2018; You et al., 2019), consistent with the fact that pruning may be framed as a regularization (rather than compression) approach. For example, variational Bayesian approaches to pruning via sparsity-inducing priors (Molchanov et al., 2017; Louizos et al., 2017) can describe weight removal as a process that reduces model description length, which in theory may help improve generalization (Rissanen, 1978). Similarly, the idea that models may be described more succinctly at flat minima has motivated pruning in service of flat minimum search (Hochreiter & Schmidhuber, 1997). Though Dinh et al. (2017) notes, however, that flatness can be arbitrarily modified by reparameterizing the function, and sharp minima can generalize well.\nVC dimension (a measure of model capacity) has motivated the use of iterative pruning to improve generalization (LeCun et al., 1990; Hassibi & Stork, 1993). Overfitting can be bounded above by an increasing function of VC dimension, which itself often increases with parameter counts, so fewer parameters can lead to a guarantee of less overfitting (Shalev-Shwartz & Ben-David, 2014). Unfortunately, such bounds can be so loose in practice that tightening them by reducing parameter counts need not translate to better generalization (Dziugaite & Roy, 2017).\nRather than support parameter-count-based arguments for generalization in DNNs, our results suggest iterative DNN pruning may improve generalization by creating various noisy versions of the internal representation of the data, which unpruned parameters try to fit to, as in noise-injection regularization (Srivastava et al., 2014; Poole et al., 2014). Dropout creates particularly similar noise, as it temporarily sets random subsets of layer outputs to zero (likely changing an input’s internal representation every epoch). Indeed, applying dropout-like zeroing noise to a subset of features during training can encourage robustness to a post-hoc pruning of that subset (Leclerc et al., 2018; Gomez et al., 2018). Iterative DNN pruning noise ultimately differs, however, as it is: applied less frequently, not temporary (except in algorithms with weight re-entry), usually not random, and less well studied.'}, {'heading': '3 APPROACH', 'text': 'Given a neural network and set of test data, let t be the top-1 test accuracy, the fraction of test data examples correctly classified multiplied by 100. We define a pruning algorithm’s instability on pruning iteration i in terms of t measured immediately before (tpre,i) and immediately after (tpost,i) pruning: instabilityi = tpre,i − tpost,i. In other words, the instability is the size of the accuracy drop caused by a particular pruning event.\nPruning Iterationsi− 1 i i+ 1\nTrain Test(tpre,i−1) Prune Test(tpost,i−1)\nTrain Test(tpre,i) Prune Test(tpost,i)\nThis measure is related to a weight’s importance (sometimes referred to as “saliency”; LeCun et al. (1990); Hassibi & Stork (1993)) to the test accuracy, in that less stable pruning algorithms target more important sets of weights (all else equal). The stability of a pruning algorithm may be affected by many factors. Our experiments (Section 4) explore the effects of the following: pruning target, pruning schedule, iterative pruning rate, and model. The remainder of this section provides an overview of these factors and demonstrates a need for a novel pruning target, which we derive.'}, {'heading': '3.1 PRUNING TARGET', 'text': 'In all of our experiments, we use iterative magnitude pruning (Han et al., 2015b), which removes weights according to some magnitude-based rule, retrains the resulting smaller network to recover from the pruning, and repeats until the desired size reduction is met. We denote pruning algorithms that target the smallest-magnitude parameters with an ""S"" subscript (e.g. pruneS), random parameters with an ""R"" subscript, and the largest-magnitude parameters with an ""L"" subscript. The usual approach to pruning involves removing parameters that have the smallest magnitudes (Li et al., 2016; Gale et al., 2019), or, similarly, those parameters least important to the loss function as determined by some other metric (LeCun et al., 1990; Hassibi & Stork, 1993; Molchanov et al., 2016; 2017; Louizos et al., 2017; Ye et al., 2018; Yu et al., 2018; You et al., 2019).'}, {'heading': '3.1.1 IDENTIFYING IMPORTANT BATCH-NORMALIZED PARAMETERS', 'text': 'The correlation between parameter magnitude and importance weakens in the presence of batch normalization (BN) (Ioffe & Szegedy, 2015). Without batch normalization, a convolutional filter with weights W will produce feature map activations with half the magnitude of a filter with weights 2W : filter magnitude clearly scales the output. With batch normalization, however, the feature maps are normalized to have zero mean and unit variance, and their ultimate magnitudes depend on the BN affine-transformation parameters γ and β. As a result, in batch normalized networks, filter magnitude does not scale the output, and equating small magnitude and unimportance may therefore be particularly flawed. This has motivated approaches to use the scale parameter γ’s magnitude to find the convolutional filters that are important to the network’s output (Ye et al., 2018; You et al., 2019). Here, we derive a novel approach to determining filter importance/magnitude that incorporates both γ and β.\nTo approximate the expected value/magnitude of a batch-normalized, post-ReLU feature map activation, we start by defining the 2D feature map produced by convolution with BN:\nM = γBN(W ∗ x) + β.\nWe approximate the activations within this feature map as Mij ∼ N (β, γ). This approximation is justified if central limit theorem assumptions are met by the dot products inW ∗x, and we empirically show in Figure A.1 that this approximation is highly accurate early in training, though it becomes less accurate as training progresses. Given this approximation, the post-ReLU feature map\nR = max{0,M}\nhas elements Rij that are either 0 or samples from a truncated normal distribution with left truncation point l = 0, right truncation point r =∞, and mean µ where\nµ = γ φ(λ)− φ(ρ)\nZ + β,\nλ = l − β γ , ρ = r − β γ , Z = Φ(ρ)− Φ(λ),\nand φ(x) and Φ(x) are the standard normal distribution’s PDF and CDF (respectively) evaluated at x. Thus, an approximation to the expected value of Rij is given by\nE[Rij ] ≈ Φ(λ)0 + (1− Φ(λ))µ.\nWe use the phrase ""E[BN] pruning"" to denote magnitude pruning that computes filter magnitude using this derived estimate of E[Rij ]. E[BN] pruning has two advantages. First, this approach avoids the problematic assumption that filter importance is tied to filter `2 norm in a batch-normalized network. Accordingly, we hypothesize that E[BN] pruning can grant better control of the stability of the neural network’s output than pruning based on filters’ `2 norms. Second, the complexity of the calculation is negligible as it requires (per filter) just a handful of arithmetic operations on scalars, and two PDF and CDF evaluations, which makes it cheaper than a data-driven approach (e.g. approximating the expected value via the sample mean of feature map activations for a batch of feature maps).'}, {'heading': '3.2 SUMMARY OF MODELS', 'text': 'We consider three basic model classes: a simple network with convolutions (2x32, pool, 2x64, pool) and fully connected layers (512, 10) that we denote Conv4, VGG11 (Simonyan & Zisserman, 2014) with its fully-connected layers replaced by a single fully-connected layer, and ResNet18 (He et al., 2016). All convolutions are 3x3. We trained these models using Adam (Kingma & Ba, 2014) with initial learning rate lr = 0.001, as we found Adam more helpful than SGD for recovering from unstable pruning (seemingly consistent with the observation in Zhu & Gupta (2017) that recovery from pruning is more difficult when learning rates are low).'}, {'heading': '3.3 ITERATIVE PRUNING RATE AND SCHEDULE', 'text': 'For Conv4, we apply pruning to its first linear layer (which contains 94% of Conv4’s 1,250,858 parameters). For VGG/ResNet, pruning targets the final four convolutional layers (which contain 90% of VGG11’s 9,231,114 parameters, and 74% of ResNet18’s 11,173,962 parameters). Pruning focused on later layers partly because, as also found in Li et al. (2016); You et al. (2019), it allowed the network to recover more easily.\nThe pruning algorithms we consider are iterative: we define a pruning schedule that describes the epochs on which pruning events occur, and set a corresponding (constant) iterative pruning rate that will ensure the total pruning percentage is met by the end of training (please see Appendix A.5 for rate and schedule details). Thus, throughout training, pruning steadily removes DNN parameters, with the iterative pruning rate determining the number pruned per event. While our plots label each pruning configuration with its iterative pruning rate, the total pruning percentages were: 42% of VGG11, 46% of ResNet18, and 10% of Conv4 (except in Appendix A.4, wherein we prune 85% of Conv4).'}, {'heading': '4 EXPERIMENTS', 'text': 'Pruning studies often aim to compress pre-trained models that generalize well, and consequently, much work has focused on metrics to identify parameter importance: if you can find the parameters that matter the least to the function computed by the DNN, then you can prune more parameters without significantly harming accuracy. As a bonus, such pruning methods can sometimes even increase generalization (Narang et al., 2017; Frankle & Carbin, 2018; You et al., 2019). However, the mechanism by which pruning induces higher generalization remains unclear. Here, rather than investigate how to best maintain accuracy when pruning the network, we instead focus on understanding the mechanisms underlying these generalization improvements.'}, {'heading': '4.1 THE GENERALIZATION-STABILITY TRADEOFF', 'text': 'Can improved generalization in pruned DNNs be explained by parameter-count reduction alone, or rather, do the properties of the pruning algorithm play an important role in generalization? As removing parameters from a DNN via pruning may make the DNN less capable of fitting to the noise in the training data, as originally suggested in LeCun et al. (1990); Hassibi & Stork (1993), we might expect that the generalization improvements observed in pruned DNNs are entirely explained by the number of parameters removed. In which case, methods that prune equal amounts of parameters would generalize similarly.\nAlternatively, perhaps some aspect of the pruning algorithm itself is responsible for increased generalization. This seems plausible as the reported generalization benefits of pruning vary widely across studies. One possible explanation for this variability is differences in the pruning algorithms themselves. A key differentiator of these algorithms is their stability: more stable approaches may compute a very close approximation to the way the loss changes with respect to each parameter and prune a single parameter at a time (Hassibi & Stork, 1993), while less stable approaches may assume that parameter magnitude and importance are roughly similar and prune many weights all at once (Han et al., 2015b). Therefore, to the extent that differences in pruning algorithms explain differences in pruning-based generalization improvements, we might expect to observe a relationship between generalization and pruning stability.\nTo determine whether pruning algorithm stability affects generalization, we compared the instability and final top-1 test accuracy of several pruning algorithms with varying pruning targets and iterative pruning rates (Figure 1). Consistent with the nature of the pruning algorithm playing a role in generalization, we observed that more unstable pruning algorithms created higher final test accuracies than those which were stable (Figure 1, right; VGG11: Pearson’s correlation r = .84, p-value = 1.6e−11; ResNet18: r = .65, p-value = 5e−5). While many pruning approaches have aimed to induce as little instability as possible, these results suggest that pruning techniques may actually facilitate better generalization when they induce more instability. Furthermore, these results suggest that parameter-count based arguments may not be sufficient to explain generalization in pruned DNNs, and suggest that the precise pruning method plays a critical role in this process.\nFigure 1 also demonstrates that pruning events for pruneL with a high iterative pruning rate (red curve, pruning as much as 13% of a given convolutional layer per pruning iteration) are substantially more destabilizing than other pruning events, but despite the dramatic pruning-induced drops in performance, the network recovers to higher performance within a few epochs. Several of these pruning events are highlighted with red arrows. Please see Appendix A.2 for visualization of the epoch-wise instabilities of each method in VGG11, and Appendix A.3 for an `2-norm pruning version of Figure 1, which has qualitatively similar results.\nInterestingly, we initially observed that ResNet18 adapted to pruning events more quickly than VGG11 (accuracy rebounded after pruning then flattened soon after instead of climbing steadily). Thinking that shortcut connections were allowing the network to adapt to pruning events too easily, we tried pruning a larger amount of the penultimate block’s output layer: this reduced the number of shortcut connections to the final block’s output layer, lengthened the adaptation period, and improved generalization. This simple improvement of pruning hyperparameters suggests a potential for further optimization of the results shown. Please see Appendix A.5.1 for all hyperparameters/details of these experiments.'}, {'heading': '4.2 THE ROLE OF WEIGHT MAGNITUDE IN PRUNING REGULARIZATION', 'text': 'We have demonstrated that, perhaps surprisingly, pruning larger magnitude weights via the E[BN] algorithm can result in larger test accuracy improvements (Figure 1). This suggests a positive correlation between pruning target magnitude and pruning’s regularization effect. However, it’s not clear whether this relationship holds more generally; i.e., perhaps it was caused by a feature of our\nE[BN] algorithm or the networks examined. Alternatively, this effect may be dependent on whether nodes/filters (structured pruning) or individual parameters (unstructured pruning) are pruned.\nAs such, we tested whether target weight magnitude correlates with pruning’s regularizing effect when using both unstructured and structured magnitude pruning on the penultimate linear layer of a small network without batch normalization (Conv4). Specifically, we constructed a pruning target for each weight-magnitude decile (see Appendix A.5.2 for details), used each target to prune ten separate networks as they trained, and compared the generalization gaps (test-train accuracy) of the pruned networks to the target pruned (Figure 2).\nFor both unstructured and structured pruning (Figure 2 left and right, respectively), we found that pruning larger weights led to better generalization gaps, though, interestingly, this effect was much more dramatic in the context of unstructured pruning than structured pruning. One possible explanation for this is that, in structured pruning, the `2 norm of pruned neurons did not vary dramatically past the fifth decile, whereas the unstructured deciles were approximately distributed exponentially. As a result, the top 50% of filters for the structured case were not clearly distinguished, making magnitude pruning much more susceptible to small sources of noise. These results suggest that, when weight magnitudes vary considerably, pruning large magnitude weights may lead to improved generalization.\nInterestingly, for ResNet18, we actually found that structured pruneL (red line) performed better than unstructured pruneL (green line) (Figure 3). The worse performance of unstructured pruneL may stem from its harming the helpful inductive bias provided by convolutional filters (i.e., perhaps removing the most important connections in all convolutional filters degrades performance more than pruning the same number of connections via removal of several entire filters) or its lower instability.'}, {'heading': '4.3 THE ROLE OF ITERATIVE PRUNING RATE IN PRUNING INSTABILITY', 'text': 'While pruning large magnitude weights appears to play a role in pruning’s ability to improve generalization, more commonly used pruning algorithms often see generalization improvements when targeting the smallest magnitude or least important parameters, suggesting that target magnitude/importance is not the only characteristic of pruning algorithms relevant to generalization. One possibility is that, given a pruning target, pruning more parameters per pruning iteration (while holding constant the total pruning percentage) may lead to greater instability. If this is the case, the generalization-stability tradeoff suggests that the increase in instability from raising the iterative pruning rate would coincide with improved generalization performance. Alternatively, if the pruning target or total pruning percentage is all that matters, we may expect that changing the iterative\npruning rate (while keeping the pruning target and total pruning percentage fixed) would not affect generalization.\nTo test this, we plotted mean instability and test accuracy as a function of different iterative pruning rates for both `2-norm and E[BN] pruning (Figure 4). Consistent with iterative pruning rate playing a role in instability, we find that (given a pruning target) more instability is induced by using larger iterative pruning rates (Figure 4 left). Moreover, pruning random or small magnitude parameters performs best at the largest iterative rate (30%), supporting the idea that these methods require a source of instability to boost generalization. Note this suggests that, when targeting less important weights, higher iterative pruning rates during training can be an effective way to induce additional instability and generalization. (Algorithm and experiment details are available in Appendix A.5.4.)\nPerhaps strangely, higher iterative pruning rates did not translate to improved generalization when targeting the largest magnitude weights (pruneL) with `2-norm pruning. The fact that pruneL does not generalize the best at the highest iterative pruning rate may be due to the reduction in pruning iterations required by the large iterative pruning rate (i.e., when the iterative rate is at 30%, the number of pruning events is capped at three, which removes 90% of a layer). Thus, while this rate grants more instability (Figure 4 left) per iteration, pruning affects the network less often. The idea that the regularizing effect of pruning is enhanced by pruning more often may also help explain the observation that methods that prune iteratively can generalize better (Han et al., 2015b).\nAnother possibility is that, since raising the iterative pruning rate (and consequently the duration between pruning events) tends to make the `2-norm worse for differentiating parameters by their importance to accuracy1, raising the iterative pruning rate causes pruneL with `2-norm pruning to target less important weights. Consequently, pruneL with `2-norm pruning may generalize worse at higher iterative rates by leaving unpruned more important weights, the presence of which can harm model generalization (Hinton & Van Camp, 1993; Morcos et al., 2018). Relatedly, this also means that pruneS with `2-norm pruning may increase (in networks with batch normalization at least) instability and generalization by failing to avoid the pruning of important parameters.'}, {'heading': '4.4 ITERATIVE PRUNING AS NOISE INJECTION', 'text': 'Our results thus far suggest that pruning improves generalization when it creates instability throughout training. These prior results, though, involved damaging model capacity simply by the nature of pruning, which decreases the number of model parameters. It therefore remains possible that the generalization benefits we’ve seen rely upon the reduction in capacity conferred by pruning. Here, we examine this critical question.\nWe first note that iterative pruning can be viewed as noise injection (Srivastava et al., 2014; Poole et al., 2014), with the peculiarity that the noise permanently zeroes a subset of weights. Removing\n1In Figure 4 left: with E[BN] pruning, pruneL tends to become more unstable relative to pruneS as the iterative rate increases, but with `2-norm pruning, pruneL and pruneS create similar instabilities for all rates.\nthe permanence of this zeroing can mitigate some of the capacity effect of pruning2 and, therefore, help us isolate and study how iterative pruning regularizes through noise injection.\nAs a baseline, we consider pruneL applied to VGG11, judging filter magnitude via the `2-norm (additional experimental details are in Appendix A.5.5). We then modify this algorithm such that, rather than permanently prune filters, it simply multiplies the filter weights by zero, then allows the zeroed weights to immediately resume training in the network (""Zeroing 0"" in Figure 5 Left). However, by allowing pruned weights to immediately recover, this experiment also removes a key, potentially regularizing aspect of pruning noise: the requirement that the rest of the network adapts to fit the new representations generated by pruning. To encourage this potentially important facet of pruning noise, we also added variants that held weights to zero for 50 and 1500 consecutive batches3. As a related experiment, we also measured the impact of adding Gaussian noise to the weights in Figure 5, right. Noise was applied either once (Gaussian 0) or repeatedly over a series of training batches (Gaussian 50/1500).\nIf the capacity effects of weight removal are not necessary to explain pruning’s effect on generalization, then we would expect that the generalization behavior of these non-permanent noise injection algorithms could mimic the generalization behavior of pruneL. Alternatively, if weight removal is a necessary component of pruning-based generalization improvements, then we would not expect close similarities between the generalization phenomena of pruneL and non-permanent pruning noise injection.\nConsistent with the capacity effects of weight removal not being necessary to explain generalization in pruned DNNs, applying zeroing noise for 50 batches to filters (rather than pruning them completely) generates strikingly similar accuracy to pruneL (Figure 5 Left). Specifically, the patterns in instability are qualitatively and quantitatively similar, as are the generalization levels throughout training.\nImportantly, we found that applying zeroing noise once (Zeroing 0; brown line) was not sufficient to generate better performance, suggesting that the regularization induced by forcing weights to adapt to noised representations is critical to pruning’s ability to improve generalization. Moreover, we found that, while applying Gaussian noise could increase generalization if applied for long enough (Gaussian 1500; purple line), it still did not match the performance of pruneL, suggesting that multiplicative zeroing noise is substantially more effective than additive Gaussian noise4. Together, these results demonstrate that pruning induced generalization benefits are not merely explained by weight removal, but rather are dependent on the regularization conferred by forcing networks to adapt to noised representations over a sufficiently long period throughout training.\n2This approach still effectively removes any weights that do not learn after reentering the model. However, we observed (see Figure 5) that pruning the reentered weights at convergence resulted in a marked drop in performance (for all noise schemes except “Zeroing 1500”), showing that the reentered weights had typically learned after reentry, and that temporary zeroing is therefore less harmful to capacity than permanent pruning.\n3The pruning noise we apply affects a particular weight for no more than one segment of training time (where a segment is a series of N consecutive batches)\n4At the scale we used.'}, {'heading': '5 DISCUSSION', 'text': 'In this study, we defined the notion of pruning algorithm instability, and applied several pruning approaches5 to multiple neural networks, assessing the approaches’ effects on instability and generalization. Throughout these experiments, we observed that pruning algorithms that generated more instability led to better generalization (as measured by test accuracy). For a given pruning target and total pruning percentage, instability and generalization could be fueled by raising iterative pruning rates (Figure 4, Section 4.3). Additionally, targeting more important weights, again holding total parameters pruned constant, led to more instability and generalization than targeting less important weights (Figure 1, Section 4.1).\nThese results support the idea that the generalization benefits of pruning cannot be explained solely by pruning’s effect on parameter counts—the properties of the pruning algorithm must be taken into account. Our analysis also suggests that the capacity effects of weight-removal may not even be necessary to explain how pruning improves generalization. Indeed, we provide an interpretation of iterative pruning as noise injection, a popular approach to regularizing DNNs, and find that making pruning noise impermanent provides pruning-like generalization benefits while not removing as much capacity as permanent pruning (Figure 5, Section 4.4).'}, {'heading': '5.1 CAVEATS AND FUTURE WORK', 'text': 'While not emphasized in our discussion, pruning algorithm stability can be a desirable property, as recovery from pruning damage is not guaranteed. Indeed, pruning too many large/important weights can lead to worse final generalization (Li et al., 2016). Recovery appears to be a function of several factors, including: learning rate (Zhu & Gupta, 2017)); presence of an ongoing regularization effect (Figure 3, Section 4.2); preservation of helpful inductive biases (Figure 3, Section 4.2); and damage to network capacity (e.g., removing too much of an important layer could cause underfitting).\nA better understanding of the factors which aid recovery from pruning instability could aid the design of novel pruning algorithms. For example, pruning methods that allow weights to re-enter the network (Narang et al., 2017) could perhaps prune important weights occasionally to enhance generalization improvements, without risking permanent damage to the pruned networks (see Appendix A.4).\nIn describing how pruning regularizes a model, we touched on similarities between pruning and noise injection. Our results, however, may also be consistent with other parameter-count-independent approaches to understanding generalization in neural networks, as pruning may reduce the information stored in the network’s weights (Hinton & Van Camp, 1993), and make the network more distributed (Morcos et al., 2018; Dettmers & Zettlemoyer, 2019). This raises the possibility that pruning noise engenders helpful properties in DNNs, though it remains unclear whether such properties might be identical to those achieved with more common noise injection schemes (Srivastava et al., 2014; Poole et al., 2014). Further exploration will be necessary to better understand the relationship between these approaches.\nOne important caveat of our results is that they were generated with CIFAR-10, a relatively small dataset, so future work will be required to evaluate whether the presented phenomena hold in larger datasets. Relatedly, we only studied pruning’s regularizing effect in isolation and did not include commonly used regularizers (e.g., weight decay) in our setups. In future work, it would be interesting to examine whether pruning complements the generalization improvements of other commonly used regularization techniques.'}, {'heading': 'A APPENDIX', 'text': 'A.1 QUALITY OF NORMALITY APPROXIMATION BY LAYER AND TRAINING TIME\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6 0.8 PD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\n5.0 2.5 0.0 2.5 5.0 Activation Magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\nPD F\nVG G1\n9 La\nye r D\nep th\nUntrained VGG19 Trained VGG19\nFigure A1: We examined the normalized activations (shown in blue histograms) of feature maps in the final eight convolutional layers of VGG19 before (left) and after (right) training to convergence. We found that the approximation to standard normality (shown in orange) of these activations is reasonable early on but degrades with training (particularly in layers near the output).\nThe main drawback to the E[BN] approach (Section 3.1.1) is the sometimes poor approximation Mij ∼ N(β, γ). In Figure A.1, the approximation’s quality depends on layer and training epoch. A less serious drawback is that this approach does not account for the strength of connections to the post-BN feature map, which could have activations with a large expected value but low importance if relatively small-magnitude weights connected the map to the following layer.\nA.2 EPOCH-WISE INSTABILITIES\n50 100 150 200 250 Epoch\n10 1\n10 2 0\n10 2\n10 1\n100 101 M ea n In st ab ilit\ny Pruning Style Prune_S 1% Prune_S E[BN] 1% Prune_S E[BN] 13% Prune_L E[BN] 13%\nFigure A2: In VGG11, pruneS E[BN] is more stable than pruneS, which uses filter-`2-norm to compare parameter magnitudes. Methods with higher iterative pruning rates create more instability on a given iteration. Means reduce along the run dimension (10 runs per configuration).\nNote that this graph uses a method (pruneS) that was included in Figure 1 right, but was not displayed in Figure 1 left due to its similarity to pruneS E[BN]. Additional experimental details are in Section A.5.1.\nA.3 `2-NORM-PRUNING VERSION OF FIGURE 1\n50 100 150 200 250 300 Epoch\n83.0 83.0\n83.5 83.5\n84.0 84.0\n84.5 84.5\n85.0 85.0\n85.5 85.5\n86.0 86.0 86.5 86.5 To p1 Ac cu ra cy % 5% Pruning Style No Pruning Prune_S 1% Prune_S 13% Prune_L 13%\n0.0 0.2 0.4 0.6 0.8 1.0 Mean Instability\n85.4\n85.6\n85.8\n86.0\n86.2\n86.4\nTo p-\n1 Ac\ncu ra\ncy %\npearsonr = 0.67; p = 0.00023\n50 100 150 200 250 300 Epoch\n84 84\n85 85\n86 86\n87 87\n88 88\nTo p-\n1 Ac\ncu ra\ncy %\n28% 41%\nPruning Style No Pruning Prune_S 1% Prune_S 13% Prune_L 13%\n0 1 2 3 4 Mean Instability\n86.5\n87.0\n87.5\n88.0\n88.5\nTo p-\n1 Ac\ncu ra\ncy %\npearsonr = 0.43; p = 0.017\nFigure A3: Pruning instability improves generalization of (Top) VGG11 and (Bottom) ResNet18 when training on CIFAR-10 (10 runs per configuration). (Left) Test accuracy during training of several models illustrates how adaptation to more unstable pruning leads to better generalization. (Right) Means reduce along the epoch dimension (creating one point per run-configuration combination).\nHere we use the same training and pruning configurations that were used for Figure 1, but we replace E[BN] pruning with `2-norm pruning. Qualitatively, the two figures’ results are similar. Interestingly, though, the correlation between instability and generalization is somewhat weaker with `2-norm pruning. This may be explained by the fact that `2-norm pruning generates a narrower spectrum of instabilities, which is perhaps due to `2-norm scoring’s inability to accurately assess parameter importance (illustrated in Figure 4).\nA.4 THE GENERALIZATION-STABILITY TRADEOFF AT 85% SPARSITY IN CONV4\n0 5 10 15 20 25 Epoch\n20\n40\n60\n80\n100\nTo p-\n1 Ac\ncu ra\ncy %\nPruning Style No Pruning Prune_S Prune_L Train/Test Train Test\nFigure A4: When training Conv4 on CIFAR10, unstable pruning can significantly improve the baseline’s generalization. The training accuracies and test accuracies (the latter were calculated immediately after pruning) illustrate how much each pruning algorithm disturbs the neural network’s output during training.\nWhile it’s unclear how much unstable pruning, which is particularly damaging to capacity, can be sustained at high sparsity levels, pruneL can lead to generalization several percentage points above the baseline/pruneS when pruning 85% of Conv4. Please see Section A.5.6 for experimental setup details.\nA.5 EXPERIMENTAL DETAILS\nUnstructured magnitude pruning entails removing individual weights (subsets of filters/neurons), which are selected for pruning based on their magnitude. Our unstructured pruning approach does not allow previously pruned weights to reenter the network (Narang et al., 2017; Zhu & Gupta, 2017; Gale et al., 2019). Structured magnitude pruning removes entire filters/neurons, which are selected based on their `2-norms or via the E[BN] calculation. Except where noted, we use structured pruning for VGG11 and ResNet18.\nWe denote the pruning of n layers of a network by specifying a series of epochs at which pruning starts s = (s1, ..., sn), a series of epochs at which pruning ends e = (e1, ..., en), a series of fractions of parameters to remove p = (p1, ..., pn), and an inter-pruning-iteration retrain period r ∈ N. For a given layer l, the retrain period r and fraction pl jointly determine the iterative pruning percentage il. Our experiments prune the same number of parameters il × size(layerl) per pruning iteration, ultimately removing pl × 100% of the parameters by the end of epoch el. Our approach is designed to study the effects of changing factors such as the iterative pruning rate and lacks some practically helpful features, e.g. hyperparameters indicating how many parameters can be safely pruned (Liu et al., 2017; Molchanov et al., 2017). When layerwise iterative pruning percentages differ (i.e., when there exists an a and b such that ia and ib are unequal), our figures state the largest iterative pruning rate that was used in any of the layers.\nFor ResNet, our pruning algorithms did not account for the magnitude of incoming shortcut connections when judging filter magnitude/importance. Though we did prune the incoming and outgoing shortcut connections associated with any pruned feature maps.\nWe used only the CIFAR-10 dataset (Krizhevsky & Hinton, 2009) in our experiments, a limitation of our study. We used batch size 128, and only used data augmentation in the decile experiment (Figure 2). For some experiments, we give multi-step learning rate schedules lrs = (x, y), which means we shrink the learning rate by a factor of 10 at epochs x and y.\nA.5.1 FIGURE 1\nWe used E[BN] pruning in all models that were pruned, except for one model that used `2-norm magnitude pruning, which was included in Figure 1 right but not displayed in Figure 1 left due to its qualitative similarity to pruneS E[BN]. We leave out ""E[BN]"" in the legend of Figure 1 left, but all models nonetheless used E[BN] pruning.\nThe models were trained on CIFAR-10 with Adam for 325 epochs with lrs = (150, 300). The error bars are 95% confidence intervals for the mean, bootstrapped from 10 distinct runs of each experiment.\nSince the layerwise pruning percentages varied, pruning required multiple iterative pruning percentages, the largest of which is denoted in the legend (rounded to the nearest integer).\nVGG11 Pruning targeted the final four convolutional layers during training with (layerwise) starting epochs s = (3, 4, 5, 6), ending epochs e = (150, 150, 150, 275), and pruning fractions p = (0.3, 0.3, 0.3, 0.9). To allow for the same amount of pruning among models with differing iterative pruning percentages, we adjusted the number of inter-pruning retraining epochs. The models with the maximum iterative pruning percentage of 1% had r = 4, while the models with the maximum iterative pruning percentage of 13% had r = 40. The model pruned with `2-norm magnitude pruning, which only appeared in Figure 1 right, had r = 4 as well.\nResNet18 Pruning targeted the final four convolutional layers of ResNet18 during training with (layerwise) starting epochs s = (3, 4, 5, 6), ending epochs e = (150, 150, 170, 275), and pruning fractions p = (0.25, 0.4, 0.25, 0.95). As noted in Section 4.1, we increased the pruning rate of the output layer of the penultimate block to remove shortcut connections to the last layer, thinking that it should increase the duration of adaptation to pruning. The models with the maximum iterative pruning percentage of 1% had r = 4, while the models with the maximum iterative pruning percentage of 13% had r = 40.\nA.5.2 FIGURE 2\nEach experiment in Figure 2 targeted one of ten weight-magnitude deciles in the post-convolutional linear layer of the Conv4 network during training on CIFAR-10 with data augmentation.\nWhile there are just ten deciles, the iterative nature of our pruning algorithms required the creation of eleven different pruning targets: ten methods pruned from the bottom of the decile upward (one experiment for each decile’s starting point: 0th percentile, 10th percentile, etc.), and one (D10) pruned from the last decile’s ending point downward (pruning the very largest collection of weights each iteration). In other words, D9 and D10 targeted the same decile (90th percentile to maximum value), but only D10 actually removed the largest weights on a given iteration (weights in the 100th-99th percentiles, for example). The D9 experiment would target weights starting from the 90th percentile (e.g. it may prune the 90th-91st percentiles on a particular iteration).\nThe training/pruning setup used the Adam optimizer, s = (4), e = (52), p = (0.1), r = 3, and lrs = (30, 60). We calculated the generalization gap on epoch 54 and sampled average pruned magnitudes on epoch 35. We obtained qualitatively similar results regardless of whether we used fewer training epochs or data augmentation. The error bars are 95% confidence intervals for the means, bootstrapped from 10 distinct runs of each configuration.\nA.5.3 FIGURE 3\nIn Figure 3, pruneL was applied to the final four convolutional layers of ResNet18 during training with (layerwise) starting epochs s = (3, 4, 5, 6), ending epochs e = (150, 150, 170, 275), and pruning fractions p = (0.25, 0.4, 0.25, 0.95). Since the layerwise pruning percentages varied, pruning required multiple iterative pruning percentages, the largest of which is denoted in the legend (rounded to the nearest integer). The models with the maximum iterative pruning percentage of 1% had r = 4, the models with the maximum iterative pruning percentage of 13% had r = 40, and the ""One Shot"" model pruned all its targeted parameters at once on epoch 246.\nWhen performing unstructured pruning, we pruned individual weights from filters based on their magnitude. The structured pruning experiments used E[BN] pruning.\nThe models were trained on CIFAR-10 with Adam for 325 epochs with lrs = (150, 300). The error bars are 95% confidence intervals for the means, bootstrapped from 10 distinct runs of each experiment.\nA.5.4 FIGURE 4\nIn Figure 4, pruning targeted the final four convolutional layers of VGG11 during training with (layerwise) starting epochs s = (3, 4, 5, 6), ending epochs e = (150, 150, 150, 275), and pruning fractions p = (0.3, 0.3, 0.3, 0.9). To create the different iterative pruning rates, we used models with inter-pruning retrain periods r = 4, r = 20, r = 40, r = 60, and r = 100. Since the layerwise pruning percentages varied, pruning required multiple iterative pruning percentages, the largest of which is denoted on the horizontal axis. An unpruned baseline model average (10 runs) is plotted on the dotted line.\nThe models were trained on CIFAR-10 with Adam for 325 epochs with lrs = (150, 300). The error bars are 95% confidence intervals for the means, bootstrapped from 10 distinct runs of each experiment.\nA.5.5 FIGURE 5\nIn Figure 5, pruning targeted the final four convolutional layers of VGG11 during training with (layerwise) starting epochs s = (3, 4, 5, 6), ending epochs e = (150, 150, 150, 275), pruning fractions p = (0.3, 0.3, 0.3, 0.9), and inter-pruning-iteration retrain period r = 40. When injecting pruning noise, we used the same pruning schedule and percentages, but applied noise to the parameters instead of removing them. The Gaussian noise had mean 0 and standard deviation equal to the empirical standard deviation of a noiseless filter from the same layer. PruneL used `2-norm pruning.\nThe models were trained on CIFAR-10 with Adam for 325 epochs with lrs = (150, 300). The error bars are 95% confidence intervals for the means, bootstrapped from 10 distinct runs of each experiment.\nA.5.6 APPENDIX A.4\nEach experiment in Appendix A.4 targeted the post-convolutional linear layer of the Conv4 network during training on CIFAR-10 with the Adam optimizer. The pruning algorithms start on epoch s = (3), end on epoch e = (18), prune the percentage p = (0.9), and prune every epoch via retrain period r = 1. These relatively simple experiments were conducted to show that, at higher sparsity (pruning 85% of the model’s parameters), unstable pruning can improve the generalization of the baseline. The error bars are 95% confidence intervals for the means, bootstrapped from 20 distinct runs of each configuration.'}], 'references': [{'title': 'Sharp minima can generalize for deep nets', 'author': ['Laurent Dinh', 'Razvan Pascanu', 'Samy Bengio', 'Yoshua Bengio'], 'venue': 'arXiv preprint arXiv:1703.04933,', 'citeRegEx': 'Dinh et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Dinh et al\\.', 'year': 2017}, {'title': 'Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data', 'author': ['Gintare Karolina Dziugaite', 'Daniel M Roy'], 'venue': 'arXiv preprint arXiv:1703.11008,', 'citeRegEx': 'Dziugaite and Roy.,? \\Q2017\\E', 'shortCiteRegEx': 'Dziugaite and Roy.', 'year': 2017}, {'title': 'The lottery ticket hypothesis: Finding sparse, trainable neural networks', 'author': ['Jonathan Frankle', 'Michael Carbin'], 'venue': 'arXiv preprint arXiv:1803.03635,', 'citeRegEx': 'Frankle and Carbin.,? \\Q2018\\E', 'shortCiteRegEx': 'Frankle and Carbin.', 'year': 2018}, {'title': 'The state of sparsity in deep neural networks', 'author': ['Trevor Gale', 'Erich Elsen', 'Sara Hooker'], 'venue': 'CoRR, abs/1902.09574,', 'citeRegEx': 'Gale et al\\.,? \\Q2019\\E', 'shortCiteRegEx': 'Gale et al\\.', 'year': 2019}, {'title': 'Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding', 'author': ['Song Han', 'Huizi Mao', 'William J Dally'], 'venue': 'arXiv preprint arXiv:1510.00149,', 'citeRegEx': 'Han et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Han et al\\.', 'year': 2015}, {'title': 'Learning both weights and connections for efficient neural network', 'author': ['Song Han', 'Jeff Pool', 'John Tran', 'William Dally'], 'venue': 'In Advances in neural information processing systems,', 'citeRegEx': 'Han et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Han et al\\.', 'year': 2015}, {'title': 'Second order derivatives for network pruning: Optimal brain surgeon', 'author': ['Babak Hassibi', 'David G Stork'], 'venue': 'In Advances in neural information processing systems,', 'citeRegEx': 'Hassibi and Stork.,? \\Q1993\\E', 'shortCiteRegEx': 'Hassibi and Stork.', 'year': 1993}, {'title': 'Deep residual learning for image recognition', 'author': ['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'], 'venue': 'In Proceedings of the IEEE conference on computer vision and pattern recognition,', 'citeRegEx': 'He et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'He et al\\.', 'year': 2016}, {'title': 'Keeping the neural networks simple by minimizing the description length of the weights', 'author': ['Geoffrey E Hinton', 'Drew Van Camp'], 'venue': 'In Proceedings of the sixth annual conference on Computational learning theory,', 'citeRegEx': 'Hinton and Camp.,? \\Q1993\\E', 'shortCiteRegEx': 'Hinton and Camp.', 'year': 1993}, {'title': 'Batch normalization: Accelerating deep network training by reducing internal covariate shift', 'author': ['Sergey Ioffe', 'Christian Szegedy'], 'venue': 'arXiv preprint arXiv:1502.03167,', 'citeRegEx': 'Ioffe and Szegedy.,? \\Q2015\\E', 'shortCiteRegEx': 'Ioffe and Szegedy.', 'year': 2015}, {'title': 'On large-batch training for deep learning: Generalization gap and sharp minima', 'author': ['Nitish Shirish Keskar', 'Dheevatsa Mudigere', 'Jorge Nocedal', 'Mikhail Smelyanskiy', 'Ping Tak Peter Tang'], 'venue': 'arXiv preprint arXiv:1609.04836,', 'citeRegEx': 'Keskar et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Keskar et al\\.', 'year': 2016}, {'title': 'Adam: A method for stochastic optimization', 'author': ['Diederik P Kingma', 'Jimmy Ba'], 'venue': 'arXiv preprint arXiv:1412.6980,', 'citeRegEx': 'Kingma and Ba.,? \\Q2014\\E', 'shortCiteRegEx': 'Kingma and Ba.', 'year': 2014}, {'title': 'Learning multiple layers of features from tiny images', 'author': ['Alex Krizhevsky', 'Geoffrey Hinton'], 'venue': 'Technical report, Citeseer,', 'citeRegEx': 'Krizhevsky and Hinton.,? \\Q2009\\E', 'shortCiteRegEx': 'Krizhevsky and Hinton.', 'year': 2009}, {'title': 'Smallify: Learning network size while training', 'author': ['Guillaume Leclerc', 'Manasi Vartak', 'Raul Castro Fernandez', 'Tim Kraska', 'Samuel Madden'], 'venue': 'arXiv preprint arXiv:1806.03723,', 'citeRegEx': 'Leclerc et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Leclerc et al\\.', 'year': 2018}, {'title': 'Optimal brain damage', 'author': ['Yann LeCun', 'John S Denker', 'Sara A Solla'], 'venue': 'In Advances in neural information processing systems,', 'citeRegEx': 'LeCun et al\\.,? \\Q1990\\E', 'shortCiteRegEx': 'LeCun et al\\.', 'year': 1990}, {'title': 'Pruning filters for efficient convnets', 'author': ['Hao Li', 'Asim Kadav', 'Igor Durdanovic', 'Hanan Samet', 'Hans Peter Graf'], 'venue': 'arXiv preprint arXiv:1608.08710,', 'citeRegEx': 'Li et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Li et al\\.', 'year': 2016}, {'title': 'Learning efficient convolutional networks through network slimming', 'author': ['Zhuang Liu', 'Jianguo Li', 'Zhiqiang Shen', 'Gao Huang', 'Shoumeng Yan', 'Changshui Zhang'], 'venue': 'In Computer Vision (ICCV),', 'citeRegEx': 'Liu et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Liu et al\\.', 'year': 2017}, {'title': 'Bayesian compression for deep learning', 'author': ['Christos Louizos', 'Karen Ullrich', 'Max Welling'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Louizos et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Louizos et al\\.', 'year': 2017}, {'title': 'Variational dropout sparsifies deep neural networks', 'author': ['Dmitry Molchanov', 'Arsenii Ashukha', 'Dmitry Vetrov'], 'venue': 'arXiv preprint arXiv:1701.05369,', 'citeRegEx': 'Molchanov et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Molchanov et al\\.', 'year': 2017}, {'title': 'Pruning convolutional neural networks for resource efficient transfer learning', 'author': ['Pavlo Molchanov', 'Stephen Tyree', 'Tero Karras', 'Timo Aila', 'Jan Kautz'], 'venue': 'arXiv preprint arXiv:1611.06440,', 'citeRegEx': 'Molchanov et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Molchanov et al\\.', 'year': 2016}, {'title': 'On the importance of single directions for generalization', 'author': ['Ari Morcos', 'David GT Barrett', 'Neil C Rabinowitz', 'Matthew Botvinick'], 'venue': 'In Proceeding of the International Conference on Learning Representations,', 'citeRegEx': 'Morcos et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Morcos et al\\.', 'year': 2018}, {'title': 'Generalization in deep networks: The role of distance from initialization', 'author': ['Vaishnavh Nagarajan', 'J Zico Kolter'], 'venue': 'arXiv preprint arXiv:1901.01672,', 'citeRegEx': 'Nagarajan and Kolter.,? \\Q2019\\E', 'shortCiteRegEx': 'Nagarajan and Kolter.', 'year': 2019}, {'title': 'Exploring sparsity in recurrent neural networks', 'author': ['Sharan Narang', 'Gregory Diamos', 'Shubho Sengupta', 'Erich Elsen'], 'venue': 'arXiv preprint arXiv:1704.05119,', 'citeRegEx': 'Narang et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Narang et al\\.', 'year': 2017}, {'title': 'In search of the real inductive bias: On the role of implicit regularization in deep learning', 'author': ['Behnam Neyshabur', 'Ryota Tomioka', 'Nathan Srebro'], 'venue': 'arXiv preprint arXiv:1412.6614,', 'citeRegEx': 'Neyshabur et al\\.,? \\Q2014\\E', 'shortCiteRegEx': 'Neyshabur et al\\.', 'year': 2014}, {'title': 'Towards understanding the role of over-parametrization in generalization of neural networks', 'author': ['Behnam Neyshabur', 'Zhiyuan Li', 'Srinadh Bhojanapalli', 'Yann LeCun', 'Nathan Srebro'], 'venue': None, 'citeRegEx': 'Neyshabur et al\\.,? \\Q1805\\E', 'shortCiteRegEx': 'Neyshabur et al\\.', 'year': 1805}, {'title': 'Analyzing noise in autoencoders and deep networks', 'author': ['Ben Poole', 'Jascha Sohl-Dickstein', 'Surya Ganguli'], 'venue': 'arXiv preprint arXiv:1406.1831,', 'citeRegEx': 'Poole et al\\.,? \\Q2014\\E', 'shortCiteRegEx': 'Poole et al\\.', 'year': 2014}, {'title': 'Modeling by shortest data', 'author': ['Jorma Rissanen'], 'venue': 'description. Automatica,', 'citeRegEx': 'Rissanen.,? \\Q1978\\E', 'shortCiteRegEx': 'Rissanen.', 'year': 1978}, {'title': 'Understanding machine learning: From theory to algorithms', 'author': ['Shai Shalev-Shwartz', 'Shai Ben-David'], 'venue': 'Cambridge university press,', 'citeRegEx': 'Shalev.Shwartz and Ben.David.,? \\Q2014\\E', 'shortCiteRegEx': 'Shalev.Shwartz and Ben.David.', 'year': 2014}, {'title': 'Very deep convolutional networks for large-scale image recognition', 'author': ['Karen Simonyan', 'Andrew Zisserman'], 'venue': 'arXiv preprint arXiv:1409.1556,', 'citeRegEx': 'Simonyan and Zisserman.,? \\Q2014\\E', 'shortCiteRegEx': 'Simonyan and Zisserman.', 'year': 2014}, {'title': 'Dropout: A simple way to prevent neural networks from overfitting', 'author': ['Nitish Srivastava', 'Geoffrey Hinton', 'Alex Krizhevsky', 'Ilya Sutskever', 'Ruslan Salakhutdinov'], 'venue': 'The Journal of Machine Learning Research,', 'citeRegEx': 'Srivastava et al\\.,? \\Q1929\\E', 'shortCiteRegEx': 'Srivastava et al\\.', 'year': 1929}, {'title': 'Hessian-based analysis of large batch training and robustness to adversaries', 'author': ['Zhewei Yao', 'Amir Gholami', 'Qi Lei', 'Kurt Keutzer', 'Michael W Mahoney'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Yao et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Yao et al\\.', 'year': 2018}, {'title': 'Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers', 'author': ['Jianbo Ye', 'Xin Lu', 'Zhe Lin', 'James Z Wang'], 'venue': 'arXiv preprint arXiv:1802.00124,', 'citeRegEx': 'Ye et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Ye et al\\.', 'year': 2018}, {'title': 'Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks', 'author': ['Zhonghui You', 'Jinmian Yan', 'Kun', 'Meng Ma Ye', 'Ping Wang'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Kun et al\\.,? \\Q2019\\E', 'shortCiteRegEx': 'Kun et al\\.', 'year': 2019}, {'title': 'Nisp: Pruning networks using neuron importance score propagation', 'author': ['Ruichi Yu', 'Ang Li', 'Chun-Fu Chen', 'Jui-Hsin Lai', 'Vlad I Morariu', 'Xintong Han', 'Mingfei Gao', 'Ching-Yung Lin', 'Larry S Davis'], 'venue': 'In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,', 'citeRegEx': 'Yu et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Yu et al\\.', 'year': 2018}, {'title': 'Understanding deep learning requires rethinking generalization', 'author': ['Chiyuan Zhang', 'Samy Bengio', 'Moritz Hardt', 'Benjamin Recht', 'Oriol Vinyals'], 'venue': 'arXiv preprint arXiv:1611.03530,', 'citeRegEx': 'Zhang et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Zhang et al\\.', 'year': 2016}, {'title': 'To prune, or not to prune: exploring the efficacy of pruning for model compression', 'author': ['Michael Zhu', 'Suyog Gupta'], 'venue': 'arXiv preprint arXiv:1710.01878,', 'citeRegEx': 'Zhu and Gupta.,? \\Q2017\\E', 'shortCiteRegEx': 'Zhu and Gupta.', 'year': 2017}, {'title': 'Structured magnitude pruning removes entire filters/neurons, which are selected based on their `2-norms or via the E[BN] calculation. Except where noted, we use structured pruning for VGG11 and ResNet18. We denote the pruning of n layers of a network by specifying a series of epochs at which pruning starts s', 'author': ['Gale'], 'venue': None, 'citeRegEx': 'Gale,? \\Q2019\\E', 'shortCiteRegEx': 'Gale', 'year': 2019}], 'referenceMentions': [{'referenceID': 14, 'context': 'Pruning weights and/or convolutional filters from deep neural networks (DNNs) can substantially shrink parameter counts with minimal loss in accuracy (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015a; Li et al., 2016; Molchanov et al., 2017; Louizos et al., 2017; Liu et al., 2017; Ye et al., 2018), enabling broader application of DNNs via reductions in memory-footprint and inference-FLOPs requirements.', 'startOffset': 150, 'endOffset': 310}, {'referenceID': 15, 'context': 'Pruning weights and/or convolutional filters from deep neural networks (DNNs) can substantially shrink parameter counts with minimal loss in accuracy (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015a; Li et al., 2016; Molchanov et al., 2017; Louizos et al., 2017; Liu et al., 2017; Ye et al., 2018), enabling broader application of DNNs via reductions in memory-footprint and inference-FLOPs requirements.', 'startOffset': 150, 'endOffset': 310}, {'referenceID': 18, 'context': 'Pruning weights and/or convolutional filters from deep neural networks (DNNs) can substantially shrink parameter counts with minimal loss in accuracy (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015a; Li et al., 2016; Molchanov et al., 2017; Louizos et al., 2017; Liu et al., 2017; Ye et al., 2018), enabling broader application of DNNs via reductions in memory-footprint and inference-FLOPs requirements.', 'startOffset': 150, 'endOffset': 310}, {'referenceID': 17, 'context': 'Pruning weights and/or convolutional filters from deep neural networks (DNNs) can substantially shrink parameter counts with minimal loss in accuracy (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015a; Li et al., 2016; Molchanov et al., 2017; Louizos et al., 2017; Liu et al., 2017; Ye et al., 2018), enabling broader application of DNNs via reductions in memory-footprint and inference-FLOPs requirements.', 'startOffset': 150, 'endOffset': 310}, {'referenceID': 16, 'context': 'Pruning weights and/or convolutional filters from deep neural networks (DNNs) can substantially shrink parameter counts with minimal loss in accuracy (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015a; Li et al., 2016; Molchanov et al., 2017; Louizos et al., 2017; Liu et al., 2017; Ye et al., 2018), enabling broader application of DNNs via reductions in memory-footprint and inference-FLOPs requirements.', 'startOffset': 150, 'endOffset': 310}, {'referenceID': 31, 'context': 'Pruning weights and/or convolutional filters from deep neural networks (DNNs) can substantially shrink parameter counts with minimal loss in accuracy (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015a; Li et al., 2016; Molchanov et al., 2017; Louizos et al., 2017; Liu et al., 2017; Ye et al., 2018), enabling broader application of DNNs via reductions in memory-footprint and inference-FLOPs requirements.', 'startOffset': 150, 'endOffset': 310}, {'referenceID': 22, 'context': 'Moreover, many pruning methods have been found to actually improve generalization (measured by model accuracy on previously unobserved inputs) (Narang et al., 2017; Frankle & Carbin, 2018; You et al., 2019).', 'startOffset': 143, 'endOffset': 206}, {'referenceID': 14, 'context': 'Consistent with this, pruning was originally motivated as a means to prevent over-parameterized networks from overfitting to comparatively small datasets (LeCun et al., 1990).', 'startOffset': 154, 'endOffset': 174}, {'referenceID': 23, 'context': 'Concern about over-parameterizing models has weakened, however, as many recent studies have found that adding parameters can actually reduce a DNN’s generalization-gap (the drop in performance when moving from previously seen to previously unseen inputs), even though it has been shown that the same networks have enough parameters to fit large datasets of randomized data (Neyshabur et al., 2014; Zhang et al., 2016).', 'startOffset': 373, 'endOffset': 417}, {'referenceID': 34, 'context': 'Concern about over-parameterizing models has weakened, however, as many recent studies have found that adding parameters can actually reduce a DNN’s generalization-gap (the drop in performance when moving from previously seen to previously unseen inputs), even though it has been shown that the same networks have enough parameters to fit large datasets of randomized data (Neyshabur et al., 2014; Zhang et al., 2016).', 'startOffset': 373, 'endOffset': 417}, {'referenceID': 10, 'context': 'Potential explanations for this unintuitive phenomenon have come via experiments (Keskar et al., 2016; Morcos et al., 2018; Yao et al., 2018; Belkin et al., 2018; Nagarajan & Kolter, 2019), and the derivation of bounds on DNN generalization-gaps that suggest less overfitting might occur as parameter counts increase (Neyshabur et al.', 'startOffset': 81, 'endOffset': 188}, {'referenceID': 20, 'context': 'Potential explanations for this unintuitive phenomenon have come via experiments (Keskar et al., 2016; Morcos et al., 2018; Yao et al., 2018; Belkin et al., 2018; Nagarajan & Kolter, 2019), and the derivation of bounds on DNN generalization-gaps that suggest less overfitting might occur as parameter counts increase (Neyshabur et al.', 'startOffset': 81, 'endOffset': 188}, {'referenceID': 30, 'context': 'Potential explanations for this unintuitive phenomenon have come via experiments (Keskar et al., 2016; Morcos et al., 2018; Yao et al., 2018; Belkin et al., 2018; Nagarajan & Kolter, 2019), and the derivation of bounds on DNN generalization-gaps that suggest less overfitting might occur as parameter counts increase (Neyshabur et al.', 'startOffset': 81, 'endOffset': 188}, {'referenceID': 14, 'context': 'Pruning may be performed post-hoc (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015b; Liu et al., 2017), or iteratively throughout training, such that there are multiple pruning events as the model trains (Hochreiter & Schmidhuber, 1997; Narang et al.', 'startOffset': 34, 'endOffset': 114}, {'referenceID': 16, 'context': 'Pruning may be performed post-hoc (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015b; Liu et al., 2017), or iteratively throughout training, such that there are multiple pruning events as the model trains (Hochreiter & Schmidhuber, 1997; Narang et al.', 'startOffset': 34, 'endOffset': 114}, {'referenceID': 22, 'context': ', 2017), or iteratively throughout training, such that there are multiple pruning events as the model trains (Hochreiter & Schmidhuber, 1997; Narang et al., 2017; Zhu & Gupta, 2017).', 'startOffset': 109, 'endOffset': 181}, {'referenceID': 3, 'context': ', 2015b) uses small-magnitude to indicate unimportance and has been shown to perform competitively with more sophisticated approaches (Gale et al., 2019).', 'startOffset': 134, 'endOffset': 153}, {'referenceID': 22, 'context': 'Many pruning studies have shown that the pruned model has heightened generalization (Narang et al., 2017; Frankle & Carbin, 2018; You et al., 2019), consistent with the fact that pruning may be framed as a regularization (rather than compression) approach.', 'startOffset': 84, 'endOffset': 147}, {'referenceID': 18, 'context': 'For example, variational Bayesian approaches to pruning via sparsity-inducing priors (Molchanov et al., 2017; Louizos et al., 2017) can describe weight removal as a process that reduces model description length, which in theory may help improve generalization (Rissanen, 1978).', 'startOffset': 85, 'endOffset': 131}, {'referenceID': 17, 'context': 'For example, variational Bayesian approaches to pruning via sparsity-inducing priors (Molchanov et al., 2017; Louizos et al., 2017) can describe weight removal as a process that reduces model description length, which in theory may help improve generalization (Rissanen, 1978).', 'startOffset': 85, 'endOffset': 131}, {'referenceID': 26, 'context': ', 2017) can describe weight removal as a process that reduces model description length, which in theory may help improve generalization (Rissanen, 1978).', 'startOffset': 136, 'endOffset': 152}, {'referenceID': 14, 'context': 'VC dimension (a measure of model capacity) has motivated the use of iterative pruning to improve generalization (LeCun et al., 1990; Hassibi & Stork, 1993).', 'startOffset': 112, 'endOffset': 155}, {'referenceID': 25, 'context': 'Rather than support parameter-count-based arguments for generalization in DNNs, our results suggest iterative DNN pruning may improve generalization by creating various noisy versions of the internal representation of the data, which unpruned parameters try to fit to, as in noise-injection regularization (Srivastava et al., 2014; Poole et al., 2014).', 'startOffset': 306, 'endOffset': 351}, {'referenceID': 13, 'context': 'Indeed, applying dropout-like zeroing noise to a subset of features during training can encourage robustness to a post-hoc pruning of that subset (Leclerc et al., 2018; Gomez et al., 2018).', 'startOffset': 146, 'endOffset': 188}, {'referenceID': 15, 'context': 'The usual approach to pruning involves removing parameters that have the smallest magnitudes (Li et al., 2016; Gale et al., 2019), or, similarly, those parameters least important to the loss function as determined by some other metric (LeCun et al.', 'startOffset': 93, 'endOffset': 129}, {'referenceID': 3, 'context': 'The usual approach to pruning involves removing parameters that have the smallest magnitudes (Li et al., 2016; Gale et al., 2019), or, similarly, those parameters least important to the loss function as determined by some other metric (LeCun et al.', 'startOffset': 93, 'endOffset': 129}, {'referenceID': 14, 'context': ', 2019), or, similarly, those parameters least important to the loss function as determined by some other metric (LeCun et al., 1990; Hassibi & Stork, 1993; Molchanov et al., 2016; 2017; Louizos et al., 2017; Ye et al., 2018; Yu et al., 2018; You et al., 2019).', 'startOffset': 113, 'endOffset': 260}, {'referenceID': 19, 'context': ', 2019), or, similarly, those parameters least important to the loss function as determined by some other metric (LeCun et al., 1990; Hassibi & Stork, 1993; Molchanov et al., 2016; 2017; Louizos et al., 2017; Ye et al., 2018; Yu et al., 2018; You et al., 2019).', 'startOffset': 113, 'endOffset': 260}, {'referenceID': 17, 'context': ', 2019), or, similarly, those parameters least important to the loss function as determined by some other metric (LeCun et al., 1990; Hassibi & Stork, 1993; Molchanov et al., 2016; 2017; Louizos et al., 2017; Ye et al., 2018; Yu et al., 2018; You et al., 2019).', 'startOffset': 113, 'endOffset': 260}, {'referenceID': 31, 'context': ', 2019), or, similarly, those parameters least important to the loss function as determined by some other metric (LeCun et al., 1990; Hassibi & Stork, 1993; Molchanov et al., 2016; 2017; Louizos et al., 2017; Ye et al., 2018; Yu et al., 2018; You et al., 2019).', 'startOffset': 113, 'endOffset': 260}, {'referenceID': 33, 'context': ', 2019), or, similarly, those parameters least important to the loss function as determined by some other metric (LeCun et al., 1990; Hassibi & Stork, 1993; Molchanov et al., 2016; 2017; Louizos et al., 2017; Ye et al., 2018; Yu et al., 2018; You et al., 2019).', 'startOffset': 113, 'endOffset': 260}, {'referenceID': 31, 'context': 'This has motivated approaches to use the scale parameter γ’s magnitude to find the convolutional filters that are important to the network’s output (Ye et al., 2018; You et al., 2019).', 'startOffset': 148, 'endOffset': 183}, {'referenceID': 7, 'context': 'We consider three basic model classes: a simple network with convolutions (2x32, pool, 2x64, pool) and fully connected layers (512, 10) that we denote Conv4, VGG11 (Simonyan & Zisserman, 2014) with its fully-connected layers replaced by a single fully-connected layer, and ResNet18 (He et al., 2016).', 'startOffset': 282, 'endOffset': 299}, {'referenceID': 22, 'context': 'As a bonus, such pruning methods can sometimes even increase generalization (Narang et al., 2017; Frankle & Carbin, 2018; You et al., 2019).', 'startOffset': 76, 'endOffset': 139}, {'referenceID': 20, 'context': 'Consequently, pruneL with `2-norm pruning may generalize worse at higher iterative rates by leaving unpruned more important weights, the presence of which can harm model generalization (Hinton & Van Camp, 1993; Morcos et al., 2018).', 'startOffset': 185, 'endOffset': 231}, {'referenceID': 25, 'context': 'We first note that iterative pruning can be viewed as noise injection (Srivastava et al., 2014; Poole et al., 2014), with the peculiarity that the noise permanently zeroes a subset of weights.', 'startOffset': 70, 'endOffset': 115}], 'year': 2019, 'abstractText': 'Pruning neural network parameters is often viewed as a means to compress models, but pruning has also been motivated by the desire to prevent overfitting. This motivation is particularly relevant given the perhaps surprising observation that a wide variety of pruning approaches increase test accuracy despite sometimes massive reductions in parameter counts. To better understand this phenomenon, we analyze the behavior of pruning over the course of training, finding that pruning’s effect on generalization relies more on the instability it generates (defined as the drops in test accuracy immediately following pruning) than on the final size of the pruned model. We demonstrate that even the pruning of unimportant parameters can lead to such instability, and show similarities between pruning and regularizing by injecting noise, suggesting a mechanism for pruning-based generalization improvements that is compatible with the strong generalization recently observed in over-parameterized networks.', 'creator': 'LaTeX with hyperref package'}","pruning weights and/or convolutional filters from deep neural networks (dnns) can substantially shrink parameter counts with minimal loss in accuracy (lecun et al., 1990; hassibi & stork, 1993; han et al., 2015a; li et al., 2016; molchanov et al., 2017; louizos et al., 2017; liu et al., 2017; ye et al., 2018), enabling broader application of dnns via reductions in memory-footprint and inference-flops requirements. moreover, many pruning methods have been found to actually improve generalization (measured by model accuracy on previously unobserved inputs) (narang et al., 2017; frankle & carbin, 2018; you et al., 2019). consistent with this, pruning was originally motivated as a means to prevent over-parameterized networks from overfitting to comparatively small datasets (lecun et al., 1990). concern about over-parameterizing models has weakened, however, as many recent studies have found that adding parameters can actually reduce a dnn’s generalization-gap (the drop in performance when moving from previously seen to previously unseen inputs), even though it has been shown that the same networks have enough parameters to fit large datasets of randomized data (neyshabur et al., 2014; zhang et al., 2016). potential explanations for this unintuitive phenomenon have come via experiments (keskar et al., 2016; morcos et al., 2018; yao et al., 2018; belkin et al., 2018; nagarajan & kolter, 2019), and the derivation of bounds on dnn generalization-gaps that suggest less overfitting might occur as parameter counts increase (neyshabur et al., 2018). this research has implications for neural network pruning, where a puzzling question has arisen: if larger parameter counts don’t increase overfitting, how does pruning parameters throughout training improve generalization? to address this question we first introduce the notion of pruning instability, which we define to be the size of the drop in network accuracy caused by a pruning iteration (section 3). we then empirically analyze the instability and generalization associated with various magnitude-pruning (han et al., 2015b) algorithms in different settings, making the following contributions: 1. we find a tradeoff between the stability and potential generalization benefits of pruning, and show iterative pruning’s similarity to regularizing with noise—suggesting a mechanism unrelated to parameter counts through which pruning appears to affect generalization. 2. we characterize the properties of pruning algorithms which lead to instability and correspondingly higher generalization. 3. we derive a batch-normalized-parameter pruning algorithm to better control pruning stability.","pruning weights and/or convolutional filters from deep neural networks (dnns) can substantially shrink parameter counts with minimal loss in accuracy (lecun et al , 2018), enabling broader application of dnns via reductions in memory-footprint and inference-flops requirements moreover, many pruning methods have been found to actually improve generalization (measured by model accuracy on previously unobserved inputs) (narang et al consistent with this, pruning was originally motivated as a means to prevent over-parameterized networks from overfitting to comparatively small datasets (lecun et al concern about over-parameterizing models has weakened, however, as many recent studies have found that adding parameters can actually reduce a dnn’s generalization-gap (the drop in performance when moving from previously seen to previously unseen inputs), even though it has been shown that the same networks have enough parameters to fit large datasets of randomized data (neyshabur et al , 2018; nagarajan & kolter, 2019), and the derivation of bounds on dnn generalization-gaps that suggest less overfitting might occur as parameter counts increase (neyshabur et al this research has implications for neural network pruning, where a puzzling question has arisen: if larger parameter counts don’t increase overfitting, how does pruning parameters throughout training improve generalization? to address this question we first introduce the notion of pruning instability, which we define to be the size of the drop in network accuracy caused by a pruning iteration (section 3) we find a tradeoff between the stability and potential generalization benefits of pruning, and show iterative pruning’s similarity to regularizing with noise—suggesting a mechanism unrelated to parameter counts through which pruning appears to affect generalization",0.9542170763015747,0.9188603162765503,0.9362049698829651
ICLR_2020_585,"Real or Not Real, that is the Question","{'source': 'CRF', 'title': None, 'authors': ['Yuanbo Xiangli', 'Yubin Deng', 'Bo Dai', 'Chen Change Loy', 'Dahua Lin'], 'emails': ['xy019@ie.cuhk.edu.hk', 'dy015@ie.cuhk.edu.hk', 'bdai@ie.cuhk.edu.hk', 'dhlin@ie.cuhk.edu.hk', 'ccloy@ntu.edu.sg'], 'sections': [{'heading': '1 INTRODUCTION', 'text': 'The development of generative adversarial network (GAN) (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017) is one of the most important topics in machine learning since its first appearance in (Goodfellow et al., 2014). It learns a discriminator along with the target generator in an adversarial manner, where the discriminator distinguishes generated samples from real ones. Due to its flexibility when dealing with high dimensional data, GAN has obtained remarkable progresses on realistic image generation (Brock et al., 2019).\nIn the standard formulation (Goodfellow et al., 2014), the realness of an input sample is estimated by the discriminator using a single scalar. However, for high dimensional data such as images, we naturally perceive them from more than one angles and deduce whether it is life-like based on multiple criteria. As shown in Fig.1, when a portrait is given, one might focus on its facial structure, skin tint, hair texture and even details like iris and teeth if allowed, each of which indicates a different aspect of realness. Based on this observation, the single scalar could be viewed as an abstract or a summarization of multiple measures, which together reflect the overall realness of an image. Such a concise measurement may convey insufficient information to guide the generator, potentially leading to well-known issues such as mode-collapse and gradient vanishing.\nIn this paper, we propose to generalize the standard framework (Goodfellow et al., 2014) by treating realness as a random variable, represented as a distribution rather than a single scalar. We refer to\n∗Equal contribution. 1Code will be available at https://github.com/kam1107/RealnessGAN\nsuch a generalization as RealnessGAN. The learning process of RealnessGAN abide by the standard setting, but in a distributional form. While the standard GAN can be viewed as a special case of RealnessGAN, RealnessGAN and the standard GAN share similar theoretical guarantees. i.e. RealnessGAN converges to a Nash-equilibrium where the generator and the discriminator reach their optimalities. Moreover, by expanding the scalar realness score into a distributional one, the discriminator D naturally provides stronger guidance to the generator G where G needs to match not only the overall realness (as in the standard GAN), but the underlying realness distribution as well. Consequently, RealnessGAN facilitates G to better approximate the data manifold while generating decent samples. As shown in the experiments, based on a rather simple DCGAN architecture, RealnessGAN could successfully learn from scratch to generate realistic images at 1024*1024 resolution.'}, {'heading': '2 REALNESSGAN', 'text': ''}, {'heading': '2.1 GENERATIVE ADVERSARIAL NETWORKS', 'text': 'Generative adversarial network jointly learns a generatorG and a discriminatorD, whereG attempts to generate samples that are indistinguishable from the real ones, andD classifies generated and real samples. In the original work of (Goodfellow et al., 2014), the learning process of D and G follows a minimax game with value function V (G,D):\nmin G max D V (G,D) = Ex∼pdata [logD(x)] + Ez∼pz [log(1−D(G(z)))], (1)\n= Ex∼pdata [log(D(x)− 0)] + Ex∼pg [log(1−D(x))], (2)\nwhere the approximated data distribution pg is defined by a prior pz on input latent variables and G. As proved by Goodfellow et al. (2014), under such a learning objective, the optimal D satisfies D∗G(x) = pdata(x) pdata(x)+pg(x)\nfor a fixed G. Fixing D at its optimal, the optimal G satisfies pg = pdata. The theoretical guarantees provide strong supports for GAN’s success in many applications (Radford et al., 2015; Yu et al., 2017; Zhu et al., 2017; Dai et al., 2017), and inspired multiple variants (Arjovsky et al., 2017; Mao et al., 2017; Zhao et al., 2017; Berthelot et al., 2017) to improve the original design. Nevertheless, a single scalar is constantly adopted as the measure of realness, while the concept of realness is essentially a random variable covering multiple factors, e.g. texture and overall configuration in the case of images. In this work, we intend to follow this observation, encouraging the discriminator D to learn a realness distribution.'}, {'heading': '2.2 A DISTRIBUTIONAL VIEW ON REALNESS', 'text': 'We start by substituting the scalar output of a discriminator D with a distribution prealness, so that for an input sample x, D(x) = {prealness(x, u);u ∈ Ω}, where Ω is the set of outcomes of prealness. Each outcome u can be viewed as a potential realness measure, estimated via some criteria. While 0 and 1 in equation 2 are used as two virtual ground-truth scalars that respectively represent the realness of real and fake images, we also need two virtual ground-truth distributions to stand for the realness distributions of real and fake images. We refer to these two distributions as A1 (real) and A0 (fake), which are also defined on Ω. As in the standard GAN where 0 and 1 can be replaced with other scalars such as−1 and 1, there are various choices forA1 andA0. Factors lead to a good pair ofA1 and A0 will be discussed later. Accordingly, the difference between two scalars is replaced with the Kullback-Leibler (KL) divergence. The minimax game between a generator G and a distributional discriminator D thus becomes\nmax G min D V (G,D) = Ex∼pdata [DKL(A1‖D(x))] + Ex∼pg [DKL(A0‖D(x))]. (3)\nAn immediate observation is that if we let prealness be a discrete distribution with two outcomes {u0, u1}, and setA0(u0) = A1(u1) = 1 andA0(u1) = A1(u0) = 0, the updated objective in equation 3 can be explicitly converted to the original objective in equation 2, suggesting RealnessGAN is a generalized version of the original GAN.\nFollowing this observation, we then extend the theoretical analysis in Goodfellow et al. (2014) to the case of RealnessGAN. Similar to Goodfellow et al. (2014), our analysis concerns the space of\nprobability density functions, where D and G are assumed to have infinite capacities. We start from finding the optimal realness discriminator D for any given generator G. Theorem 1. When G is fixed, for any outcome u and input sample x, the optimal discriminator D satisfies\nD?G(x, u) = A1(u)pdata(x) +A0(u)pg(x)\npdata(x) + pg(x) . (4)\nProof. Given a fixed G, the objective of D is:\nmin D V (G,D) = Ex∼pdata [DKL(A1‖D(x))] + Ex∼pg [DKL(A0‖D(x))], (5)\n= ∫ x ( pdata(x) ∫ u A1(u) log A1(u) D(x, u) du+ pg(x) ∫ u A0(u) log A0(u) D(x, u) du ) dx,\n(6) = − ∫ x (pdata(x)h(A1) + pg(x)h(A0)) dx\n− ∫ x ∫ u (pdata(x)A1(u) + pg(x)A0(u)) logD(x, u)dudx, (7)\nwhere h(A1) and h(A0) are their entropies. Marking the first term in equation 7 as C1 since it is irrelevant to D, the objective thus is equivalent to:\nmin D V (G,D) = − ∫ x (pdata(x) + pg(x)) ∫ u pdata(x)A1(u) + pg(x)A0(u) pdata(x) + pg(x) logD(x, u)dudx+ C1,\n(8)\nwhere px(u) = pdata(x)A1(u)+pg(x)A0(u)\npdata(x)+pg(x) is a distribution defined on Ω. Let C2 = pdata(x) + pg(x),\nwe then have\nmin D V (G,D) = C1 + ∫ x C2 ( − ∫ u px(u) logD(x, u)du+ h(px)− h(px) ) dx, (9)\n= C1 + ∫ x C2DKL(px‖D(x))dx+ ∫ x C2h(px)dx. (10)\nObserving equation 10, one can see that for any valid x, when DKL(px‖D(x)) achieves its minimum, D obtains its optimal D?, leading to D?(x) = px, which concludes the proof.\nNext, we move on to the conditions for G to reach its optimal when D = D?G. Theorem 2. When D = D?G, and there exists an outcome u ∈ Ω such that A1(u) 6= A0(u), the maximum of V (G,D?G) is achieved if and only if pg = pdata.\nProof. When pg = pdata, D?G(x, u) = A1(u)+A0(u) 2 , we have:\nV ?(G,D?G) = ∫ u A1(u) log 2A1(u) A1(u) +A0(u) +A0(u) log 2A0(u) A1(u) +A0(u) du. (11)\nSubtracting V ?(G,D?G) from V (G,D ? G) gives: V ′(G,D?G) = V (G,D ? G)− V ?(G,D?G)\n= ∫ x ∫ u (pdata(x)A1(u) + pg(x)A0(u)) log (pdata(x) + pg(x))(A1(u) +A0(u)) 2(pdata(x)A1(u) + pg(x)A0(u)) dudx,\n(12) = −2 ∫ x ∫ u pdata(x)A1(u) + pg(x)A0(u) 2 log pdata(x)A1(u)+pg(x)A0(u) 2 (pdata(x)+pg(x))(A1(u)+A0(u))\n4\ndudx,\n(13)\n= −2DKL( pdataA1 + pgA0 2 ‖ (pdata + pg)(A1 +A0) 4 ). (14)\nSince V ?(G,D?G) is a constant with respect to G, maximizing V (G,D ? G) is equivalent to maximizing V ′(G,D?G). The optimal V ′(G,D?G) is achieved if and only if the KL divergence reaches its minimum, where:\npdataA1 + pgA0 2 = (pdata + pg)(A1 +A0) 4 , (15)\n(pdata − pg)(A1 −A0) = 0, (16)\nfor any valid x and u. Hence, as long as there exists a valid u that A1(u) 6= A0(u), we have pdata = pg for any valid x.'}, {'heading': '2.3 DISCUSSION', 'text': 'The theoretical analysis gives us more insights on RealnessGAN.\nNumber of outcomes: according to equation 16, each u ∈ Ω with A0(u) 6= A1(u) may work as a constraint, pushing pg towards pdata. In the case of discrete distributions, along with the increment of the number of outcomes, the constraints imposed on G accordingly become more rigorous and can cost G more effort to learn. This is due to the fact that having more outcomes suggests a more finegrained shape of the realness distribution for G to match. In Sec.4, we verified that it is beneficial to update G an increasing number of times before D’s update as the number of outcomes grows.\nEffectiveness of anchors: view equation 16 as a cost function to minimize, when pdata 6= pg , for some u ∈ Ω, the larger the difference betweenA1(u) andA0(u) is, the stronger the constraint on G becomes. Intuitively, RealnessGAN can be more efficiently trained if we choose A0 and A1 to be adequately different.\nObjective of G: according to equation 3, the best way to fool D is to increase the KL divergence between D(x) and the anchor distribution A0 of fake samples, rather than decreasing the KL divergence betweenD(x) and the anchor distributionA1 of real samples. It’s worth noting that these two objectives are equivalent in the original work (Goodfellow et al., 2014). An intuitive explanation is that, in the distributional view of realness, realness distributions of real samples are not necessarily identical. It is possible that each of them corresponds to a distinct one. While A1 only serves as an anchor, it is ineffective to drag all generated samples towards the same target.\nFlexibility of RealnessGAN: as a generalization of the standard framework, it is straightforward to integrate RealnessGAN with different GAN architectures, such as progressive GANs (Karras et al., 2018; 2019) and conditional GANs (Zhu et al., 2017; Ledig et al., 2017). Moreover, one may also combine the perspective of RealnessGAN with other reformulations of the standard GAN, such as replacing the KL divergence in equation 3 with the Earth Mover’s Distance.'}, {'heading': '2.4 IMPLEMENTATION', 'text': 'In our implementation, the realness distribution prealness is characterized as a discrete distribution over N outcomes Ω = {u0, u1, ..., uN−1}. Given an input sample x, the discriminator D returns N probabilities on these outcomes, following:\nprealness(x, ui) = eψi(x)∑ j e ψj(x) , (17)\nwhere ψ = (ψ0,ψ1, ...,ψN−1) are the parameters of D. Similarly, A1 and A0 are discrete distributions defined on Ω.\nAs shown in the theoretical analysis, the ideal objective for G is maximizing the KL divergence between D(x) of generated samples and A0:\n(Gobjective1) min G −Ez∼pz [DKL(A0‖D(G(z))]. (18)\nHowever, as the discriminator D is not always at its optimal, especially in the early stage, directly applying this objective in practice could only lead to a generator with limited generative power. Consequently, a regularizer is needed to improve G. There are several choices for the regularizer, such as the relativistic term introduced in (Jolicoeur-Martineau, 2019) that minimizes the KL divergence\nbetween D(x) of generated samples and random real samples, or the term that minimizes the KL divergence betweenA1 andD(x) of generated samples, each of which leads to a different objective:\n(Gobjective2) min G Ex∼pdata,z∼pz [DKL(D(x)‖D(G(z))]− Ez∼pz [DKL(A0‖D(G(z))], (19)\n(Gobjective3) min G Ez∼pz [DKL(A1‖D(G(z))]− Ez∼pz [DKL(A0‖D(G(z))]. (20)\nIn Sec.4, these objectives are compared. And the objective in equation 19 is adopted as the default choice.\nFeature resampling. In practice, especially in the context of images, we are learning from a limited number of discrete samples coming from a continuous data manifold. We may encounter issues caused by insufficient data coverage during the training process. Inspired by conditioning augmentation mentioned in (Zhang et al., 2016), we introduce a resampling technique performed on the realness output to augment data variance. Given a mini-batch {x0, ...,xM−1} of size M , a Gaussian distributionN (µi, σi) is fitted on {ψi(x0),ψi(x1), ...,ψi(xM−1)}, which are logits computed by D on i-th outcome. We then resample M new logits {ψ′i(x0), ...,ψ′i(xM−1);ψ′i ∼ N (µi, σi)} for i-th outcome and use them succeedingly.\nThe randomness introduced by resampling benefits the training of RealnessGAN in two aspects. First of all, it augments data by probing instances around the limited training samples, leading to more robust models. Secondly, the resampling approach implicitly demands instances of ψi(x) to be homologous throughout the mini-batch, such that each outcome reflects realness consistently across samples. We empirically found the learning curve of RealnessGAN is more stable if feature resampling is utilized, especially in the latter stage, where models are prone to overfit.'}, {'heading': '3 RELATED WORK', 'text': 'Generative adversarial network (GAN) was first proposed in (Goodfellow et al., 2014), which jointly learns a discriminatorD and a generatorG in an adversarial manner. Due to its outstanding learning ability, GANs have been adopted in various generative tasks (Radford et al., 2015; Yu et al., 2017; Zhu et al., 2017), among which Deep Convolutional GAN (DCGAN) (Radford et al., 2015) has shown promising results in image generation.\nAlthough remarkable progress has been made. GAN is known to suffer from gradient diminishing and mode collapse. Variants of GAN have been proposed targeting these issues. Specifically, Wasserstein GAN (WGAN) Arjovsky et al. (2017) replaces JS-divergence with Earth-Mover’s Distance, and Least-Square GAN (LSGAN) (Mao et al., 2017) transforms the objective ofG to Pearson divergence. Energy-based GAN (EBGAN) (Zhao et al., 2017) and Boundary Equilibrium GAN (BEGAN) (Berthelot et al., 2017) employ a pre-trained auto-encoder as the discriminator, learning to distinguish between real and generated samples via reconstruction. Besides adjusting the objective of GAN, alternative approaches include more sophisticated architectures and training paradigms. Generally, ProgressiveGAN (Karras et al., 2018) and StyleGAN (Karras et al., 2019) propose a progressive paradigm, which starts from a shallow model focusing on a low resolution, and gradually grows into a deeper model to incorporate more details as resolution grows. On the other hand, COCO-GAN (Lin et al., 2019) tackles high resolution image generation in a divide-and-conquer strategy. It learns to produce decent patches at corresponding sub-regions, and splices the patches to produce a higher resolution image.\nIt’s worth noting that many works on generative adversarial networks have discussed ‘distributions’ (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017), which usually refers to the underlying distribution of samples. Some of the existing works aim to improve the original objective using different metrics to measure the divergence between the learned distribution pg and the real distribution pdata. Nevertheless, a single scalar is constantly adopted to represent the concept of realness. In this paper, we propose a complementary modification that models realness as a random variable follows the distribution prealness. In the future work, we may study the combination of realness discriminator and other GAN variants to enhance the effectiveness and stability of adversarial learning.'}, {'heading': '4 EXPERIMENTS', 'text': 'In this section we study RealnessGAN from multiple aspects. Specifically, 1) we firstly focus on RealnessGAN’s mode coverage ability on a synthetic dataset. 2) Then we evaluate RealnessGAN on CIFAR10 (32*32) (Krizhevsky, 2009) and CelebA (256*256) (Liu et al., 2015) datasets qualitatively and quantitatively. 3) Finally we explore RealnessGAN on high-resolution image generation task, which is known to be challenging for unconditional non-progressive architectures. Surprisingly, on the FFHQ dataset (Karras et al., 2019), RealnessGAN managed to generate images at the 1024*1024 resolution based on a non-progressive architecture. We compare RealnessGAN to other popular objectives in generative adversarial learning, including the standard GAN (Std-GAN) (Radford et al., 2015), WGAN-GP (Arjovsky et al., 2017), HingeGAN (Zhao et al., 2017) and LSGAN (Mao et al., 2017).\nFor experiments on synthetic dataset, we use a generator with four fully-connected hidden layers, each of which has 400 units, followed by batch normalization and ReLU activation. The discriminator has three fully-connected hidden layers, with 200 units each layer. LinearMaxout with 5 maxout pieces are adopted and no batch normalization is used in the discriminator. The latent input z is a 32-dimensional vector sampled from a Gaussian distribution N (0, I). All models are trained using Adam (Kingma & Ba, 2015) for 500 iterations.\nOn real-world datasets, the network architecture is identical to the DCGAN architecture in Radford et al. (2015), with the prior pz(z) a 128-dimensional Gaussian distribution N (0, I). Models are trained using Adam (Kingma & Ba, 2015) for 520k iterations. To guarantee training stability, we adopt settings that are proved to be effective for baseline methods. Batch normalization (Ioffe & Szegedy, 2015) is used in G, and spectral normalization (Miyato et al., 2018) is used in D. For WGAN-GP we use lr = 1e − 4, β1 = 0.5, β2 = 0.9, updating D for 5 times per G’s update (Gulrajani et al., 2017); for the remaining models, we use lr = 2e − 4, β1 = 0.5, β2 = 0.999, updating D for one time per G’s update (Radford et al., 2015). Fréchet Inception Distance (FID) (Heusel et al., 2017) and Sliced Wasserstein Distance (SWD) (Karras et al., 2018) are reported as the evaluation metrics. Unless otherwise stated, A1 and A0 are chosen to resemble the shapes of two normal distributions with a positive skewness and a negative skewness, respectively. In particular, the number of outcomes are empirically set to 51 for CelebA and FFHQ datasets, and 3 for CIFAR10 dataset.'}, {'heading': '4.1 SYNTHETIC DATASET', 'text': 'Since pdata is usually intractable on real datasets, we use a toy dataset to compare the learned distribution pg and the data distribution pdata. The toy dataset consists of 100, 000 2D points sampled from a mixture of 9 isotropic Gaussian distributions whose means are arranged in a 3 by 3 grid, with variances equal to 0.05. As shown in Fig.2, the data distribution pdata contains 9 welly separated modes, making it a difficult task despite its low-dimensional nature.\nTo evaluate pg , we draw 10, 000 samples and measure their quality and diversity. As suggested in (Dumoulin et al., 2016), we regard a sample as of high quality if it is within 4σ from the µ of its nearest Gaussian. When a Gaussian is assigned with more than 100 high quality samples, we consider this mode of pdata is recovered in pg . Fig.2 visualizes the sampled points of different methods, where LSGAN and HingeGAN suffer from significant mode collapse, recovering only a single mode. Points sampled by WGAN-GP are overly disperse, and only 0.03% of them are of high quality. While Std-GAN recovers 4 modes in pdata with 32.4% high quality samples, 8 modes are recovered by RealnessGAN with 60.2% high quality samples. The average σs of these high quality samples in Std-GAN and RealnessGAN are respectively 0.083 and 0.043. The results suggest that treating realness as a random variable rather than a single scalar leads to a more strict discriminator\nthat criticizes generated samples from various aspects, which provides more informative guidance. Consequently, pg learned by RealnessGAN is more diverse and compact.\nWe further study the effect of adjusting the number of outcomes in the realness distribution prealness on this dataset. To start with, we fix kG and kD to be 1, which are the number of updates forG andD in one iteration, and adjust the number of outcomes of prealness,A0 andA1. As shown in the first row of Fig.3, it can be observed that in general G recovers less modes as the number of outcomes grows, which is a direct result of D becoming increasingly rigorous and imposing more constraints on G. An intuitive solution is to increase kG such that G is able to catch up with current D. The second row of Fig.3 demonstrates the converged cases achieved with suitable kGs, suggesting RealnessGAN is effective when sufficient learning capacity is granted to G. The ratio of high quality samples rHQ and the number of recovered modes nmode in these cases are plotted in Fig.3. The two curves imply that besides kG, rHQ and nmode are all positively related to the number of outcomes, validating that measuring realness from more aspects leads to a better generator.'}, {'heading': '4.2 REAL-WORLD DATASETS', 'text': 'As GAN has shown promising results when modeling complex data such as natural images, we evaluate RealnessGAN on real-world datasets, namely CIFAR10, CelebA and FFHQ, which respectively contains images at 32*32, 256*256 and 1024*1024 resolutions. The training curves of baseline methods and RealnessGAN on CelebA and CIFAR10 are shown in Fig.4. The qualitative results measured in FID and SWD are listed in Tab.1. We report the minimum, the maximum, the mean and the standard deviation computed along the training process. On both datasets, compared to baselines, RealnessGAN obtains better scores in both metrics. Meantime, the learning process of RealnessGAN is smoother and steadier (see SD in Tab.1 and curves in Fig.4). Samples of generated images on both datasets are included in Fig.8.\nOn FFHQ, we push the resolution of generated images to 1024*1024, which is known to be challenging especially for a non-progressive architecture. As shown in Fig.8, despite building on a relatively simple DCGAN architecture, RealnessGAN is able to produce realistic samples from scratch at such a high resolution. Quantitatively, RealnessGAN obtains an FID score of 17.18. For reference, our\nre-implemented StyleGAN (Karras et al., 2019) trained under a similar setting receives an FID score of 16.12. These results strongly support the effectiveness of RealnessGAN, as StyleGAN is one of the most advanced GAN architectures so far.'}, {'heading': '4.3 ABLATION STUDY', 'text': 'The implementation of RealnessGAN offers several choices that also worth digging into. On synthetic dataset, we explored the relationship between the number of outcomes and G’s update frequency. On real-world dataset, apart from evaluating RealnessGAN as a whole, we also studied the affect of feature resampling, different settings of A0 and A1 and choices of G’s objective.\nTable 2: Minimum (min), maximum (max), mean and standard deviation (SD) of FID on CelebA using different anchor distributions, calculated at 20k, 30k, ... iterations.\nDKL(A1‖A0) Min Max Mean SD 1.66 31.01 96.11 40.75 11.83 5.11 26.22 87.98 36.11 9.83 7.81 25.98 85.51 36.30 10.04\n11.05 23.51 81.30 30.82 7.61\n0 10 20 30 40 50 Iterations (10k)\n20 40 60 80\n100 120 140 160 180 200 FI D\nw/o resampling w/ resampling\nFigure 5: Training FID curves of RealnessGAN with and without feature re-sampling.\nFeature Resampling. Fig.5 shows the training curves of RealnessGAN with and without feature resampling. It can be noticed that despite the results are similar, feature resampling stabilizes the training process especially in the latter stage.\nEffectiveness of Anchors. Tab.2 reports the results of varying the KL divergence between anchor distributions A0 and A1. The FID score indicates that, as the KL divergence between A0 and A1 increases, RealnessGAN tends to perform better, which verifies our discussion in Sec.2.3 that a larger difference between anchor distributions imposes stronger constraints on G. To further testify, two different pairs of anchors with similar KL divergences (11.95 and 11.67) are exploited and they yield comparable FID scores (23.98 and 24.22).\nObjective of G. As mentioned in Sec.2.3, theoretically, the objective of G is maxG Ex∼pg [DKL(A0‖D(x))]. However, in practice, since D is not always optimal, we need either a pair of A0 and A1 that are drastically different, or an additional constraint to aid this objective. Fig.6 shows that, with the ideal objective alone, when the KL divergence between A0 and A1 is sufficiently large, on CelebA we could obtain a generator with limited generative power. On the other hand, by applying constraints as discussed in Sec.2.4, G can learn to produce more realistic samples as demonstrated in Fig.8. Similar results are observed on CIFAR10, where RealnessGAN obtains comparable FID scores with and without constraints, as shown in Tab.3. Fig.7 also provides the training curves of RealnessGAN on CelebA using these two alternative objectives.'}, {'heading': '5 CONCLUSION', 'text': 'In this paper, we extend the view of realness in generative adversarial networks under a distributional perspective. In our proposed extension, RealnessGAN, we represent the concept of realness as a realness distribution rather than a single scalar. so that the corresponding discriminator estimates realness from multiple angles, providing more informative guidance to the generator. We prove RealnessGAN has theoretical guarantees on the optimality of the generator and the discriminator. On both synthetic and real-world datasets, RealnessGAN also demonstrates the ability of effectively and steadily capturing the underlying data distribution.\nAcknowledgement We thank Zhizhong Li for helpful discussion on the theoretical analysis. This work is partially supported by the Collaborative Research Grant of ”Large-scale Multi-modality Analytics” from SenseTime (CUHK Agreement No. TS1712093), the General Research Funds (GRF) of Hong Kong (No. 14209217 and No. 14205719), Singapore MOE AcRF Tier 1, NTU SUG, and NTU NAP.'}], 'references': [{'title': 'Began: Boundary equilibrium generative adversarial networks', 'author': ['David Berthelot', 'Thomas Schumm', 'Luke Metz'], 'venue': 'arXiv preprint arXiv:1703.10717,', 'citeRegEx': 'Berthelot et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Berthelot et al\\.', 'year': 2017}, {'title': 'Large scale gan training for high fidelity natural image synthesis', 'author': ['Andrew Brock', 'Jeff Donahue', 'Karen Simonyan'], 'venue': 'In ICLR,', 'citeRegEx': 'Brock et al\\.,? \\Q2019\\E', 'shortCiteRegEx': 'Brock et al\\.', 'year': 2019}, {'title': 'Towards diverse and natural image descriptions via a conditional gan', 'author': ['Bo Dai', 'Sanja Fidler', 'Raquel Urtasun', 'Dahua Lin'], 'venue': 'In Proceedings of the IEEE International Conference on Computer Vision,', 'citeRegEx': 'Dai et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Dai et al\\.', 'year': 2017}, {'title': 'Generative adversarial nets', 'author': ['Ian Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Aaron Courville', 'Yoshua Bengio'], 'venue': 'In NIPS,', 'citeRegEx': 'Goodfellow et al\\.,? \\Q2014\\E', 'shortCiteRegEx': 'Goodfellow et al\\.', 'year': 2014}, {'title': 'Improved training of wasserstein gans', 'author': ['Ishaan Gulrajani', 'Faruk Ahmed', 'Martin Arjovsky', 'Vincent Dumoulin', 'Aaron Courville'], 'venue': 'In Proceedings of the 31st International Conference on Neural Information Processing Systems,', 'citeRegEx': 'Gulrajani et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Gulrajani et al\\.', 'year': 2017}, {'title': 'Gans trained by a two time-scale update rule converge to a local nash equilibrium', 'author': ['Martin Heusel', 'Hubert Ramsauer', 'Thomas Unterthiner', 'Bernhard Nessler', 'Sepp Hochreiter'], 'venue': 'In Advances in Neural Information Processing Systems,', 'citeRegEx': 'Heusel et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Heusel et al\\.', 'year': 2017}, {'title': 'Batch normalization: Accelerating deep network training by reducing internal covariate shift', 'author': ['Sergey Ioffe', 'Christian Szegedy'], 'venue': 'In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37,', 'citeRegEx': 'Ioffe and Szegedy.,? \\Q2015\\E', 'shortCiteRegEx': 'Ioffe and Szegedy.', 'year': 2015}, {'title': 'The relativistic discriminator: a key element missing from standard gan', 'author': ['Alexia Jolicoeur-Martineau'], 'venue': 'In ICLR,', 'citeRegEx': 'Jolicoeur.Martineau.,? \\Q2019\\E', 'shortCiteRegEx': 'Jolicoeur.Martineau.', 'year': 2019}, {'title': 'Progressive growing of GANs for improved quality, stability, and variation', 'author': ['Tero Karras', 'Timo Aila', 'Samuli Laine', 'Jaakko Lehtinen'], 'venue': 'In International Conference on Learning Representations,', 'citeRegEx': 'Karras et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Karras et al\\.', 'year': 2018}, {'title': 'A style-based generator architecture for generative adversarial networks', 'author': ['Tero Karras', 'Samuli Laine', 'Timo Aila'], 'venue': 'In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),', 'citeRegEx': 'Karras et al\\.,? \\Q2019\\E', 'shortCiteRegEx': 'Karras et al\\.', 'year': 2019}, {'title': 'Adam: A method for stochastic optimization', 'author': ['Diederik P. Kingma', 'Jimmy Ba'], 'venue': 'In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,', 'citeRegEx': 'Kingma and Ba.,? \\Q2015\\E', 'shortCiteRegEx': 'Kingma and Ba.', 'year': 2015}, {'title': 'Learning multiple layers of features from tiny images', 'author': ['Alex Krizhevsky'], 'venue': 'Technical report,', 'citeRegEx': 'Krizhevsky.,? \\Q2009\\E', 'shortCiteRegEx': 'Krizhevsky.', 'year': 2009}, {'title': 'Photorealistic single image super-resolution using a generative adversarial network', 'author': ['Christian Ledig', 'Lucas Theis', 'Ferenc Huszár', 'Jose Caballero', 'Andrew Cunningham', 'Alejandro Acosta', 'Andrew Aitken', 'Alykhan Tejani', 'Johannes Totz', 'Zehan Wang', 'Wenzhe Shi'], 'venue': 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR),', 'citeRegEx': 'Ledig et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Ledig et al\\.', 'year': 2017}, {'title': 'COCO-GAN: Conditional coordinate generative adversarial network, 2019', 'author': ['Chieh Hubert Lin', 'Chia-Che Chang', 'Yu-Sheng Chen', 'Da-Cheng Juan', 'Wei Wei', 'Hwann-Tzong Chen'], 'venue': 'URL https: //openreview.net/forum?id=r14Aas09Y7', 'citeRegEx': 'Lin et al\\.,? \\Q2019\\E', 'shortCiteRegEx': 'Lin et al\\.', 'year': 2019}, {'title': 'Deep learning face attributes in the wild', 'author': ['Ziwei Liu', 'Ping Luo', 'Xiaogang Wang', 'Xiaoou Tang'], 'venue': 'In Proceedings of International Conference on Computer Vision (ICCV),', 'citeRegEx': 'Liu et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Liu et al\\.', 'year': 2015}, {'title': 'Least squares generative adversarial networks', 'author': ['Xudong Mao', 'Qing Li', 'Haoran Xie', 'Raymond YK Lau', 'Zhen Wang', 'Stephen Paul Smolley'], 'venue': None, 'citeRegEx': 'Mao et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Mao et al\\.', 'year': 2017}, {'title': 'Spectral normalization for generative adversarial networks', 'author': ['Takeru Miyato', 'Toshiki Kataoka', 'Masanori Koyama', 'Yuichi Yoshida'], 'venue': 'In International Conference on Learning Representations,', 'citeRegEx': 'Miyato et al\\.,? \\Q2018\\E', 'shortCiteRegEx': 'Miyato et al\\.', 'year': 2018}, {'title': 'Unsupervised representation learning with deep convolutional generative adversarial networks', 'author': ['Alec Radford', 'Luke Metz', 'Soumith Chintala'], 'venue': 'arXiv preprint arXiv:1511.06434,', 'citeRegEx': 'Radford et al\\.,? \\Q2015\\E', 'shortCiteRegEx': 'Radford et al\\.', 'year': 2015}, {'title': 'Seqgan: Sequence generative adversarial nets with policy gradient', 'author': ['Lantao Yu', 'Weinan Zhang', 'Jun Wang', 'Yong Yu'], 'venue': 'In AAAI,', 'citeRegEx': 'Yu et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Yu et al\\.', 'year': 2017}, {'title': 'Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks', 'author': ['Han Zhang', 'Tao Xu', 'Hongsheng Li', 'Shaoting Zhang', 'Xiaolei Huang', 'Xiaogang Wang', 'Dimitris N. Metaxas'], 'venue': 'IEEE International Conference on Computer Vision (ICCV),', 'citeRegEx': 'Zhang et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Zhang et al\\.', 'year': 2017}, {'title': 'Energy-based generative adversarial network', 'author': ['Junbo Zhao', 'Michael Mathieu', 'Yann LeCun'], 'venue': 'In ICLR,', 'citeRegEx': 'Zhao et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Zhao et al\\.', 'year': 2017}, {'title': 'Unpaired image-to-image translation using cycle-consistent adversarial networks', 'author': ['Jun-Yan Zhu', 'Taesung Park', 'Phillip Isola', 'Alexei A Efros'], 'venue': 'In ICCV,', 'citeRegEx': 'Zhu et al\\.,? \\Q2017\\E', 'shortCiteRegEx': 'Zhu et al\\.', 'year': 2017}], 'referenceMentions': [{'referenceID': 17, 'context': 'Moreover, it enables the basic DCGAN (Radford et al., 2015) architecture to generate realistic images at 1024*1024 resolution when trained from scratch.', 'startOffset': 37, 'endOffset': 59}, {'referenceID': 3, 'context': 'The development of generative adversarial network (GAN) (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017) is one of the most important topics in machine learning since its first appearance in (Goodfellow et al.', 'startOffset': 56, 'endOffset': 126}, {'referenceID': 17, 'context': 'The development of generative adversarial network (GAN) (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017) is one of the most important topics in machine learning since its first appearance in (Goodfellow et al.', 'startOffset': 56, 'endOffset': 126}, {'referenceID': 3, 'context': ', 2017) is one of the most important topics in machine learning since its first appearance in (Goodfellow et al., 2014).', 'startOffset': 94, 'endOffset': 119}, {'referenceID': 1, 'context': 'Due to its flexibility when dealing with high dimensional data, GAN has obtained remarkable progresses on realistic image generation (Brock et al., 2019).', 'startOffset': 133, 'endOffset': 153}, {'referenceID': 3, 'context': 'In the standard formulation (Goodfellow et al., 2014), the realness of an input sample is estimated by the discriminator using a single scalar.', 'startOffset': 28, 'endOffset': 53}, {'referenceID': 3, 'context': 'In this paper, we propose to generalize the standard framework (Goodfellow et al., 2014) by treating realness as a random variable, represented as a distribution rather than a single scalar.', 'startOffset': 63, 'endOffset': 88}, {'referenceID': 3, 'context': 'In the original work of (Goodfellow et al., 2014), the learning process of D and G follows a minimax game with value function V (G,D):', 'startOffset': 24, 'endOffset': 49}, {'referenceID': 17, 'context': 'The theoretical guarantees provide strong supports for GAN’s success in many applications (Radford et al., 2015; Yu et al., 2017; Zhu et al., 2017; Dai et al., 2017), and inspired multiple variants (Arjovsky et al.', 'startOffset': 90, 'endOffset': 165}, {'referenceID': 18, 'context': 'The theoretical guarantees provide strong supports for GAN’s success in many applications (Radford et al., 2015; Yu et al., 2017; Zhu et al., 2017; Dai et al., 2017), and inspired multiple variants (Arjovsky et al.', 'startOffset': 90, 'endOffset': 165}, {'referenceID': 21, 'context': 'The theoretical guarantees provide strong supports for GAN’s success in many applications (Radford et al., 2015; Yu et al., 2017; Zhu et al., 2017; Dai et al., 2017), and inspired multiple variants (Arjovsky et al.', 'startOffset': 90, 'endOffset': 165}, {'referenceID': 2, 'context': 'The theoretical guarantees provide strong supports for GAN’s success in many applications (Radford et al., 2015; Yu et al., 2017; Zhu et al., 2017; Dai et al., 2017), and inspired multiple variants (Arjovsky et al.', 'startOffset': 90, 'endOffset': 165}, {'referenceID': 15, 'context': ', 2017), and inspired multiple variants (Arjovsky et al., 2017; Mao et al., 2017; Zhao et al., 2017; Berthelot et al., 2017) to improve the original design.', 'startOffset': 40, 'endOffset': 124}, {'referenceID': 20, 'context': ', 2017), and inspired multiple variants (Arjovsky et al., 2017; Mao et al., 2017; Zhao et al., 2017; Berthelot et al., 2017) to improve the original design.', 'startOffset': 40, 'endOffset': 124}, {'referenceID': 0, 'context': ', 2017), and inspired multiple variants (Arjovsky et al., 2017; Mao et al., 2017; Zhao et al., 2017; Berthelot et al., 2017) to improve the original design.', 'startOffset': 40, 'endOffset': 124}, {'referenceID': 3, 'context': 'It’s worth noting that these two objectives are equivalent in the original work (Goodfellow et al., 2014).', 'startOffset': 80, 'endOffset': 105}, {'referenceID': 8, 'context': 'Flexibility of RealnessGAN: as a generalization of the standard framework, it is straightforward to integrate RealnessGAN with different GAN architectures, such as progressive GANs (Karras et al., 2018; 2019) and conditional GANs (Zhu et al.', 'startOffset': 181, 'endOffset': 208}, {'referenceID': 21, 'context': ', 2018; 2019) and conditional GANs (Zhu et al., 2017; Ledig et al., 2017).', 'startOffset': 35, 'endOffset': 73}, {'referenceID': 12, 'context': ', 2018; 2019) and conditional GANs (Zhu et al., 2017; Ledig et al., 2017).', 'startOffset': 35, 'endOffset': 73}, {'referenceID': 7, 'context': 'There are several choices for the regularizer, such as the relativistic term introduced in (Jolicoeur-Martineau, 2019) that minimizes the KL divergence', 'startOffset': 91, 'endOffset': 118}, {'referenceID': 3, 'context': 'Generative adversarial network (GAN) was first proposed in (Goodfellow et al., 2014), which jointly learns a discriminatorD and a generatorG in an adversarial manner.', 'startOffset': 59, 'endOffset': 84}, {'referenceID': 17, 'context': 'Due to its outstanding learning ability, GANs have been adopted in various generative tasks (Radford et al., 2015; Yu et al., 2017; Zhu et al., 2017), among which Deep Convolutional GAN (DCGAN) (Radford et al.', 'startOffset': 92, 'endOffset': 149}, {'referenceID': 18, 'context': 'Due to its outstanding learning ability, GANs have been adopted in various generative tasks (Radford et al., 2015; Yu et al., 2017; Zhu et al., 2017), among which Deep Convolutional GAN (DCGAN) (Radford et al.', 'startOffset': 92, 'endOffset': 149}, {'referenceID': 21, 'context': 'Due to its outstanding learning ability, GANs have been adopted in various generative tasks (Radford et al., 2015; Yu et al., 2017; Zhu et al., 2017), among which Deep Convolutional GAN (DCGAN) (Radford et al.', 'startOffset': 92, 'endOffset': 149}, {'referenceID': 17, 'context': ', 2017), among which Deep Convolutional GAN (DCGAN) (Radford et al., 2015) has shown promising results in image generation.', 'startOffset': 52, 'endOffset': 74}, {'referenceID': 15, 'context': '(2017) replaces JS-divergence with Earth-Mover’s Distance, and Least-Square GAN (LSGAN) (Mao et al., 2017) transforms the objective ofG to Pearson divergence.', 'startOffset': 88, 'endOffset': 106}, {'referenceID': 20, 'context': 'Energy-based GAN (EBGAN) (Zhao et al., 2017) and Boundary Equilibrium GAN (BEGAN) (Berthelot et al.', 'startOffset': 25, 'endOffset': 44}, {'referenceID': 0, 'context': ', 2017) and Boundary Equilibrium GAN (BEGAN) (Berthelot et al., 2017) employ a pre-trained auto-encoder as the discriminator, learning to distinguish between real and generated samples via reconstruction.', 'startOffset': 45, 'endOffset': 69}, {'referenceID': 8, 'context': 'Generally, ProgressiveGAN (Karras et al., 2018) and StyleGAN (Karras et al.', 'startOffset': 26, 'endOffset': 47}, {'referenceID': 9, 'context': ', 2018) and StyleGAN (Karras et al., 2019) propose a progressive paradigm, which starts from a shallow model focusing on a low resolution, and gradually grows into a deeper model to incorporate more details as resolution grows.', 'startOffset': 21, 'endOffset': 42}, {'referenceID': 13, 'context': 'On the other hand, COCO-GAN (Lin et al., 2019) tackles high resolution image generation in a divide-and-conquer strategy.', 'startOffset': 28, 'endOffset': 46}, {'referenceID': 3, 'context': 'It’s worth noting that many works on generative adversarial networks have discussed ‘distributions’ (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017), which usually refers to the underlying distribution of samples.', 'startOffset': 100, 'endOffset': 170}, {'referenceID': 17, 'context': 'It’s worth noting that many works on generative adversarial networks have discussed ‘distributions’ (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017), which usually refers to the underlying distribution of samples.', 'startOffset': 100, 'endOffset': 170}, {'referenceID': 11, 'context': '2) Then we evaluate RealnessGAN on CIFAR10 (32*32) (Krizhevsky, 2009) and CelebA (256*256) (Liu et al.', 'startOffset': 51, 'endOffset': 69}, {'referenceID': 14, 'context': '2) Then we evaluate RealnessGAN on CIFAR10 (32*32) (Krizhevsky, 2009) and CelebA (256*256) (Liu et al., 2015) datasets qualitatively and quantitatively.', 'startOffset': 91, 'endOffset': 109}, {'referenceID': 9, 'context': 'Surprisingly, on the FFHQ dataset (Karras et al., 2019), RealnessGAN managed to generate images at the 1024*1024 resolution based on a non-progressive architecture.', 'startOffset': 34, 'endOffset': 55}, {'referenceID': 17, 'context': 'We compare RealnessGAN to other popular objectives in generative adversarial learning, including the standard GAN (Std-GAN) (Radford et al., 2015), WGAN-GP (Arjovsky et al.', 'startOffset': 124, 'endOffset': 146}, {'referenceID': 20, 'context': ', 2017), HingeGAN (Zhao et al., 2017) and LSGAN (Mao et al.', 'startOffset': 18, 'endOffset': 37}, {'referenceID': 16, 'context': 'Batch normalization (Ioffe & Szegedy, 2015) is used in G, and spectral normalization (Miyato et al., 2018) is used in D.', 'startOffset': 85, 'endOffset': 106}, {'referenceID': 4, 'context': '9, updating D for 5 times per G’s update (Gulrajani et al., 2017); for the remaining models, we use lr = 2e − 4, β1 = 0.', 'startOffset': 41, 'endOffset': 65}, {'referenceID': 17, 'context': '999, updating D for one time per G’s update (Radford et al., 2015).', 'startOffset': 44, 'endOffset': 66}, {'referenceID': 5, 'context': 'Fréchet Inception Distance (FID) (Heusel et al., 2017) and Sliced Wasserstein Distance (SWD) (Karras et al.', 'startOffset': 33, 'endOffset': 54}, {'referenceID': 8, 'context': ', 2017) and Sliced Wasserstein Distance (SWD) (Karras et al., 2018) are reported as the evaluation metrics.', 'startOffset': 46, 'endOffset': 67}, {'referenceID': 9, 'context': 're-implemented StyleGAN (Karras et al., 2019) trained under a similar setting receives an FID score of 16.', 'startOffset': 24, 'endOffset': 45}], 'year': 2020, 'abstractText': 'While generative adversarial networks (GAN) have been widely adopted in various topics, in this paper we generalize the standard GAN to a new perspective by treating realness as a random variable that can be estimated from multiple angles. In this generalized framework, referred to as RealnessGAN1, the discriminator outputs a distribution as the measure of realness. While RealnessGAN shares similar theoretical guarantees with the standard GAN, it provides more insights on adversarial learning. Compared to multiple baselines, RealnessGAN provides stronger guidance for the generator, achieving improvements on both synthetic and real-world datasets. Moreover, it enables the basic DCGAN (Radford et al., 2015) architecture to generate realistic images at 1024*1024 resolution when trained from scratch.', 'creator': 'LaTeX with hyperref'}","the development of generative adversarial network (gan) (goodfellow et al., 2014; radford et al., 2015; arjovsky et al., 2017) is one of the most important topics in machine learning since its first appearance in (goodfellow et al., 2014). it learns a discriminator along with the target generator in an adversarial manner, where the discriminator distinguishes generated samples from real ones. due to its flexibility when dealing with high dimensional data, gan has obtained remarkable progresses on realistic image generation (brock et al., 2019). in the standard formulation (goodfellow et al., 2014), the realness of an input sample is estimated by the discriminator using a single scalar. however, for high dimensional data such as images, we naturally perceive them from more than one angles and deduce whether it is life-like based on multiple criteria. as shown in fig.1, when a portrait is given, one might focus on its facial structure, skin tint, hair texture and even details like iris and teeth if allowed, each of which indicates a different aspect of realness. based on this observation, the single scalar could be viewed as an abstract or a summarization of multiple measures, which together reflect the overall realness of an image. such a concise measurement may convey insufficient information to guide the generator, potentially leading to well-known issues such as mode-collapse and gradient vanishing. in this paper, we propose to generalize the standard framework (goodfellow et al., 2014) by treating realness as a random variable, represented as a distribution rather than a single scalar. we refer to ∗equal contribution. 1code will be available at https://github.com/kam1107/realnessgan such a generalization as realnessgan. the learning process of realnessgan abide by the standard setting, but in a distributional form. while the standard gan can be viewed as a special case of realnessgan, realnessgan and the standard gan share similar theoretical guarantees. i.e. realnessgan converges to a nash-equilibrium where the generator and the discriminator reach their optimalities. moreover, by expanding the scalar realness score into a distributional one, the discriminator d naturally provides stronger guidance to the generator g where g needs to match not only the overall realness (as in the standard gan), but the underlying realness distribution as well. consequently, realnessgan facilitates g to better approximate the data manifold while generating decent samples. as shown in the experiments, based on a rather simple dcgan architecture, realnessgan could successfully learn from scratch to generate realistic images at 1024*1024 resolution.","it learns a discriminator along with the target generator in an adversarial manner, where the discriminator distinguishes generated samples from real ones due to its flexibility when dealing with high dimensional data, gan has obtained remarkable progresses on realistic image generation (brock et al however, for high dimensional data such as images, we naturally perceive them from more than one angles and deduce whether it is life-like based on multiple criteria 1, when a portrait is given, one might focus on its facial structure, skin tint, hair texture and even details like iris and teeth if allowed, each of which indicates a different aspect of realness based on this observation, the single scalar could be viewed as an abstract or a summarization of multiple measures, which together reflect the overall realness of an image such a concise measurement may convey insufficient information to guide the generator, potentially leading to well-known issues such as mode-collapse and gradient vanishing moreover, by expanding the scalar realness score into a distributional one, the discriminator d naturally provides stronger guidance to the generator g where g needs to match not only the overall realness (as in the standard gan), but the underlying realness distribution as well as shown in the experiments, based on a rather simple dcgan architecture, realnessgan could successfully learn from scratch to generate realistic images at 1024*1024 resolution",0.9302762746810913,0.8751381635665894,0.9018652439117432
