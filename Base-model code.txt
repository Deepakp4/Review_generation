# -*- coding: utf-8 -*-
"""bs-model-rem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gHnrfKEyxhk0CxlI4xiDyMXOo1lPisAF
"""

!pip install jsonlines nltk scikit-learn rouge-score

!pip install sentence-transformers # installing the missing packages

import jsonlines
import nltk
import re
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import sent_tokenize, word_tokenize
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from tqdm import tqdm
import pickle
import logging
import warnings
import random

# Downloading NLTK data
nltk.download('punkt', quiet=True)
nltk.download('wordnet')
# Loading our test data
test_data = []
with jsonlines.open('/content/drive/MyDrive/Dissertation_review/model_training/dataset_TTV/test.jsonl', mode='r') as reader:
    for obj in reader:
        test_data.append(obj)
test = pd.DataFrame(test_data)

# Suppressing warnings and logging from transformer library
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)

# Text cleaning function
def clean_text(text):
    if not isinstance(text, str):
        text = str(text)
    text = re.sub(r'\s+', ' ', text)
    text = text.strip().lower()
    words = text.split()
    return ' '.join(words)

# function to extract relavant sections from data and content column
def extract_section(content, section_name):
    try:
        section_text = ''
        section_keywords = {
            'abstract': ['abstract'],
            'introduction': ['introduction', '1 introduction'],
            'methods': ['method', 'methodology', 'experimental'],
            'related work': ['related work', 'background', 'literature review'],
            'results': ['result', 'finding'],
            'conclusion': ['conclusion', 'discussion']
        }

        for section in content:
            heading = section.get('heading', '')
            if heading is None:
                heading = ''

            normalized_heading = heading.lower().strip()

            if any(keyword in normalized_heading for keyword in section_keywords[section_name]):
                text = section.get('text', '')
                if not isinstance(text, str):
                    text = str(text) if text is not None else ''
                section_text = clean_text(text)
                break

        return section_text if section_text else f'No {section_name.capitalize()} Found'
    except Exception as e:
        return f'Error extracting {section_name}: {str(e)}'

# extracting sentences from sections through Tfidf
def tfidf_extractive_review(document, target_word_count=300):
    sentences = sent_tokenize(document)
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)
    sentence_scores = tfidf_matrix.mean(axis=1).flatten().tolist()[0]
    ranked_sentences = sorted(((score, i) for i, score in enumerate(sentence_scores)), reverse=True)

    review = []
    word_count = 0
    for score, idx in ranked_sentences:
        review.append(sentences[idx])
        word_count += len(word_tokenize(sentences[idx]))
        if 200 <= word_count <= 350:
            break

    return ' '.join(review)

# Result matrix
def evaluate_rouge(generated_review, reference_review):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference_review, generated_review)
    return {
        'ROUGE-1': scores['rouge1'].fmeasure * 100,
        'ROUGE-2': scores['rouge2'].fmeasure * 100,
        'ROUGE-L': scores['rougeL'].fmeasure * 100
    }

def evaluate_additional_metrics(predictions, references):
    bleu_scores = []
    meteor_scores = []
    f1_scores = []
    precision_scores = []
    recall_scores = []

    smoothie = SmoothingFunction().method4

    for pred, ref in zip(predictions, references):
        pred_tokens = word_tokenize(pred)
        ref_tokens = [word_tokenize(ref)]

        # BLEU score
        bleu = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothie)
        bleu_scores.append(bleu)

        # METEOR score
        meteor = meteor_score(ref_tokens, pred_tokens)
        meteor_scores.append(meteor)

        # Precision, Recall, F1 (token-level)
        pred_set = set(pred_tokens)
        ref_set = set(word_tokenize(ref))
        common_tokens = pred_set.intersection(ref_set)
        precision = len(common_tokens) / len(pred_set) if len(pred_set) > 0 else 0
        recall = len(common_tokens) / len(ref_set)
        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0

        precision_scores.append(precision)
        recall_scores.append(recall)
        f1_scores.append(f1)

    return {
        'BLEU': bleu_scores,
        'METEOR': meteor_scores,
        'Precision': precision_scores,
        'Recall': recall_scores,
        'F1': f1_scores
    }

def analyze_section_batch(test, section_name, batch_size=50, output_file='section_analysis_results.txt'):
    max_rouge1, max_rouge2, max_rougeL = 0, 0, 0
    sum_rouge1, sum_rouge2, sum_rougeL = 0, 0, 0
    num_examples = 0

    generated_reviews = []
    reference_reviews = []

    with open(output_file, 'a') as f:
        f.write(f"\n{'='*50}\n")
        f.write(f"Analyzing {section_name.capitalize()} Section\n")
        f.write(f"{'='*50}\n")

        for start_idx in range(0, len(test), batch_size):
            end_idx = min(start_idx + batch_size, len(test))
            batch = test.iloc[start_idx:end_idx]

            for idx, row in batch.iterrows():
                if section_name == 'abstract':
                    section_text = row.get('abstract', '')
                else:
                    content = row['content']
                    section_text = extract_section(content, section_name)

                reference_review = row['review']

                if not section_text or section_text.startswith(f'No {section_name.capitalize()} Found') or section_text.startswith(f'Error extracting'):
                    f.write(f"Warning: No valid {section_name} found for document {idx + 1}\n")
                    continue

                generated_review = tfidf_extractive_review(section_text, target_word_count=300)
                generated_reviews.append(generated_review)
                reference_reviews.append(reference_review)

                rouge_scores = evaluate_rouge(generated_review, reference_review)

                f.write(f"\nDocument {idx + 1}:\n")
                f.write(f"{section_name.capitalize()}: {section_text[:100]}...\n")
                f.write(f"Generated Review: {generated_review[:100]}...\n")
                f.write(f"ROUGE Scores: {rouge_scores}\n")

                max_rouge1 = max(max_rouge1, rouge_scores['ROUGE-1'])
                max_rouge2 = max(max_rouge2, rouge_scores['ROUGE-2'])
                max_rougeL = max(max_rougeL, rouge_scores['ROUGE-L'])

                sum_rouge1 += rouge_scores['ROUGE-1']
                sum_rouge2 += rouge_scores['ROUGE-2']
                sum_rougeL += rouge_scores['ROUGE-L']

                num_examples += 1

            print(f"Processed {section_name} for documents {start_idx + 1} to {end_idx}")

    mean_rouge1 = sum_rouge1 / num_examples if num_examples > 0 else 0
    mean_rouge2 = sum_rouge2 / num_examples if num_examples > 0 else 0
    mean_rougeL = sum_rougeL / num_examples if num_examples > 0 else 0

    # Calculate additional metrics
    additional_metrics = evaluate_additional_metrics(generated_reviews, reference_reviews)

    summary_stats = {
        "Number of documents processed": num_examples,
        "Maximum ROUGE-1": max_rouge1,
        "Maximum ROUGE-2": max_rouge2,
        "Maximum ROUGE-L": max_rougeL,
        "Mean ROUGE-1": mean_rouge1,
        "Mean ROUGE-2": mean_rouge2,
        "Mean ROUGE-L": mean_rougeL,
        "Maximum BLEU": max(additional_metrics['BLEU']) if additional_metrics['BLEU'] else 0,
        "Mean BLEU": np.mean(additional_metrics['BLEU']) if additional_metrics['BLEU'] else 0,
        "Maximum METEOR": max(additional_metrics['METEOR']) if additional_metrics['METEOR'] else 0,
        "Mean METEOR": np.mean(additional_metrics['METEOR']) if additional_metrics['METEOR'] else 0,
        "Maximum Precision": max(additional_metrics['Precision']) if additional_metrics['Precision'] else 0,
        "Mean Precision": np.mean(additional_metrics['Precision']) if additional_metrics['Precision'] else 0,
        "Maximum Recall": max(additional_metrics['Recall']) if additional_metrics['Recall'] else 0,
        "Mean Recall": np.mean(additional_metrics['Recall']) if additional_metrics['Recall'] else 0,
        "Maximum F1": max(additional_metrics['F1']) if additional_metrics['F1'] else 0,
        "Mean F1": np.mean(additional_metrics['F1']) if additional_metrics['F1'] else 0
    }

    with open(output_file, 'a') as f:
        f.write(f"\nSummary Statistics for {section_name.capitalize()} Model:\n")
        for key, value in summary_stats.items():
            f.write(f"{key}: {value}\n")

    print(f"\nSummary Statistics for {section_name.capitalize()} Model:")
    for key, value in summary_stats.items():
        print(f"{key}: {value}")

    return summary_stats

def analyze_all_sections(test, batch_size=100, output_file='all_sections_analysis_results.txt'):
    sections = ['abstract', 'introduction', 'methods', 'related work', 'results', 'conclusion']
    all_sections_stats = {}

    for section in sections:
        print(f"\n{'='*50}")
        print(f"Analyzing {section.capitalize()} Section")
        print(f"{'='*50}")
        section_stats = analyze_section_batch(test, section, batch_size, output_file)
        all_sections_stats[section] = section_stats

    # Calculate overall statistics
    overall_stats = {
        "Number of documents processed": sum(stats["Number of documents processed"] for stats in all_sections_stats.values())
    }

    metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU', 'METEOR', 'Precision', 'Recall', 'F1']

    for metric in metrics:
        max_values = [stats[f"Maximum {metric}"] for stats in all_sections_stats.values() if f"Maximum {metric}" in stats]
        mean_values = [stats[f"Mean {metric}"] for stats in all_sections_stats.values() if f"Average {metric}" in stats]

        if max_values:
            overall_stats[f"Overall Maximum {metric}"] = max(max_values)
        if mean_values:
            overall_stats[f"Overall Mean {metric}"] = np.mean(mean_values)

    with open(output_file, 'a') as f:
        f.write("\n\n" + "="*50 + "\n")
        f.write("Overall Summary Statistics\n")
        f.write("="*50 + "\n")
        for key, value in overall_stats.items():
            f.write(f"{key}: {value}\n")

    print("\nOverall Summary Statistics:")
    for key, value in overall_stats.items():
        print(f"{key}: {value}")

    print(f"\nDetailed results have been saved to {output_file}")

import warnings

warnings.filterwarnings("ignore", message=".*HF_TOKEN.*")

# Main execution of our code 
analyze_all_sections(test, batch_size=200, output_file='all_sections_analysis_results.txt')

"""Abstract section has seperate column so, again need to train for seperate column."""